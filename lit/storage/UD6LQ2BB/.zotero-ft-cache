Frank Rosenblatt: Principles of N eurodynamics: Perceptrons and the Theory of Brain Mechanisms
c. VON DER MALSBURGI
Frank Rosenblatt's intention with his book, according to his own introduction, is not just to describe a machine, the perceptron, but rather to put forward a theory. He formulates a series of machines. Each machine serves to introduce a new concept.
The basic model is a three-layer perceptron. Each layer is a set of simple threshold elements, or "neurons". Layers S, A, R are coupled in series by synaptic connections S --4 A --4 R. S is a sensory surface (e.g. a retina). Layer A contains "associator units" , or feature detector cells, and R contains R-cells, or "recognition cells". Present a pattern SI in S. After one synaptic delay a pattern Al in A is active, after another delay a set of R-cells fires. An R-cell is to be activated precisely when the pattern projected onto S is of a certain type (e.g. is a triangle). Each A-unit receives connections from a specific subset of the cells of S and is fired by a specific pattern on this support. The purpose of the A-layer is to reduce the overlap between patterns which are to be classified differently.
The weights of the connections from A to an R-cell are set by synaptic plasticity. When a pattern has been presented to S, an external "teacher" decides for the R-cell whether it has responded correctly. If the cell fires although it shouldn't, the synaptic weights of all currently active A-cells to the R-cell are reduced. If the cell doesn't fire although it should, the synaptic weights of all active A-cells to the R-cell are increased. No changes take place in the case of correct response.
There is a convergence theorem. For the case that there exists a vector of synaptic A ~ R connections which leads to the correct classification of patterns by the R-celI, the theorem states that such a vector can be found by the above learning procedure in finite time.
It is a serious weakness of the three-layer perceptron that it cannot generalize. If an R-cell has learnt to discriminate a particular pattern in S from all other patterns, it does not recognize the same pattern in other positions on S. Rosenblatt therefore introduces the four-layer perceptron. It has layers S, A (1), A (2) and R. For each cell in A (2) there exist many cells in A (1), each
1 Max-Planck-Institut fiir Biophysikalische Chemie, 3400 Gottingen, FRG
Brain Theory Edited by G. Palm and A. Aertsen
Â© Springer-Verlag Berlin Heidelberg 1986

246
responding to the same feature in a different position on S, each having a connection to the cell in A (2), and each being able to fire the cell in A (2). Thus, a cell in A (2) responds to a feature irrespective of the feature's position. The set of cells in A (2) activated by a pattern in S is invariant with respect to the position of the pattern in S. (The concept of invariance is treated by Rosenblatt with respect to more general groups of transformations.)
With later versions of the perceptron Rosenblatt introduces cross-couplings within a layer and back-couplings to a previous layer. Transmission delays enable the perceptron to recognize temporal patterns. Numerous versions of cellular reaction types and of synaptic plasticity procedures are discussed. The book presents extensive statistical evaluations of cell numbers and of overlaps between sets of cells activated in the A-layers in response to different stimuli. A great number of experiments with concrete versions of perceptrons are presented and discussed. The parts most valuable to the present-day reader are, however, the various summaries and discussions at the beginnings and the ends of chapters. It is these which contain a wealth of ideas and insights regarding perception and brain function. Nobody was more aware of the limations of the perceptron approach than Rosenblatt, and one could only wish that more of the amateurs of brain theory read the book before they embarked on extensive projects in pattern recognition and brain modelling.
Rosenblatt's book crystallizes a number of concepts which together characterize the brain theory era of the last 25 years. Let me mention some of the most important of these concepts. Perceptrons are input-driven deterministic machines. A few transmission-delays after the presentation of a stimulus the machine has settled into a stable state. Neither has the perceptron spontaneity (e,g. active generation and testing of hypotheses), nor can it iterate more than a few times. This simplicity is dictated mainly by limitations in the power of analysis. No scheme establishing a connection between local structure and global function in back-coupled networks was known at the time.
The perceptron explicitly formulates a theory of perception. According to this, perception is a passive process (as has just been stated). The perceptron is based on the idea that a pattern on a sensory surface can be completely represented by the vector of activities of a fixed set of feature detector cells. Recognition of a pattern is tantamount to the activation of an R-cell ("pontifical cell"). A whole scene is represented by a set of such recognition cells. The perceptron thus explicitely formulates the position of semantic atomism, which to the present day is. the basis of most of brain-theory: complex symbols (for the representation of patterns, scenes, etc.) are additively composed of atoms (cells), each of which has some more or less elementary bit of meaning attached to it (blue light on point x of retina, my grandmother, ... ).
The perceptron exemplifies hetero-organization. The structure of a perceptron comes from two sources. It is "genetic" information which defines the functional constraints of cells and synapses, the division into planes (S, A (1), A (2), R), and especially also the structure of the S -+ A connections which determine the features that can be detected. The patterns to be recognized are defined

247
by the "teacher", and are written into the system with the help of synaptic plasticity. The perceptron cannot develop new features or new patterns on its own.
The perceptron has various limitations, many of which have been discussed by Rosenblatt himself in his book. The most famous limitation of the threelayer perceptron is its unability to distinguish connected from unconnected figures. (This limitation is already mentioned in Rosenblatt's book and has been extensively discussed in a book by M. Minsky and S. Papert.) The difficulty can be easily mended in a scheme involving a miminum of serial computation.
A more serious limitation is the complete lack of syntactical information in the perceptron, as was pointed out by Rosenblatt himself. He employed the following example. Suppose there are four R-cells: Rt:,. and Ro recognize a triangle and a square, RT and R~ recognize that there is some pattern in the upper and in the lower half of retina. What, if there is a triangle in the upper half and a square in the lower half? All four cells go on. The situation is confused with one in which the positions of triangle and square are exchanged. Additional machinery for the expression of syntactical information would enable the system to make statements like "Rt:,. and RT refer to the same object", and thereby resolve the ambiguity. In layer A (2) of a four-layer perceptron the same difficulty leads to more fundamental problems. There is no information in A (2) expressing the spatial grouping of features within the original pattern. This leads to the serious ambiguity between different patterns which contain the same local features but in a different arrangement. Also, information on the segmentation of a scene into objects cannot be expressed. This is a fundamental flaw of semantic atomism. Rosenblatt attempts a solution, by introducing extra connections in his perceptron with the result that activity in the R-layer is restricted to those cells which refer to one object only (e.g. Rt:,. and Rr in the example).
The whole perceptron approach has been haunted by a lack of appreciation of quantitative limitations. The convergence theorem, for instance, is of no practical use if convergence takes eons. Another limitation concerns numbers of feature detectors (in the A-layers). Pattern separation is only possible if the sets of activated feature cells are sufficiently disjunct. This in turn means that features must be rather specific to small pattern classes. In a rich environment the number of feature cells required diverges quickly. Realistic cell numbers rigidly limit the perceptron to very small pattern universes. A solution to this problem can only come from a scheme by which highly specific feature units are combinatorially built from low-specificity cells.
The perceptron and the ideas set out in Rosenblatt's book were of great historical importance. After an era which had indulged in formulating and reformulating universality claims in splendid generality, the perceptron focussed attention on an important particular problem, pattern classification (although Rosenblatt couldn't resist including a useless universality claim in his book). There may have been several hundred groups all over the world experimenting with and theorizing about perceptron-like structures. The perceptron and the

248
associative memory continue to be the dominating ideas of brain theory, although they both date back to before 1960. The perceptron may be considered the last major artificial intelligence project which was formulated in neural terms and which has had close interactions with experimental neurobiology. After the perceptron (and perhaps partly in reaction to the disappointment with the perceptron) neurobiology and artificial intelligence have slowly lost contact and have ceased to inspire each other. Only recent years have seen a cautious revival of this symbiosis, in the form of massively parallel models of image interpretation and low-level vision.
On the surface, the reaction of the academic community to the perceptron has not exactly been a warm-hearted welcome. Some well-published theories were clearly elaborate versions of the perceptron yet failed to give explicit reference to Rosenblatt. Some of the reactions, especially the book by Minsky and Papert, identified the perceptron with its first bridge-head, the "simple perceptron", and then rejected it as being insufficient (see above), instead of taking it as a road into a new unknown land, as had been the intention of Rosenblatt. As a consequence of this reaction, perceptron theory has not become a focus of intellectual activity, in spite of its great influence on the practical activities of model builders. There may have been several reasons for this reaction of the academic community - the early frustrations after the exaggerated initial claims, the close engagement of Rosenblatt with the military, the inadequacy of the mathematical methods at the time, and the absorption of artificial intelligence into computer science and the algorithmic approach.
Two developments of recent years may revive interest in artificial intelligence "neural style". With probabilistic methods ("annealing", BoltzmannGibbs-statistics) it has recently become possible to theoretically derive the global behaviour of cross-coupled nervous network models from knowledge about local structure. Now, one can deal with cooperative phenomena and pattern formation in iterative processes. The other development is that the classical approach of computer science - algorithmically controlled sequential machines - is beginning to feel its quantitative limits with respect to software volume and computing time. For the future, one may hope that more intellectual effort will be devoted to nervous networks, that is, to self-organizing massively parallel structures. This development would have to take up the thread were Rosenblatt has dropped it.
ReferencÂ£'
Rosenblatt F (1961) Principles of neurodynamics: Perceptrons and the theory of brain mechanism. Spartan Books. Washington, DC

