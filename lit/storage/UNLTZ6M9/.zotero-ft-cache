
Skip to main content
Cornell University
We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate
arxiv logo > cs > arXiv:1803.05407

Help | Advanced Search
Search
Computer Science > Machine Learning
(cs)
[Submitted on 14 Mar 2018 ( v1 ), last revised 25 Feb 2019 (this version, v3)]
Title: Averaging Weights Leads to Wider Optima and Better Generalization
Authors: Pavel Izmailov , Dmitrii Podoprikhin , Timur Garipov , Dmitry Vetrov , Andrew Gordon Wilson
View a PDF of the paper titled Averaging Weights Leads to Wider Optima and Better Generalization, by Pavel Izmailov and 4 other authors
View PDF

    Abstract: Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead. 

Comments: 	Appears at the Conference on Uncertainty in Artificial Intelligence (UAI), 2018
Subjects: 	Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)
Cite as: 	arXiv:1803.05407 [cs.LG]
  	(or arXiv:1803.05407v3 [cs.LG] for this version)
  	https://doi.org/10.48550/arXiv.1803.05407
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Andrew Wilson [ view email ]
[v1] Wed, 14 Mar 2018 17:09:27 UTC (1,402 KB)
[v2] Wed, 8 Aug 2018 08:49:15 UTC (1,404 KB)
[v3] Mon, 25 Feb 2019 14:18:11 UTC (1,404 KB)
Full-text links:
Access Paper:

    View a PDF of the paper titled Averaging Weights Leads to Wider Optima and Better Generalization, by Pavel Izmailov and 4 other authors
    View PDF
    TeX Source
    Other Formats 

view license
Current browse context:
cs.LG
< prev   |   next >
new | recent | 1803
Change to browse by:
cs
cs.AI
cs.CV
stat
stat.ML
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

5 blog links
( what is this? )
DBLP - CS Bibliography
listing | bibtex
Pavel Izmailov
Dmitrii Podoprikhin
Timur Garipov
Dmitry P. Vetrov
Andrew Gordon Wilson
a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

