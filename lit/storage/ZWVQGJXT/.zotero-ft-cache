









JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article
Elsevier logo ScienceDirect

    Journals & Books 

    Search 

Register Sign in

    View  PDF
    Download full issue 

Outline

    Abstract
    Keywords
    1. Introduction
    2. Preliminaries
    3. Classical ridgelet transform
    4. Ridgelet transform with respect to distributions
    5. Reconstruction formula for weak ridgelet transform
    6. Neural network with unbounded activation functions
    7. Numerical examples of reconstruction
    8. Concluding remarks
    Acknowledgements
    Appendix A. Proof of Theorem 4.2
    Appendix B. Proof of Theorem 5.4
    Appendix C. Proof of Theorem 5.6
    Appendix D. Proof of Theorem 5.7
    Appendix E. Proof of Theorem 5.11
    Appendix F. Proofs of Example 6.4 and Example 6.10
    References 

Show full outline
Cited by (292)
Figures (6)

    Fig. 1. Zoo of activation functions: the Gaussian G(z) (red), the first derivative…
    Fig. 2. Ridgelet transform Rψf(a,b) of f(x)=sin⁡2πx defined on [−1,1] with respect to ψ
    Fig. 3. Reconstruction with the derivative of sigmoidal function σ′, sigmoidal function…
    Fig. 4. Reconstruction with truncated power functions — Dirac's δ, unit step z+0, and…
    Fig. 5. Reconstruction with linear function η(z)=z
    Fig. 6. Reconstruction with RBF G, unit step z+0, and ReLU z+

Tables (5)

    Table 1
    Table 2
    Table 3
    Table 4
    Table 5 

Elsevier
Applied and Computational Harmonic Analysis
Volume 43, Issue 2 , September 2017, Pages 233-268
Applied and Computational Harmonic Analysis
Neural network with unbounded activation functions is universal approximator
Author links open overlay panel Sho Sonoda , Noboru Murata
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.acha.2015.12.005 Get rights and content
Under an Elsevier user license
open archive
Abstract

This paper presents an investigation of the approximation property of neural networks with unbounded activation functions, such as the rectified linear unit (ReLU), which is the new de-facto standard of deep learning . The ReLU network can be analyzed by the ridgelet transform with respect to Lizorkin distributions. By showing three reconstruction formulas by using the Fourier slice theorem, the Radon transform, and Parseval's relation , it is shown that a neural network with unbounded activation functions still satisfies the universal approximation property. As an additional consequence, the ridgelet transform, or the backprojection filter in the Radon domain, is what the network learns after backpropagation. Subject to a constructive admissibility condition, the trained network can be obtained by simply discretizing the ridgelet transform, without backpropagation. Numerical examples not only support the consistency of the admissibility condition but also imply that some non-admissible cases result in low-pass filtering.

    Previous article in issue
    Next article in issue 

Keywords
Neural network
Integral representation
Rectified linear unit (ReLU)
Universal approximation
Ridgelet transform
Admissibility condition
Lizorkin distribution
Radon transform
Backprojection filter
Bounded extension to L 2
1. Introduction

Consider approximating a function f : R m → C by the neural network g J with an activation function η : R → C (1) g J ( x ) = 1 J ∑ j J c j η ( a j ⋅ x − b j ) , ( a j , b j , c j ) ∈ R m × R × C where we refer to ( a j , b j ) as a hidden parameter and c j as an output parameter. Let Y m + 1 denote the space of hidden parameters R m × R . The network g J can be obtained by discretizing the integral representation of the neural network (2) g ( x ) = ∫ Y m + 1 T ( a , b ) η ( a ⋅ x − b ) d μ ( a , b ) , where T : Y m + 1 → C corresponds to a continuous version of the output parameter; μ denotes a measure on Y m + 1 . The right-hand side expression is known as the dual ridgelet transform of T with respect to η (3) R η † T ( x ) = ∫ Y m + 1 T ( a , b ) η ( a ⋅ x − b ) d a d b ‖ a ‖ . By substituting in T ( a , b ) the ridgelet transform of f with respect to ψ (4) R ψ f ( a , b ) : = ∫ R m f ( x ) ψ ( a ⋅ x − b ) ‾ ‖ a ‖ d x , under some good conditions, namely the admissibility of ( ψ , η ) and some regularity of f , we can reconstruct f by (5) R η † R ψ f = f . By discretizing the reconstruction formula, we can verify the approximation property of neural networks with the activation function η .

In this study, we investigate the approximation property of neural networks for the case in which η is a Lizorkin distribution, by extensively constructing the ridgelet transform with respect to Lizorkin distributions. The Lizorkin distribution space S 0 ′ is such a large space that contains the rectified linear unit (ReLU) z + , truncated power functions z + k , and other unbounded functions that have at most polynomial growth (but do not have polynomials as such). Table 1 and Fig. 1 give some examples of Lizorkin distributions.

Table 1 . Zoo of activation functions with which the corresponding neural network can approximate arbitrary functions in L 1 ( R m ) in the sense of pointwise convergence (§ 5.2 ) and in L 2 ( R m ) in the sense of mean convergence (§ 5.3 ). The third column indicates the space W ( R ) to which an activation function η belong (§ 6.1 , 6.2 ).
Activation function	η ( z )	W
Unbounded functions
  Truncated power function	z + k : = { z k z > 0 0 z ≤ 0 , k ∈ N 0 	S 0 ′
  Rectified linear unit (ReLU)	z + 	S 0 ′
  Softplus function	σ ( − 1 ) ( z ) : = log ⁡ ( 1 + e z ) 	O M

Bounded but not integrable functions
  Unit step function	z + 0 	S 0 ′
  (Standard) sigmoidal function	σ ( z ) : = ( 1 + e − z ) − 1 	O M
  Hyperbolic tangent function	tanh ⁡ ( z ) 	O M

Bump functions
  (Gaussian) radial basis function	G ( z ) : = ( 2 π ) − 1 / 2 exp ⁡ ( − z 2 / 2 ) 	S
  The first derivative of sigmoidal function	σ ′ ( z )	S
  Dirac's δ 	δ ( z )	S 0 ′

Oscillatory functions
  The k th derivative of RBF	G ( k ) ( z )	S
  The k th derivative of sigmoidal function	σ ( k ) ( z )	S
  The k th derivative of Dirac's δ 	δ ( k ) ( z )	S 0 ′
Fig. 1

    Download : Download high-res image (55KB)
    Download : Download full-size image 

Fig. 1 . Zoo of activation functions: the Gaussian G( z ) (red), the first derivative G ′ ( z ) (yellow), the second derivative G ″ ( z ) (green); a truncated power function z + 2 (blue), the ReLU z + (sky blue), the unit step function z + 0 (rose). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

Recall that the derivative of the ReLU z + is the step function z + 0 . Formally, the following suggestive formula (6) ∫ Y m + 1 T ( a , b ) η ′ ( a ⋅ x − b ) d a d b ‖ a ‖ = ∫ Y m + 1 ∂ b T ( a , b ) η ( a ⋅ x − b ) d a d b ‖ a ‖ , holds, because the integral representation is a convolution in b . This formula suggests that once we have T step ( a , b ) for the step function, which is implicitly known to exist based on some of our preceding studies [1] , [2] , then we can formally obtain T ReLU ( a , b ) for the ReLU by differentiating T ReLU ( a , b ) = ∂ b T step ( a , b ) .
1.1. ReLU and other unbounded activation functions

The ReLU [3] , [4] , [5] , [6] became a new building block of deep neural networks , in the place of traditional bounded activation functions such as the sigmoidal function and the radial basis function (RBF). Compared with traditional units, a neural network with the ReLU is said [3] , [7] , [8] , [9] , [6] to learn faster because it has larger gradients that can alleviate the vanishing gradient [3] , and perform more efficiently because it extracts sparser features. To date, these hypotheses have only been empirically verified without analytical evaluation.

It is worth noting that in approximation theory , it was already shown in the 1990s that neural networks with such unbounded activation functions have the universal approximation property. To be precise, if the activation function is not a polynomial function , then the family of all neural networks is dense in some functional spaces such as L p ( R m ) and C 0 ( R m ) . Mhaskar and Micchelli [10] seem to be the first to have shown such universality by using the B-spline. Later, Leshno et al. [11] reached a stronger claim by using functional analysis . Refer to Pinkus [12] for more details.

In this study, we initially work through the same statement by using harmonic analysis , or the ridgelet transform. One strength is that our results are very constructive. Therefore, we can construct what the network will learn during backpropagation. Note that for bounded cases this idea is already implicit in [13] and [2] , and explicit in [14] .
1.2. Integral representation of neural network and ridgelet transform

We use the integral representation of neural networks introduced by Murata [1] . As already mentioned, the integral representation corresponds to the dual ridgelet transform. In addition, the ridgelet transform corresponds to the composite of a wavelet transform after the Radon transform. Therefore, neural networks have a profound connection with harmonic analysis and tomography.

As Kůrková [15] noted, the idea of discretizing integral transforms to obtain an approximation is very old in approximation theory. As for neural networks, at first, Carroll and Dickinson [16] and Ito [13] regarded a neural network as a Radon transform [17] . Irie and Miyake [18] , Funahashi [19] , Jones [20] , and Barron [21] used Fourier analysis to show the approximation property in a constructive way. Kůrková [15] applied Barron's error bound to evaluate the complexity of neural networks. Refer to Kainen et al. [22] for more details.

In the late 1990s, Candès [23] , [24] , Rubin [25] , and Murata [1] independently proposed the so-called ridgelet transform, which has since been investigated by a number of authors [26] , [27] , [28] , [29] , [30] , [31] .
1.3. Variations of ridgelet transform

A ridgelet transform R ψ , along with its reconstruction property, is determined by four classes of functions: domain X ( R m ) , range Y ( Y m + 1 ) , ridgelet Z ( R ) , and dual ridgelet W ( R ) . (7)

The following ladder relations by Schwartz [32] are fundamental for describing the variations of the ridgelet transform: (8) where the meaning of symbols are given below in Table 2 .

Table 2 . Classes of functions and distributions, and corresponding dual spaces.
Space	A ( R k ) 	Dual space	A ′ ( R k )
Polynomials of all degree	P ( R k ) 	–	
Smooth functions	E ( R k ) 	Compactly supported distributions	E ′ ( R k )
Rapidly decreasing functions	S ( R k ) 	Tempered distributions	S ′ ( R k )
Compactly supported smooth functions	D ( R k ) 	Schwartz distributions	D ′ ( R k )
L p of Sobolev order ∞ (1 ≤  p  < ∞)	D L p ( R k ) 	Schwartz dists. (1/ p  + 1/ q  = 1)	D L q ′ ( R k )
Completion of D ( R k ) in D L ∞ ( R k ) 	B ˙ ( R k ) 	Schwartz dists. ( p  = 1)	D L 1 ′ ( R k )
Slowly increasing functions	O M ( R k ) 	–	
–		Rapidly decreasing distributions	O C ′ ( R k )
Lizorkin functions	S 0 ( R k ) 	Lizorkin distributions	S 0 ′ ( R k )

The integral transform T by Murata [1] coincides with the case for Z ⊂ D and W ⊂ E ∩ L 1 . Candès [23] , [24] proposed the “ridgelet transform” for Z = W ⊂ S . Kostadinova et al. [30] , [31] defined the ridgelet transform for the Lizorkin distributions X = S 0 ′ , which is the broadest domain ever known, at the cost of restricting the choice of ridgelet functions to the Lizorkin functions W = Z = S 0 ⊂ S .
1.4. Our goal

Although many researchers have investigated the ridgelet transform [26] , [29] , [30] , [31] , in all the settings Z does not directly admit some fundamental activation functions, namely the sigmoidal function and the ReLU. One of the challenges we faced is to define the ridgelet transform for W = S 0 ′ , which admits the sigmoidal function and the ReLU.
2. Preliminaries
2.1. Notations

Throughout this paper, we consider approximating f : R m → C by a neural network g with hidden parameters ( a , b ) . Following Kostadinova et al. [30] , [31] , we denote by Y m + 1 : = R m × R the space of parameters ( a , b ) . As already denoted, we symbolize the domain of a ridgelet transform as X ( R m ) , the range as Y ( Y m + 1 ) , the space of ridgelets as Z ( R ) , and the space of dual ridgelets as W ( R ) .

We denote by S m − 1 the ( m − 1 ) -sphere { u ∈ R m | ‖ u ‖ = 1 } ; by R + the open half-line { α ∈ R | α > 0 } ; by H the open half-space R + × R . We denote by N and N 0 the sets of natural numbers excluding 0 and including 0, respectively.

We denote by ⋅ ˜ the reflection f ˜ ( x ) : = f ( − x ) ; by ⋅ ‾ the complex conjugate; by a ≲ b that there exists a constant C ≥ 0 such that a ≤ C b .
2.2. Class of functions and distributions

Following Schwartz, we denote the classes of functions and distributions as in Table 2 . For Schwartz's distributions , we refer to Schwartz [32] and Trèves [33] ; for Lebesgue spaces, Rudin [34] , Brezis [35] and Yosida [36] ; for Lizorkin distributions, Yuan et al. [37] and Holschneider [38] .

The space S 0 ( R k ) of Lizorkin functions is a closed subspace of S ( R k ) that consists of elements such that all moments vanish. That is, S 0 ( R k ) : = { ϕ ∈ S ( R k ) | ∫ R k x α ϕ ( x ) d x = 0  for any  α ∈ N 0 k } . The dual space S 0 ′ ( R k ) , known as the Lizorkin distribution space, is homeomorphic to the quotient space of S ′ ( R k ) by the space of all polynomials P ( R k ) . That is, S 0 ′ ( R k ) ≅ S ′ ( R k ) / P ( R k ) . Refer to Yuan et al. [37, Prop. 8.1] for more details. In this work we identify and treat every polynomial as zero in the Lizorkin distribution. That is, for p ∈ S ′ ( R k ) , if p ∈ P ( R k ) then p ≡ 0 in S 0 ′ ( R k ) .

For S m − 1 , we work on the two subspaces D ( S m − 1 ) ⊂ D ( R m ) and E ′ ( S m − 1 ) ⊂ E ′ ( R m ) . In addition, we identify D = S = O M = E and E ′ = O C ′ = S ′ = D L p ′ = D ′ .

For H , let E ( H ) ⊂ E ( R 2 ) and D ( H ) ⊂ D ( R 2 ) . For T ∈ E ( H ) , write (9) D s , t k , ℓ T ( α , β ) : = ( α + 1 / α ) s ( 1 + β 2 ) t / 2 ∂ α k ∂ β ℓ T ( α , β ) , s , t , k , ℓ ∈ N 0 . The space S ( H ) consists of T ∈ E ( H ) such that for any s , t , k , ℓ ∈ N 0 , the seminorm below is finite (10) sup ( α , β ) ∈ H ⁡ | D s , t k , ℓ T ( α , β ) | < ∞ . The space O M ( H ) consists of T ∈ E ( H ) such that for any k , ℓ ∈ N 0 there exist s , t ∈ N 0 such that (11) | D 0 , 0 k , ℓ T ( α , β ) | ≲ ( α + 1 / α ) s ( 1 + β 2 ) t / 2 . The space D ′ ( H ) consists of all bounded linear functionals Φ on D ( H ) such that for every compact set K ⊂ H , there exists N ∈ N 0 such that (12) | ∫ K T ( α , β ) Φ ( α , β ) d α d β α | ≲ ∑ k , ℓ ≤ N sup ( α , β ) ∈ H ⁡ | D 0 , 0 k , ℓ T ( α , β ) | , ∀ T ∈ D ( K ) where the integral is understood as the action of Φ. The space S ′ ( H ) consists of Φ ∈ S ( H ) for which there exists N ∈ N 0 such that (13) | ∫ H T ( α , β ) Φ ( α , β ) d α d β α | ≲ ∑ s , t , k , ℓ ≤ N sup ( α , β ) ∈ H ⁡ | D s , t k , ℓ T ( α , β ) | , ∀ T ∈ S ( H ) .
2.3. Convolution of distributions

Table 3 lists the convergent convolutions of distributions and their ranges by Schwartz [32] .

Table 3 . Range of convolution (excerpt from Schwartz [32] ).
Case	A 1 	A 2 	⁎ A 1 ⁎ A 2
Regularization	D 	D ′ , D L p ′ , E ′ 	E , L p , D
Compactly supported distribution	E ′ 	E ′ , E , D ′ 	E ′ , E , D ′
Regularization	S 	S , S ′ 	S , O M
Schwartz convolutor	O C ′ 	S , O C ′ , D L p ′ , S ′ 	S , O C ′ , D L p ′ , S ′
Young's inequality	L p 	L q 	L r (1/ r  = 1/ p  + 1/ q  − 1)
Young's inequality	D L p ′ 	D L q , D L q ′ 	D L r ′ ( 1 / r = 1 / p + 1 / q − 1 )

In general a convolution of distributions may neither commute ⁎ ⁎ ϕ ⁎ ψ ≠ ψ ⁎ ϕ nor associate ⁎ ⁎ ⁎ ⁎ ϕ ⁎ ( ψ ⁎ η ) ≠ ( ϕ ⁎ ψ ) ⁎ η . According to Schwartz [32, Ch. 6, Th. 7, Ch. 7, Th. 7] , both ⁎ ⁎ ⁎ D ′ ⁎ E ′ ⁎ E ′ ⁎ ⋯ and ⁎ ⁎ ⁎ S ′ ⁎ O C ′ ⁎ O C ′ ⁎ ⋯ are commutative and associative.
2.4. Fourier analysis

The Fourier transform ⋅ ˆ of f : R m → C and the inverse Fourier transform ⋅ ˇ of F : R m → C are given by (14) f ˆ ( ξ ) : = ∫ R m f ( x ) e − i x ⋅ ξ d x , ξ ∈ R m (15) F ˇ ( x ) : = 1 ( 2 π ) m ∫ R m F ( ξ ) e i x ⋅ ξ d ξ , x ∈ R m .

The Hilbert transform H of f : R → C is given by (16) H f ( s ) : = i π p . v . ∫ − ∞ ∞ f ( t ) s − t d t , s ∈ R where p . v . ∫ − ∞ ∞ denotes the principal value . We set the coefficients above to satisfy (17) H f ˆ ( ω ) = sgn ω ⋅ f ˆ ( ω ) , (18) H 2 f ( s ) = f ( s ) .
2.5. Radon transform

The Radon transform R of f : R m → C and the dual Radon transform ⁎ R ⁎ of Φ : S m − 1 × R → C are given by (19) R f ( u , p ) : = ∫ ( R u ) ⊥ f ( p u + y ) d y , ( u , p ) ∈ S m − 1 × R (20) ⁎ R ⁎ Φ ( x ) : = ∫ S m − 1 Φ ( u , u ⋅ x ) d u , x ∈ R m where ( R u ) ⊥ : = { y ∈ R m | y ⋅ u = 0 } denotes the orthogonal complement of a line R u ⊂ R m ; and d y denotes the Lebesgue measure on ( R u ) ⊥ ; and d u denotes the surface measure on S m − 1 .

We use the following fundamental results [17] , [39] for f ∈ L 1 ( R m ) without proof: Radon's inversion formula (21) ⁎ R ⁎ Λ m − 1 R f = 2 ( 2 π ) m − 1 f , where the backprojection filter Λ m is defined in (24) ; the Fourier slice theorem (22) f ˆ ( ω u ) = ∫ R R f ( u , p ) e − i p ω d p , ( u , ω ) ∈ S m − 1 × R where the left-hand side is the m -dimensional Fourier transform, whereas the right-hand side is the one-dimensional Fourier transform of the Radon transform; and a corollary of Fubini's theorem (23) ∫ R R f ( u , p ) d p = ∫ R m f ( x ) d x , a.e.  u ∈ S m − 1 .
2.6. Backprojection filter

For a function Φ ( u , p ) , we define the backprojection filter Λ m as (24) Λ m Φ ( u , p ) : = { ∂ p m Φ ( u , p ) , m even H p ∂ p m Φ ( u , p ) , m odd . where H p and ∂ p denote the Hilbert transform and the partial differentiation with respect to p , respectively. It is designed as a one-dimensional Fourier multiplier with respect to p → ω such that (25) Λ m Φ ˆ ( u , ω ) = i m | ω | m Φ ˆ ( u , ω ) .
3. Classical ridgelet transform
3.1. An overview

The ridgelet transform R ψ f of f : R m → C with respect to ψ : R → C is formally given by (26) R ψ f ( a , b ) : = ∫ R m f ( x ) ψ ( a ⋅ x − b ) ‾ ‖ a ‖ s d x , ( a , b ) ∈ Y m + 1  and  s > 0 . The factor | a | s is simply posed for technical convenience. After the next section we set s = 1 , which simplifies some notations (e.g., Theorem 4.2 ). Murata [1] originally posed s = 0 , which is suitable for the Euclidean formulation. Other authors such as Candès [24] used s = 1 / 2 , Rubin [25] used s = m , and Kostadinova et al. [30] used s = 1 .

When f ∈ L 1 ( R m ) and ψ ∈ L ∞ ( R ) , by using Hölder's inequality , the ridgelet transform is absolutely convergent at every ( a , b ) ∈ Y m + 1 , (27) ∫ R m | f ( x ) ψ ( a ⋅ x − b ) ‾ ‖ a ‖ s | d x ≤ ‖ f ‖ L 1 ( R m ) ⋅ ‖ ψ ‖ L ∞ ( R ) ⋅ ‖ a ‖ s < ∞ . In particular when s = 0 , the estimate is independent of a and thus R ψ f ∈ L ∞ ( Y m + 1 ) . Furthermore, R is a bounded bilinear operator L 1 ( R m ) × L ∞ ( R ) → L ∞ ( Y m + 1 ) .

The dual ridgelet transform R η † T of T : Y m + 1 → C with respect to η : R → C is formally given by (28) R η † T ( x ) : = ∫ Y m + 1 T ( a , b ) η ( a ⋅ x − b ) ‖ a ‖ − s d a d b , x ∈ R m . The integral is absolutely convergent when η ∈ L ∞ ( R ) and T ∈ L 1 ( Y m + 1 ; ‖ a ‖ − s d a d b ) at every x ∈ R m , (29) ∫ Y m + 1 | T ( a , b ) η ( a ⋅ x − b ) | ‖ a ‖ − s d a d b ≤ ‖ T ‖ L 1 ( Y m + 1 ; ‖ a ‖ − s d a d b ) ⋅ ‖ η ‖ L ∞ ( R ) < ∞ , and thus R † is a bounded bilinear operator L 1 ( Y m + 1 ; ‖ a ‖ − s d a d b ) × L ∞ ( R ) → L ∞ ( R m ) .

Two functions ψ and η are said to be admissible when (30) K ψ , η : = ( 2 π ) m − 1 ∫ − ∞ ∞ ψ ˆ ( ζ ) ‾ η ˆ ( ζ ) | ζ | m d ζ , is finite and not zero. Provided that ψ , η , and f belong to some good classes, and ψ and η are admissible, then the reconstruction formula (31) R η † R ψ f = K η , ψ f , holds.
3.2. Ridgelet transform in other expressions

It is convenient to write the ridgelet transform in “polar” coordinates as (32) R ψ f ( u , α , β ) = ∫ R m f ( x ) ψ ( u ⋅ x − β α ) ‾ 1 α s d x , where “polar” variables are given by (33) u : = a / ‖ a ‖ , α : = 1 / ‖ a ‖ , β : = b / ‖ a ‖ . Emphasizing the connection with wavelet analysis , we define the “radius” α as reciprocal. Provided there is no likelihood of confusion, we use the same symbol Y m + 1 for the parameter space, regardless of whether it is parametrized by ( a , b ) ∈ R m × R or ( u , α , β ) ∈ S m − 1 × R + × R .

For a fixed ( u , α , β ) ∈ Y m + 1 , the ridgelet function (34) ψ u , α , β ( x ) : = ψ ( u ⋅ x − β α ) 1 α s , x ∈ R m behaves as a constant function on ( R u ) ⊥ , and as a dilated and translated wavelet function on R u . That is, by using the orthogonal decomposition x = p u + y with p ∈ R and y ∈ ( R u ) ⊥ , (35) ψ u , α , β ( x ) = ψ ( u ⋅ ( p u + y ) − β α ) 1 α s = ψ ( p − β α ) 1 α s ⊗ 1 ( y ) .

By using the decomposition above and Fubini's theorem , and assuming that the ridgelet transform is absolutely convergent, we have the following equivalent expressions (36) R ψ f ( u , α , β ) = ∫ R ( ∫ ( R u ) ⊥ f ( p u + y ) d y ) ψ ( p − β α ) ‾ 1 α s d p (37) = ∫ R R f ( u , p ) ψ ( p − β α ) ‾ 1 α s d p (38) = ∫ R α 1 − s R f ( u , α z + β ) ψ ( z ) ‾ d z (weak form) (39) ⁎ = ( R f ( u , ⋅ ) ⁎ ψ α ˜ ‾ ) ( β ) , ψ α ( p ) : = ψ ( p α ) 1 α s (convolution form) (40) = 1 2 π ∫ R f ˆ ( ω u ) ψ ˆ ( α ω ) ‾ α 1 − s e i ω β d ω , (Fourier slice th. [30] ) where R denotes the Radon transform (19) ; the Fourier form follows by applying the identity F ω − 1 F p = Id to the convolution form . These reformulations reflect a well-known claim [28] , [30] that ridgelet analysis is wavelet analysis in the Radon domain.
3.3. Dual ridgelet transform in other expressions

Provided the dual ridgelet transform (28) is absolutely convergent, some changes of variables lead to other expressions. (41) R η † T ( x ) = ∫ R m ∫ R T ( a , b ) η ( a ⋅ x − b ) ‖ a ‖ − s d b d a (42) = ∫ 0 ∞ ∫ S m − 1 ∫ R T ( r u , b ) η ( r u ⋅ x − b ) d b d u r m − s − 1 d r (43) = ∫ S m − 1 ∫ 0 ∞ ∫ R T ( u α , β α ) η ( u ⋅ x − β α ) d β d α d u α m − s + 2 (polar expression) (44) = ∫ S m − 1 ∫ 0 ∞ ∫ R T ( u , α , u ⋅ x − α z ) η ( z ) d z d α d u α m − s + 1 , (weak form) where every integral is understood to be an iterated integral; the second equation follows by substituting ( r , u ) ← ( ‖ a ‖ , a / ‖ a ‖ ) and using the coarea formula for polar coordinates; the third equation follows by substituting ( α , β ) ← ( 1 / r , b / r ) and using Fubini's theorem; in the fourth equation with a slight abuse of notation, we write T ( u , α , β ) : = T ( u / α , β / α ) .

Furthermore, write η α ( p ) : = η ( p / α ) / α t . Recall that the dual Radon transform ⁎ R ⁎ is given by (20) and the Mellin transform M [38] is given by M f ( z ) : = ∫ 0 ∞ f ( α ) α z − 1 d α , z ∈ C . Then, (45) ⁎ ⁎ R η † T ( x ) = R ⁎ [ M [ T ( u , α , ⋅ ) ⁎ η α ] ( s + t − m − 1 ) ] ( x ) . Note that the composition of the Mellin transform and the convolution is the dual wavelet transform [38] . Thus, the dual ridgelet transform is the composition of the dual Radon transform and the dual wavelet transform.
4. Ridgelet transform with respect to distributions

Using the weak expressions (38) and (44) , we define the ridgelet transform with respect to distributions. Henceforth, we focus on the case for which the index s in (26) equals 1.
4.1. Definition and well-definedness

Definition 4.1 Ridgelet transform with respect to distributions

The ridgelet transform R ψ f of a function f ∈ X ( R m ) with respect to a distribution ψ ∈ Z ( R ) is given by (46) R ψ f ( u , α , β ) : = ∫ R R f ( u , α z + β ) ψ ( z ) ‾ d z , ( u , α , β ) ∈ Y m + 1 where ∫ R ⋅ ψ ( z ) ‾ d z is understood as the action of a distribution ψ .

Obviously, this “weak” definition coincides with the ordinary strong one when ψ coincides with a locally integrable function ( L loc 1 ) . With a slight abuse of notation, the weak definition coincides with the convolution form (47) ⁎ R ψ f ( u , α , β ) = ( R f ( u , ⋅ ) ⁎ ψ α ˜ ‾ ) ( β ) , ( u , α , β ) ∈ Y m + 1 where ψ α ( p ) : = ψ ( p / α ) / α ; the convolution ⁎ ⋅ ⁎ ⋅ , dilation ⋅ α , reflection ⋅ ˜ , and complex conjugation ⋅ ‾ are understood as operations for Schwartz distributions .

Theorem 4.2 Balancing theorem

The ridgelet transform R : X ( R m ) × Z ( R ) → Y ( Y m + 1 ) is well defined as a bilinear map when X and Z are chosen from Table 4 .

Table 4 . Combinations of classes for which the ridgelet transform is well defined as a bilinear map. The first and third columns list domains X ( R m ) of f and Z ( R ) of ψ , respectively. The second column lists the range of the Radon transform R f ( u , p ) for which we reused the same symbol X as it coincides. The fourth, fifth, and sixth columns list the range of the ridgelet transform with respect to β , ( α , β ), and ( u , α , β ), respectively.
f ( x )	R f ( u , p )	ψ ( z )	R ψ f ( u , α , β )
X ( R m ) 	X ( S m − 1 × R ) 	Z ( R ) 	B ( R ) 	A ( H ) 	Y ( Y m + 1 )
D 	D 	D ′ 	E 	E 	E
E ′ 	E ′ 	D ′ 	D ′ 	D ′ 	D ′
S 	S 	S ′ 	O M 	O M 	O M
O C ′ 	O C ′ 	S ′ 	S ′ 	S ′ 	S ′
L 1 	L 1 	L p  ∩  C 0 	L p  ∩  C 0 	S ′ 	S ′
D L 1 ′ 	D L 1 ′ 	D L p ′ 	D L p ′ 	S ′ 	S ′

The proof is provided in Appendix A . Note that each Z is (almost) the largest in the sense that the convolution ⁎ B = X ⁎ Z converges. Thus, Table 4 suggests that there is a trade-off relation between X and Z , that is, as X increases, Z decreases and vice versa.

Extension of the ridgelet transform of non-integrable functions requires more sophisticated approaches, because a direct computation of the Radon transform may diverge. For instance, Kostadinova et al. [30] extend X = S 0 ′ by using a duality technique. In § 5.3 we extend the ridgelet transform to L 2 ( R m ) , by using the bounded extension procedure.

Proposition 4.3
Continuity of the ridgelet transform L 1 ( R m ) → L ∞ ( Y m + 1 )

Fix ψ ∈ S ( R ) . The ridgelet transform R ψ : L 1 ( R m ) → L ∞ ( Y m + 1 ) is bounded.

Proof

Fix an arbitrary f ∈ L 1 ( R m ) and ψ ∈ S ( R ) . Recall that this case is absolutely convergent . By using the convolution form, (48) ⁎ ess sup ( u , α , β ) | ( R f ( u , ⋅ ) ⁎ ψ α ˜ ‾ ) ( β ) | ≤ ‖ f ‖ L 1 ( R m ) ⋅ ess sup ( α , β ) | ψ α ( β ) | (49) ≤ ‖ f ‖ L 1 ( R m ) ⋅ ess sup ( r , β ) | r ⋅ ψ ( r β ) | < ∞ , where the first inequality follows by using Young's inequality and applying ∫ R | R f ( u , p ) | d p ≤ ‖ f ‖ 1 ; the second inequality follows by changing the variable r ← 1 / α , and the resultant is finite because ψ decays rapidly.  □

The ridgelet transform R ψ is injective when ψ is admissible , because if ψ is admissible then the reconstruction formula holds and thus R ψ has the inverse. However, R ψ is not always injective. For instance, take a Laplacian f : = Δ g of some function g ∈ S ( R m ) and a polynomial ψ ( z ) = z + 1 , which satisfies ψ ( 2 ) ≡ 0 . According to Table 4 , R ψ f exists as a smooth function because f ∈ S ( R m ) and ψ ∈ S ′ ( R ) . In this case R ψ f = 0 , which means R ψ is not injective. That is, (50) ⁎ R ψ f ( u , α , β ) = ( R Δ g ( u , ⋅ ) ⁎ ψ α ˜ ‾ ) ( β ) (51) ⁎ = ( ∂ 2 R g ( u , ⋅ ) ⁎ ψ α ˜ ‾ ) ( β ) (52) ⁎ = ( R g ( u , ⋅ ) ⁎ ∂ 2 ψ α ˜ ‾ ) ( β ) (53) ⁎ = ( R g ( u , ⋅ ) ⁎ 0 ) ( β ) (54) = 0 , where the second equality follows by the intertwining relation R Δ g ( u , p ) = ∂ p 2 R g ( u , p ) [17] . Clearly the non-injectivity stems from the choice of ψ . In fact, as we see in the next section, no polynomial can be admissible and thus R ψ is not injective for any polynomial ψ .
4.2. Dual ridgelet transform with respect to distributions

Definition 4.4 Dual ridgelet transform with respect to distributions

The dual ridgelet transform R η † T of T ∈ Y ( Y m + 1 ) with respect to η ∈ W ( R ) is given by (55) R η † T ( x ) = lim δ → ∞ ε → 0 ⁡ ∫ S m − 1 ∫ ε δ ∫ R T ( u , α , u ⋅ x − α z ) η ( z ) d z d α d u α m , x ∈ R m where ∫ R ⋅ η ( z ) d z is understood as the action of a distribution η .

If the dual ridgelet transform R η † exists, then it coincides with the dual operator [36] of the ridgelet transform R η .

Theorem 4.5

Let X and Z be chosen from Table 4 . Fix ψ ∈ Z . Assume that R ψ : X ( R m ) → Y ( Y m + 1 ) is injective and that R ψ † : Y ′ ( Y m + 1 ) → X ′ ( R m ) exists. Then R ψ † is the dual operator ( R ψ ) ′ : Y ′ ( Y m + 1 ) → X ′ ( R m ) of R ψ .

Proof

By assumption R ψ is densely defined on X ( R m ) and injective. Therefore, by a classical result on the existence of the dual operator [36, VII. 1. Th. 1, pp.193] , there uniquely exists a dual operator ( R ψ ) ′ : Y ′ ( Y m + 1 ) → X ′ ( R m ) . On the other hand, for f ∈ X ( R m ) and T ∈ Y ( Y m + 1 ) , (56) 〈 R ψ f , T 〉 Y m + 1 = ∫ R m × Y m + 1 f ( x ) ψ ( a ⋅ x − b ) T ( a , b ) ‾ d x d a d b = 〈 f , R ψ † T 〉 R m . By the uniqueness of the dual operator, we can conclude ( R ψ ) ′ = R ψ † .  □

5. Reconstruction formula for weak ridgelet transform

In this section we discuss the admissibility condition and the reconstruction formula, not only in the Fourier domain as many authors did [23] , [24] , [1] , [30] , [31] , but also in the real domain and in the Radon domain. Both domains are key to the constructive formulation. In § 5.1 we derive a constructive admissibility condition. In § 5.2 we show two reconstruction formulas. The first of these formulas is obtained by using the Fourier slice theorem and the other by using the Radon transform. In § 5.3 we will extend the ridgelet transform to L 2 .
5.1. Admissibility condition

Definition 5.1 Admissibility condition

A pair ( ψ , η ) ∈ S ( R ) × S ′ ( R ) is said to be admissible when there exists a neighborhood Ω ⊂ R of 0 such that η ˆ ∈ L loc 1 ( Ω ∖ { 0 } ) , and the integral (57) K ψ , η : = ( 2 π ) m − 1 ( ∫ Ω ∖ { 0 } + ∫ R ∖ Ω ) ψ ˆ ( ζ ) ‾ η ˆ ( ζ ) | ζ | m d ζ , converges and is not zero, where ∫ Ω ∖ { 0 } and ∫ R ∖ Ω are understood as Lebesgue's integral and the action of η ˆ , respectively.

Using the Fourier transform in W requires us to assume that W ⊂ S ′ .

The second integral ∫ R ∖ Ω is always finite because | ζ | − m ∈ O M ( R ∖ Ω ) and thus | ζ | − m ψ ˆ ( ζ ) ‾ decays rapidly; therefore, by definition the action of a tempered distribution η ˆ always converges. The convergence of the first integral ∫ Ω ∖ { 0 } does not depend on the choice of Ω because for every two neighborhoods Ω and Ω ′ of 0, the residual ∫ Ω ∖ Ω ′ is always finite. Hence, the convergence of K ψ , η does not depend on the choice of Ω.

The removal of 0 from the integral is essential because a product of two singular distributions , which is indeterminate in general, can occur at 0. See examples below. In Appendix C , we have to treat | ζ | − m as a locally integrable function , rather than simply a regularized distribution such as Hadamard's finite part. If the integrand coincides with a function at 0, then obviously ∫ R ∖ { 0 } = ∫ R .

If η ˆ is supported in the singleton {0} then η cannot be admissible because K ψ , η = 0 for any ψ ∈ S ( R ) . According to Rudin [34, Ex. 7.16] , it happens if and only if η is a polynomial. Therefore, it is natural to take W = S ′ / P ≅ S 0 ′ rather than W = S ′ . That is, in S 0 ′ ( R ) , we identify a polynomial η ∈ P ( R ) as 0 ∈ S ′ ( R ) . The integral K ψ , η is well-defined for S 0 ′ ( R ) . Namely K ψ , η is invariant under the addition of a polynomial Q to η (58) K ψ , η = K ψ , η + Q .

Example 5.2
Modification of Schwartz [32, Ch. 5, Th. 6]

Let η ( z ) = z and ψ ( z ) = Λ G ( z ) with G ( z ) = exp ⁡ ( − z 2 / 2 ) . Then, (59) η ˆ ( ζ ) = δ ( ζ )  and  ψ ˆ ( ζ ) ‾ = | ζ | ⋅ G ( ζ ) . In this case the product of the two distributions is not associative (60) ∫ R p . v . 1 | ζ | × ( | ζ | ⋅ G ( ζ ) × δ ( ζ ) ) d ζ = 0 , (61) ∫ R ( p . v . 1 | ζ | × | ζ | ⋅ G ( ζ ) ) × δ ( ζ ) d ζ = G ( 0 ) ≠ 0 . On the other hand (57) is well defined (62) K ψ , η = ∫ 0 < | ζ | < 1 | ζ | ⋅ G ( ζ ) × 0 | ζ | d ζ + ∫ 1 ≤ | ζ | | ζ | ⋅ G ( ζ ) | ζ | δ ( ζ ) d ζ = 0 .

Example 5.3

Let η ( z ) = z + 0 + ( 2 π ) − 1 exp ⁡ i z and ψ ( z ) = Λ G ( z ) . Then, (63) η ˆ ( ζ ) = i ζ + δ ( ζ ) + δ ( ζ − 1 )  and  ψ ˆ ( ζ ) ‾ = | ζ | ⋅ G ( ζ ) . The product of the two distributions is not associative (64) ∫ R p . v . 1 | ζ | × ( | ζ | ⋅ G ( ζ ) × ( i ζ + δ ( ζ ) + δ ( ζ − 1 ) ) ) d ζ = G ( 1 ) , (65) ∫ R ( p . v . 1 | ζ | × | ζ | ⋅ G ( ζ ) ) × ( i ζ + δ ( ζ ) + δ ( ζ − 1 ) ) d ζ = G ( 0 ) + G ( 1 ) ≠ 0 . On the other hand, (57) is well defined (66) K ψ , η = ∫ 0 < | ζ | < 1 | ζ | ⋅ G ( ζ ) × i ζ − 1 | ζ | d ζ + ∫ 1 ≤ | ζ | | ζ | ⋅ G ( ζ ) | ζ | ( p . v . i ζ + δ ( ζ ) + δ ( ζ − 1 ) ) d ζ = ∞ + G ( 1 ) .

Observe that formally the integrand u ˆ ( ζ ) : = ψ ˆ ( ζ ) ‾ η ˆ ( ζ ) | ζ | − m is a solution of | ζ | m u ˆ ( ζ ) = ψ ˆ ( ζ ) ‾ η ˆ ( ζ ) . By taking the Fourier inversion, we have ⁎ Λ m u = ψ ˜ ‾ ⁎ η . To be exact, η ˆ may contain a point mass at the origin, such as Dirac's δ .

Theorem 5.4 Structure theorem for admissible pairs

Let ( ψ , η ) ∈ S ( R ) × S ′ ( R ) . Assume that there exists k ∈ N 0 such that (67) η ˆ ( ζ ) = ∑ j = 0 k c j δ ( j ) ( ζ ) , ζ ∈ { 0 } . Assume that there exists a neighborhood Ω of 0 such that η ˆ ∈ C 0 ( Ω ∖ { 0 } ) . Then ψ and η are admissible if and only if there exists u ∈ O M ( R ) such that (68) ⁎ Λ m u = ψ ˜ ‾ ⁎ ( η − ∑ j = 0 k c j z j )   and   ∫ R ∖ { 0 } u ˆ ( ζ ) d ζ ≠ 0 , where Λ is the backprojection filter defined in (24) . In addition, lim ζ → + 0 ⁡ | u ˆ ( ζ ) | < ∞ and lim ζ → − 0 ⁡ | u ˆ ( ζ ) | < ∞ .

The proof is provided in Appendix B . Note that the continuity implies local integrability. If ψ has ℓ vanishing moments with ℓ ≥ k , namely ∫ R ψ ( z ) z j d z = 0 for j ≤ ℓ , then the condition reduces to (69) ⁎ Λ m u = ψ ˜ ‾ ⁎ η , | ∫ R u ( z ) d z | < ∞  and  ∫ R u ˆ ( ζ ) d ζ ≠ 0 .

As a consequence of Theorem 5.4 , we can construct admissible pairs as below.

Corollary 5.5 Construction of admissible pairs

Given η ∈ S 0 ′ ( R ) . Assume that there exists a neighborhood Ω of 0 and k ∈ N 0 such that ζ k ⋅ η ˆ ( ζ ) ∈ C 0 ( Ω ) . Take ψ 0 ∈ S ( R ) such that (70) ∫ R ζ k ψ 0 ˆ ( ζ ) ‾ η ˆ ( ζ ) d ζ ≠ 0 . Then (71) ψ : = Λ m ψ 0 ( k ) , is admissible with η.

The proof is obvious because ⁎ u : = ψ 0 ( k ) ˜ ‾ ⁎ η satisfies the conditions in Theorem 5.4 .
5.2. Reconstruction formula

Theorem 5.6 Reconstruction formula

Let f ∈ L 1 ( R m ) satisfy f ˆ ∈ L 1 ( R m ) and let ( ψ , η ) ∈ S ( R ) × S 0 ′ ( R ) be admissible. Then the reconstruction formula (72) R η † R ψ f ( x ) = K ψ , η f ( x ) , holds for almost every x ∈ R m . The equality holds for every point where f is continuous.

The proof is provided in Appendix C . The admissibility condition can be easily inverted to ( ψ , η ) ∈ S 0 ′ × S . However, extensions to S 0 ′ × S 0 ′ and S × D ′ may not be easy. This is because the multiplication S 0 ′ ⋅ S 0 ′ is not always commutative, nor associative, and the Fourier transform is not always defined over D ′ [32] .

The following theorem is another suggestive reconstruction formula that implies wavelet analysis in the Radon domain works as a backprojection filter. In other words, the admissibility condition requires ( ψ , η ) to construct the filter Λ m . Note that similar techniques are obtained for “wavelet measures” by Rubin [29] , [25] .

Theorem 5.7 Reconstruction formula via radon transform

Let f ∈ L 1 ( R m ) be sufficiently smooth and ( ψ , η ) ∈ S ( R ) × S ′ ( R ) be admissible. Assume that there exists a real-valued smooth and integrable function u such that (73) ⁎ Λ m u = ψ ˜ ‾ ⁎ η   and   ∫ R u ˆ ( ζ ) d ζ = − 1 . Then, (74) ⁎ R η † R ψ f ( x ) = R ⁎ Λ m − 1 R f ( x ) = 2 ( 2 π ) m − 1 f ( x ) , holds for almost every x ∈ R m .

The proof is provided in Appendix D . Note that here we imposed a stronger condition on u than the u ∈ L 1 ( R ∖ { 0 } ) we imposed in Theorem 5.4 .

Recall intertwining relations ( [17, Lem. 2.1, Th. 3.1, Th. 3.7] ) (75) ⁎ ⁎ ( − Δ ) m − 1 2 R ⁎ = Λ m − 1 R ,  and  R ( − Δ ) m − 1 2 = R ⁎ Λ m − 1 . Therefore, we have the following.

Corollary 5.8

(76) ⁎ ⁎ ⁎ R η † R ψ = R ⁎ Λ m − 1 R = ( − Δ ) m − 1 2 R ⁎ R = R ⁎ R ( − Δ ) m − 1 2 .

5.3. Extension to L 2

By ( ⋅ , ⋅ ) and ‖ ⋅ ‖ 2 , with a slight abuse of notation, we denote the inner product of L 2 ( R m ) and L 2 ( Y m + 1 ) . Here we endow Y m + 1 with a fixed measure α − m d α d β d u , and omit writing it explicitly as L 2 ( Y m + 1 ; … ) . We say that ψ is self-admissible if ψ is admissible in itself, i.e. the pair ( ψ , ψ ) is admissible. The following relation is immediate by the duality.

Theorem 5.9 Parseval's relation and Plancherel's identity

Let ( ψ , η ) ∈ S × S ′ be admissible with, for simplicity, K ψ , η = 1 . For f , g ∈ L 1 ∩ L 2 ( R m ) , (77) ( R ψ f , R η g ) = ( R η † R ψ f , g ) = ( f , g ) . Parseval's Relation In particular, if ψ is self-admissible, then (78) ‖ R ψ f ‖ 2 = ‖ f ‖ 2 . Plancherel's identity

Recall Proposition 4.3 that the ridgelet transform is a bounded linear operator on L 1 ( R m ) . If ψ ∈ S ( R ) is self-admissible, then we can extend the ridgelet transform to L 2 ( R m ) , by following the bounded extension procedure [40, 2.2.4] . That is, for f ∈ L 2 ( R m ) , take a sequence f n ∈ L 1 ∩ L 2 ( R m ) such that f n → f in L 2 . Then by Plancherel's identity, (79) ‖ f n − f m ‖ 2 = ‖ R ψ f n − R ψ f m ‖ 2 , ∀ n , m ∈ N . The right-hand side is a Cauchy sequence in L 2 ( Y m + 1 ) as n , m → ∞ . By the completeness, there uniquely exists the limit T ∞ ∈ L 2 ( Y m + 1 ) of R ψ f n . We regard T ∞ as the ridgelet transform of f and define R ψ f : = T ∞ .

Theorem 5.10
Bounded extension of ridgelet transform on L 2

Let ψ ∈ S ( R ) be self-admissible with K ψ , ψ = 1 . The ridgelet transform on L 1 ∩ L 2 ( R m ) admits a unique bounded extension to L 2 ( R m ) , with satisfying ‖ R ψ f ‖ 2 = ‖ f ‖ 2 .

We say that ( ψ , η ) and ( ψ ⋆ , η ⋆ ) are equivalent , if two admissible pairs ( ψ , η ) and ( ψ ⋆ , η ⋆ ) define the same convolution ⁎ ⁎ ψ ˜ ‾ ⁎ η = ψ ⋆ ˜ ‾ ⁎ η ⋆ in common. If ( ψ , η ) and ( ψ ⋆ , η ⋆ ) are equivalent, then obviously (80) ( R ψ f , R η g ) = ( R ψ ⋆ f , R η ⋆ g ) . We say that an admissible pair ( ψ , η ) is admissibly decomposable , when there exist self-admissible pairs ( ψ ⋆ , ψ ⋆ ) and ( η ⋆ , η ⋆ ) such that ( ψ ⋆ , η ⋆ ) is equivalent to ( ψ , η ) . If ( ψ , η ) is admissibly decomposable with ( ψ ⋆ , η ⋆ ) , then by the Schwartz inequality (81) ( R ψ f , R η g ) ≤ ‖ R ψ ⋆ f ‖ 2 ‖ R η ⋆ g ‖ 2 .

Theorem 5.11
Reconstruction formula in L 2

Let f ∈ L 2 ( R m ) and ( ψ , η ) ∈ S × S ′ be admissibly decomposable with K ψ , η = 1 . Then, (82) R η † R ψ f → f , in L 2 .

The proof is provided in Appendix E . Even when ψ is not self-admissible and thus R ψ cannot be defined on L 2 ( R m ) , the reconstruction operator R η † R ψ can be defined with the aid of η .
6. Neural network with unbounded activation functions

In this section we instantiate the universal approximation property for the variants of neural networks. Recall that a neural network coincides with the dual ridgelet transform of a function. Henceforth, we rephrase a dual ridgelet function as an activation function. According to the reconstruction formulas ( Theorem 5.6 , Theorem 5.7 , Theorem 5.11 ), we can determine whether a neural network with an activation function η is a universal approximator by checking the admissibility of η .

Table 1 lists some Lizorkin distributions for potential activation functions. In § 6.1 we verify that they belong to S 0 ′ ( R ) and some of them belong to O M ( R ) and S ( R ) , which are subspaces of S 0 ′ ( R ) . In § 6.2 we show that they are admissible with some ridgelet function ψ ∈ S ( R ) ; therefore, each of their corresponding neural networks is a universal approximator.
6.1. Examples of Lizorkin distributions

We proved the class properties by using the following propositions.

Proposition 6.1
Tempered distribution S ′ ( R ) [40, Ex. 2.3.5]

Let g ∈ L loc 1 ( R ) . If | g ( z ) | ≲ ( 1 + | z | ) k for some k ∈ N 0 , then g ∈ S ′ ( R ) .

Proposition 6.2
Slowly increasing function O M ( R ) [40, Def. 2.3.15]

Let g ∈ E ( R ) . If for any α ∈ N 0 , | ∂ α g ( x ) | ≲ ( 1 + | z | ) k α for some k α ∈ N 0 , then g ∈ O M ( R ) .

Example 6.3

Truncated power functions z + k ( k ∈ N 0 ) , which contain the ReLU z + and the step function z + 0 , belong to S 0 ′ ( R ) .

Proof

For any ℓ ∈ N 0 there exists a constant C ℓ such that | ∂ ℓ ( z + k ) | ≤ C ℓ ( 1 + | z | ) k − ℓ . Hence, z + k ∈ S 0 ′ ( R ) .  □

Example 6.4

The sigmoidal function σ ( z ) and the softplus σ ( − 1 ) ( z ) belong to O M ( R ) . The derivatives σ ( k ) ( z ) ( k ∈ N ) belong to S ( R ) . Hyperbolic tangent tanh ⁡ ( z ) belongs to O M ( R ) .

The proof is provided in Appendix F .

Example 6.5

(See [40, Ex. 2.2.2] .) RBF G ( z ) and their derivatives G ( k ) ( z ) belong to S ( R ) .

Example 6.6

(See [40, Ex. 2.3.5] .) Dirac's δ ( z ) and their derivatives δ ( k ) ( z ) belong to S ′ ( R ) .

6.2. K ψ , η when ψ is a derivative of the Gaussian

Given an activation function η ∈ S 0 ′ ( R ) , according to Corollary 5.5 we can construct an admissible ridgelet function ψ ∈ S ( R ) by letting (83) ψ : = Λ m ψ 0 , where ψ 0 ∈ S ( R ) satisfies (84) 〈 η ˆ , ψ 0 ˆ 〉 : = ∫ R ∖ { 0 } ψ 0 ˆ ( ζ ) ‾ η ˆ ( ζ ) d ζ ≠ 0 , ± ∞ .

Here we consider the case when ψ 0 is given by (85) ψ 0 = G ( ℓ ) , for some ℓ ∈ N 0 , where G denotes the Gaussian G ( z ) : = exp ⁡ ( − z 2 / 2 ) .

The Fourier transform of the Gaussian is given by G ˆ ( ζ ) = exp ⁡ ( − ζ 2 / 2 ) = 2 π G ( ζ ) . The Hilbert transform of the Gaussian, which we encounter by computing ψ = Λ m G when m is odd, is given by (86) H G ( z ) = 2 i π F ( z 2 ) , where F ( z ) is the Dawson function F ( z ) : = exp ⁡ ( − z 2 ) ∫ 0 z exp ⁡ ( w 2 ) d w .

Example 6.7

z + k ( k ∈ N 0 ) is admissible with ψ = Λ m G ( ℓ + k + 1 ) ( ℓ ∈ N 0 ) iff ℓ is even. If odd, then K ψ , η = 0 .

Proof

It follows from the fact that, according to Gel'fand and Shilov [41, § 9.3] , (87) z + k ˆ ( ζ ) = k ! ( i ζ ) k + 1 + π i k δ ( k ) ( ζ ) , k ∈ N 0 .  □

Example 6.8

η ( z ) = δ ( k ) ( z ) ( k ∈ N 0 ) is admissible with ψ = Λ m G iff k is even. If odd, then K ψ , η = 0 .

In contrast to polynomial functions , Dirac's δ can be an admissible activation function.

Example 6.9

η ( z ) = G ( k ) ( z ) ( k ∈ N 0 ) is admissible with ψ = Λ m G iff k is even. If odd, then K ψ , η = 0 .

Example 6.10

η ( z ) = σ ( k ) ( z ) ( k ∈ N 0 ) is admissible with ψ = Λ m G iff k is odd. If odd, then K ψ , η = 0 . σ ( − 1 ) is admissible with ψ = Λ m G ″ .

The proof is provided in Appendix F .
7. Numerical examples of reconstruction

We performed some numerical experiments on reconstructing a one-dimensional signal and a two-dimensional image, with reference to our theoretical diagnoses for admissibility in the previous section. Table 5 lists the diagnoses of ( Λ m ψ 0 , η ) we employ in this section. The symbols ‘+,’ ‘0,’ and ‘∞’ in each cell indicate that K ψ , η of the corresponding ( ψ , η ) converges to a non-zero constant (+), converges to zero (0), and diverges (∞). Hence, by Theorem 5.6 , if the cell ( ψ , η ) indicates ‘+’ then a neural network with an activation function η is a universal approximator.

Table 5 . Theoretical diagnoses for admissibility of ψ  = Λ m ψ 0 and η . ‘+’ indicates that ( ψ , η ) is admissible. ‘0’ and ‘∞’ indicate that K ψ , η vanishes and diverges, respectively, and thus ( ψ , η ) is not admissible.
Activation function	η 	ψ  = Λ m G	ψ  = Λ m G ′ 	ψ  = Λ m G ″
Derivative of sigmoidal ft.	σ ′ 	+	0	+
Sigmoidal function	σ 	∞	+	0
Softplus	σ (−1) 	∞	∞	+

Dirac's δ 	δ 	+	0	+
Unit step function	z + 0 	∞	+	0
ReLU	z + 	∞	∞	+

Linear function	z 	0	0	0

RBF	G	+	0	+
7.1. Sinusoidal curve

We studied a one-dimensional signal f ( x ) = sin ⁡ 2 π x defined on x ∈ [ − 1 , 1 ] . The ridgelet functions ψ = Λ ψ 0 were chosen from derivatives of the Gaussian ψ 0 = G ( ℓ ) , ( ℓ = 0 , 1 , 2 ) . The activation functions η were chosen from among the softplus σ ( − 1 ) , the sigmoidal function σ and its derivative σ ′ , the ReLU z + , unit step function z + 0 , and Dirac's δ . In addition, we examined the case when the activation function is simply a linear function: η ( z ) = z , which cannot be admissible because the Fourier transform of polynomials is supported at the origin in the Fourier domain.

The signal was sampled from [ − 1 , 1 ] with Δ x = 1 / 100 . We computed the reconstruction formula (88) ∫ R ∫ R R ψ f ( a , b ) η ( a ⋅ x − b ) d a d b | a | , by simply discretizing ( a , b ) ∈ [ − 30 , 30 ] × [ − 30 , 30 ] by Δ a = Δ b = 1 / 10 . That is, (89) R ψ f ( a , b ) ≈ ∑ n = 0 N f ( x n ) ψ ( a ⋅ x n − b ) ‾ | a | Δ x , x n = x 0 + n Δ x (90) R η † R ψ f ( x ) ≈ ∑ ( i , j ) = ( 0 , 0 ) I , J R ψ f ( a i , b j ) η ( a i ⋅ x − b j ) Δ a Δ b | a i | , a i = a 0 + i Δ a , b j = b 0 + j Δ b where x 0 = − 1 , a 0 = − 30 , b 0 = − 30 , and N = 200 , ( I , J ) = ( 600 , 600 ) .

Fig. 2 depicts the ridgelet transform R ψ f ( a , b ) . As the order ℓ of ψ = Λ G ( ℓ ) increases, the localization of R ψ f increases. As shown in Fig. 3 , every R ψ f can be reconstructed to f with some admissible activation function η . It is somewhat intriguing that the case ψ = Λ G ″ can be reconstructed with two different activation functions.
Fig. 2

    Download : Download high-res image (276KB)
    Download : Download full-size image 

Fig. 2 . Ridgelet transform R ψ f ( a , b ) of f ( x ) = sin ⁡ 2 π x defined on [−1,1] with respect to ψ .
Fig. 3

    Download : Download high-res image (292KB)
    Download : Download full-size image 

Fig. 3 . Reconstruction with the derivative of sigmoidal function σ ′ , sigmoidal function σ , and softplus σ (−1) . The solid line plots the reconstruction result; the dotted line plots the original signal.

Fig. 3 , Fig. 4 , Fig. 5 tile the results of reconstruction with sigmoidal functions, truncated power functions, and a linear function. The solid line is a plot of the reconstruction result; the dotted line draws the original signal. In each of the figures, the theoretical diagnoses and experimental results are almost consistent and reasonable.
Fig. 4

    Download : Download high-res image (385KB)
    Download : Download full-size image 

Fig. 4 . Reconstruction with truncated power functions — Dirac's δ , unit step z + 0 , and ReLU z + . The solid line plots the reconstruction result; the dotted line plots the original signal.
Fig. 5

    Download : Download high-res image (84KB)
    Download : Download full-size image 

Fig. 5 . Reconstruction with linear function η ( z )= z . The solid line plots the reconstruction result; the dotted line plots the original signal.

In Fig. 3 , at the bottom left, the reconstruction signal with the softplus seems incompletely reconstructed, in spite of Table 5 indicating '∞'. Recall that σ ( − 1 ) ˆ ( ζ ) has a pole ζ − 2 ; thus, we can understand this cell in terms of ⁎ σ ( − 1 ) ⁎ Λ G working as an integrator , that is, a low-pass filter .

In Fig. 4 , in the top row, all the reconstructions with Dirac's δ fail. These results seem to contradict the theory. However, it simply reflects the implementation difficulty of realizing Dirac's δ , because δ ( z ) is a “function” that is almost constantly zero, except for the origin. Nevertheless, z = a x − b rarely happens to be exactly zero, provided a , b , and x are discretized. This is the reason why this row fails. At the bottom left, the ReLU seems to lack sharpness for reconstruction. Here we can again understand that ⁎ z + ⁎ Λ G worked as a low-pass filter. It is worth noting that the unit step function and the ReLU provide a sharper reconstruction than the sigmoidal function and the softplus.

In Fig. 5 , all the reconstructions with a linear function fail. This is consistent with the theory that polynomials cannot be admissible as their Fourier transforms are singular at the origin.
7.2. Shepp–Logan phantom

We next studied a gray-scale image Shepp–Logan phantom [42] . The ridgelet functions ψ = Λ 2 ψ 0 were chosen from the ℓ th derivatives of the Gaussian ψ 0 = G ( ℓ ) , ( ℓ = 0 , 1 , 2 ) . The activation functions η were chosen from the RBF G (instead of Dirac's δ ), the unit step function z + 0 , and the ReLU z + .

The original image was composed of 256 × 256 pixels. We treated it as a two-dimensional signal f ( x ) defined on [ − 1 , 1 ] 2 . We computed the reconstruction formula (91) ∫ R ∫ R 2 R ψ f ( a , b ) η ( a ⋅ x − b ) d a d b ‖ a ‖ , by discretizing ( a , b ) ∈ [ − 300 , 300 ] 2 × [ − 30 , 30 ] by Δ a = ( 1 , 1 ) and Δ b = 1 .

Fig. 6 lists the results of the reconstruction. As observed in the one-dimensional case, the results are fairly consistent with the theory. Again, at the bottom left, the reconstructed image seems dim. Our understanding is that it was caused by low-pass filtering.
Fig. 6

    Download : Download high-res image (882KB)
    Download : Download full-size image 

Fig. 6 . Reconstruction with RBF G, unit step z + 0 , and ReLU z + .
8. Concluding remarks

We have shown that neural networks with unbounded non-polynomial activation functions have the universal approximation property. Because the integral representation of the neural network coincides with the dual ridgelet transform, our goal reduces to constructing the ridgelet transform with respect to distributions. Our results cover a wide range of activation functions: not only the traditional RBF, sigmoidal function, and unit step function, but also truncated power functions z + k , which contain the ReLU and even Dirac's δ . In particular, we concluded that a neural network can approximate L 1 ∩ C 0 functions in the pointwise sense, and L 2 functions in the L 2 sense, when its activation “function” is a Lizorkin distribution ( S 0 ′ ) that is admissible. The Lizorkin distribution is a tempered distribution ( S ′ ) that is not a polynomial. As an important consequence, what a neural network learns is a ridgelet transform of the target function f . In other words, during backpropagation the network indirectly searches for an admissible ridgelet function, by constructing a backprojection filter.

Using the weak form expression of the ridgelet transform, we extensively defined the ridgelet transform with respect to distributions. Theorem 4.2 guarantees the existence of the ridgelet transform with respect to distributions. Table 4 suggests that for the convolution of distributions to converge, the class X of domain and the class Z of ridgelets should be balanced. Proposition 4.3 states that R ψ : L 1 ( R m ) → S ′ ( Y m + 1 ) is a bounded linear operator . Theorem 4.5 states that the dual ridgelet transform coincides with a dual operator. Provided the reconstruction formula holds, that is, when the ridgelets are admissible, the ridgelet transform is injective and the dual ridgelet transform is surjective.

For an unbounded η ∈ Z ( R ) to be admissible, it cannot be a polynomial and it can be associated with a backprojection filter. If η ∈ Z ( R ) is a polynomial then the product of distributions in the admissibility condition should be indeterminate. Therefore, Z ( R ) excludes polynomials. Theorem 5.4 rephrases the admissibility condition in the real domain. As a direct consequence, Corollary 5.5 gives a constructive sufficiently admissible condition.

After investigating the construction of the admissibility condition, we showed that formulas can be reconstructed on L 1 ( R m ) in two ways. Theorem 5.6 uses the Fourier slice theorem. Theorem 5.7 uses approximations to the identity and reduces to the inversion formula of the Radon transform. Theorem 5.7 as well as Corollary 5.8 suggest that the admissibility condition requires ( ψ , η ) to construct a backprojection filter.

In addition, we have extended the ridgelet transform on L 1 ( R m ) to L 2 ( R m ) . Theorem 5.9 states that Parseval's relation , which is a weak version of the reconstruction formula, holds on L 1 ∩ L 2 ( R m ) . Theorem 5.10 follows the bounded extension of R ψ from L 1 ∩ L 2 ( R m ) to L 2 ( R m ) . Theorem 5.11 gives the reconstruction formula in L 2 ( R m ) .

By showing that z + k and other activation functions belong to S 0 ′ , and that they are admissible with some derivatives of the Gaussian, we proved the universal approximation property of a neural network with an unbounded activation function. Numerical examples were consistent with our theoretical diagnoses on the admissibility. In addition, we found that some non-admissible combinations worked as a low-pass filter; for example, ( ψ , η ) = ( Λ m [ Gaussian ] , ReLU ) and ( ψ , η ) = ( Λ m [ Gaussian ] , softplus ) .

We plan to perform the following interesting investigations in future.

    1.

    Given an activation function η ∈ S 0 ′ ( R ) , which is the “best” ridgelet function ψ ∈ S ( R ) ?

        In fact, for a given activation function η , we have plenty of choices. By Corollary 5.5 , all elements of (92) A η : = { Λ m ψ 0 | ψ 0 ∈ S ( R )  such that  〈 η ˆ , ψ 0 ˆ 〉  is finite and nonzero.  } , are admissible with η .

    2.

    How are ridgelet functions related to deep neural networks?

        Because ridgelet analysis is so fruitful, we aim to develop “deep” ridgelet analysis. One of the essential leaps from shallow to deep is that the network output expands from scalar to vector because a deep structure is a cascade of multi-input multi-output layers. In this regard, we expect Corollary 5.8 to play a key role. By using the intertwining relations, we can “cascade” the reconstruction operators as below (93) ⁎ ⁎ R η † R ψ R η † R ψ = R ⁎ Λ k − 1 R ( − Δ ) m − 1 − k + ℓ 2 R ⁎ Λ ℓ − 1 R . ( 0 ≤ k , ℓ ≤ m ) This equation suggests that the cascade of ridgelet transforms coincides with a composite of backprojection filtering in the Radon domain and differentiation in the real domain. We conjecture that this point of view can be expected to facilitate analysis of the deep structure.

Acknowledgements

The authors would like to thank the anonymous reviewers for fruitful comments and suggestions to improve the quality of the paper. The authors would like to express their appreciation toward Dr. Hideitsu Hino for his kind support with writing the paper. This work was supported by JSPS KAKENHI Grand Number 15J07517 .
Appendix A. Proof of Theorem 4.2

A ridgelet transform R ψ f ( u , α , β ) is the convolution of a Radon transform R f ( u , p ) and a dilated distribution ψ α ( p ) in the sense of a Schwartz distribution. That is, (A.1) ⁎ f ( x ) ↦ R f ( u , p ) ↦ ( R f ( u , ⋅ ) ⁎ ψ α ˜ ‾ ) ( β ) = R ψ f ( u , α , β ) . We verify that the ridgelet transform is well defined in a stepwise manner. Provided there is no danger of confusion, in the following steps we denote by X the classes D , E ′ , S , O C ′ , L 1 , or D L 1 ′ .

Step 1: Class X ( S m − 1 × R ) of R f ( u , p )

Hertle's results found [39, Th. 4.6, Cor. 4.8] that the Radon transform is the continuous injection (A.2) R : X ( R m ) ↪ X ( S m − 1 × R ) , where X = D , E ′ , S , O C ′ , L 1 , or D L 1 ′ ; if f ∈ X ( R m ) then R f ∈ X ( S m − 1 × R ) , which determines the second column. Our possible choice of the domain X is restricted to them.

Step 2: Class B ( R ) of R ψ f ( u , α , β ) with respect to β

Fix α > 0 . Recall that ⁎ R ψ f ( u , α , β ) = ( R f ( u , ⋅ ) ⁎ ψ α ˜ ‾ ) ( β ) in the sense of Schwartz distributions. By the nuclearity of X [33, §51] , the kernel theorem (A.3) X ( S m − 1 × R ) ≅ X ( S m − 1 ) ⊗ ˆ X ( R ) , holds. Therefore, we can omit u ∈ S m − 1 in the considerations for ( α , β ) ∈ H . According to Schwartz's results shown in Table 3 , for the convolution ⁎ g ⁎ ψ of g ∈ X ( R ) and ψ ∈ Z ( R ) to converge in B ( R ) , we can assign the largest possible class Z for each X as in the third column. Note that for X = L 1 we even assumed the continuity Z = L p ∩ C 0 , which is technically required in Step 3. Obviously for Z = D ′ , S ′ , L p ∩ C 0 , or D L p ′ , if ψ ∈ Z ( R ) then ψ α ∈ Z ( R ) . Therefore, we can determine the fourth column by evaluating ⁎ X ⁎ Z according to Table 3 .

Step 3: Class A ( H ) of R ψ f ( u , α , β ) with respect to ( α , β )

Fix u 0 ∈ S m − 1 and assume f ∈ X ( R m ) . Write g ( p ) : = R f ( u 0 , p ) and (A.4) W [ ψ ; g ] ( α , β ) : = ∫ R g ( α z + β ) ψ ( z ) ‾ d z , then R ψ f ( u 0 , α , β ) = W [ ψ ; g ] ( α , β ) for every ( α , β ) ∈ H . By the kernel theorem, g ∈ X ( R ) .

Case 3a: ( X = D  and  Z = D ′  then  B = E  and  A = E )

We begin by considering the case in the first row. Observe that (A.5) ∂ α W [ ψ ; g ] ( α , β ) = ∂ α ∫ R g ( α z + β ) ψ ( z ) ‾ d z = ∫ R g ′ ( α z + β ) z ⋅ ψ ( z ) ‾ d z = W [ z ⋅ ψ ; g ′ ] ( α , β ) , (A.6) ∂ β W [ ψ ; g ] ( α , β ) = ∂ β ∫ R g ( α z + β ) ψ ( z ) ‾ d z = ∫ R g ′ ( α z + β ) ψ ( z ) ‾ d z = W [ ψ ; g ′ ] ( α , β ) , and thus that for every k , ℓ ∈ N 0 , (A.7) ∂ α k ∂ β ℓ W [ ψ ; g ] ( α , β ) = W [ z k ⋅ ψ ; g ( k + ℓ ) ] ( α , β ) . Obviously if g ∈ D ( R ) and ψ ∈ D ′ ( R ) then g ( k + ℓ ) ∈ D ( R ) and z k ⋅ ψ ∈ D ′ ( R ) , respectively, and thus ∂ α k ∂ β ℓ W [ ψ ; g ] ( α , β ) exists at every ( α , β ) ∈ H . Therefore, we can conclude that if g ∈ D ( R ) and ψ ∈ D ′ ( R ) then W [ ψ ; g ] ∈ E ( H ) .

Case 3b: ( X = E ′  and  Z = D ′  then  B = D ′  and  A = D ′ )

Let g ∈ E ′ ( R ) and ψ ∈ D ′ ( R ) . We show that W [ ψ ; g ] ∈ D ′ ( H ) , that is, for every compact set K ⊂ H , there exists N ∈ N 0 such that (A.8) | ∫ K T ( α , β ) W [ ψ ; g ] ( α , β ) d α d β α | ≲ ∑ k , ℓ ≤ N sup ( α , β ) ∈ H ⁡ | ∂ α k ∂ β ℓ T ( α , β ) | , ∀ T ∈ D ( K ) . Fix an arbitrary compact set K ⊂ H and a smooth function T ∈ D ( K ) , which is supported in K. Take two compact sets A ⊂ R + and B ⊂ R such that K ⊂ A × B . By the assumption that g ∈ E ′ ( R ) and ψ ∈ D ′ ( R ) , there exist k , ℓ ∈ N 0 such that (A.9) | ∫ R u ( z ) g ( z ) d z | ≲ sup z ∈ supp g ⁡ | u ( k ) ( z ) | , ∀ u ∈ E ( R ) (A.10) | ∫ R v ( z ) ψ ( z ) ‾ d z | ≲ sup z ∈ R ⁡ | v ( ℓ ) ( z ) | , ∀ v ∈ D ( B ) . Observe that for every fixed α , ⁎ T ( α , ⋅ ) ⁎ g ˜ ∈ D ′ ( R ) . Then, by applying (A.9) and (A.10) incrementally, (A.11) | ∫ R T ( α , β ) ∫ R g ( α z + β ) ψ ( z ) ‾ d z d α d β α | ≤ ∫ 0 ∞ | ∫ R ∫ R T ( α , β − α z ) ψ ( z ) ‾ d z ⋅ g ( β ) d β | d α α (A.12) ≲ ∫ 0 ∞ sup β ∈ supp g ⁡ | ∫ R ∂ β k T ( α , β − α z ) ψ ( z ) d z | d α α (A.13) ≲ ∫ 0 ∞ sup β ∈ supp g ⁡ sup z ⁡ | ∂ β k + ℓ T ( α , β − α z ) | α ℓ − 1 d α (A.14) = ∫ A sup β ∈ B ⁡ | ∂ β k + ℓ T ( α , β ) | α ℓ − 1 d α (A.15) ≤ sup ( α , β ) ∈ K ⁡ | ∂ β k + ℓ T ( α , β ) | ⋅ ∫ A α ℓ − 1 d α , where the third inequality follows by repeatedly applying ∂ z [ T ( α , β − α z ) ] = ( − α ) ∂ β T ( α , β − α z ) ; the fourth inequality follows by the compactness of the support of T. Thus, we conclude that W [ ψ ; g ] ∈ D ′ ( H ) .

Case 3c: ( X = S  and  Z = S ′  then  B = O M  and  A = O M )

Let g ∈ S ( R ) and ψ ∈ S ′ ( R ) . Recall the case when X = D . Obviously, for every k , ℓ ∈ N 0 , g ( k + ℓ ) ∈ S ( R ) and z k ⋅ ψ ∈ S ′ ( R ) , respectively, which implies W [ ψ ; g ] ∈ E ( H ) . Now we even show that W [ ψ ; g ] ∈ O M ( H ) , that is, for every k , ℓ ∈ N 0 there exist s , t ∈ N 0 such that (A.16) | ∂ α k ∂ β ℓ W [ ψ ; g ] ( α , β ) | ≲ ( α + 1 / α ) s ( 1 + β 2 ) t / 2 . Recall that by (A.7) , we can regard ∂ α k ∂ β ℓ W [ ψ ; g ] ( α , β ) as ∂ α 0 ∂ β 0 W [ ψ 0 ; g 0 ] ( α , β ) , by setting g 0 : = g ( k + ℓ ) ∈ S ( R ) and ψ 0 : = z k ⋅ ψ ∈ S ′ ( R ) . Henceforth we focus on the case when k = ℓ = 0 . Since ψ ∈ S ′ ( R ) , there exists N ∈ N 0 such that (A.17) | ∫ R u ( z ) ψ ( z ) ‾ d z | ≲ ∑ s , t ≤ N sup z ∈ R ⁡ | z s u ( t ) ( z ) | , ∀ u ∈ S ( R ) . By substituting u ( z ) ← g ( α z + β ) , we have (A.18) | ∫ R g ( α z + β ) ψ ( z ) ‾ d z | ≲ ∑ s , t ≤ N sup z ∈ R ⁡ | z s ∂ z t g ( α z + β ) | (A.19) = ∑ s , t ≤ N sup p ∈ R ⁡ | ( p − β α ) s α t g ( t ) ( p ) | (A.20) ≲ ∑ s , t ≤ N α t − s β s sup p ∈ R ⁡ | p s g ( t ) ( p ) | (A.21) ≲ ( α + 1 / α ) N ( 1 + β 2 ) N / 2 , where the second equation follows by substituting p ← α z + β ; the fourth inequality follows because every sup p ⁡ | p s g t ( p ) | is finite by assumption that g ∈ S ( R ) . Therefore, we can conclude that if g ∈ S ( R ) and ψ ∈ S ′ ( R ) then W [ ψ ; g ] ∈ O M ( H ) .

Case 3d: ( X = O C ′  and  Z = S ′  then  B = S ′  and  A = S ′ )

Let g ∈ O C ′ ( R ) and ψ ∈ S ′ ( R ) . We show that W [ ψ ; g ] ∈ S ′ ( H ) , that is, there exists N ∈ N 0 depending only on ψ and g such that (A.22) | ∫ H T ( α , β ) W [ ψ ; g ] ( α , β ) d α d β α | ≲ ∑ s , t , k , ℓ ≤ N sup α , β ∈ H ⁡ | D s , t k , ℓ T ( α , β ) | , ∀ T ∈ S ( H ) where we defined (A.23) D s , t k , ℓ T ( α , β ) : = ( α + 1 / α ) s ( 1 + β 2 ) t / 2 ∂ α k ∂ β ℓ T ( α , β ) . Fix an arbitrary T ∈ S ( H ) . By the assumption that ψ ∈ S ′ ( R ) , there exist s , t ∈ N 0 such that (A.24) | ∫ R u ( z ) ψ ( z ) ‾ d z | ≲ sup z ⁡ | z t u ( s ) ( z ) | , ∀ u ∈ S ( R ) . Observe that for every fixed α , ⁎ T ( α , ⋅ ) ⁎ g ˜ ∈ S ( R ) . Then we can provide an estimate as below. (A.25) | ∫ H T ( α , β ) ∫ R g ( α z + β ) ψ ( z ) ‾ d z d α d β α | ≤ ∫ 0 ∞ | ∫ R ∫ R T ( α , β ) g ( α z + β ) d β ⋅ ψ ( z ) ‾ d z | d α α (A.26) ≲ ∫ R sup z ⁡ | z t ∫ R D s , 0 0 , 0 T ( α , β ) g ( s ) ( α z + β ) d β | d α α (A.27) ≲ ∫ R sup p ⁡ | p t ∫ R D s + t , 0 0 , 0 T ( α , β ) g ( s ) ( p + β ) d β | d α α (A.28) ≤ ∫ R ∫ R sup p ⁡ | p t g ( s ) ( p + β ) | | D s + t , 0 0 , 0 T ( α , β ) | d β d α α (A.29) ≲ ∫ R ∫ R sup p ⁡ | ( 1 + | p + β | 2 ) t / 2 g ( s ) ( p + β ) | | D s + t , t 0 , 0 T ( α , β ) | d β d α α (A.30) ≲ ∫ H | D s + t , t 0 , 0 T ( α , β ) | d β d α α (A.31) ≤ sup ( α , β ) ∈ H ⁡ | D s + t + ε , t + δ 0 , 0 T ( α , β ) | ∫ H ( α + 1 / α ) − ε ( 1 + β 2 ) − δ / 2 d β d α α , where the second inequality follows by repeatedly applying ∂ z [ g ( α z + β ) ] = α ⋅ g ′ ( α z + β ) and α ≲ α + 1 / α ; the third inequality follows by changing the variable p ← α z and applying ( α + 1 / α ) s ⋅ α − t ≲ ( α + 1 / α ) s + t ; the fifth inequality follows by applying | p | ≲ ( 1 + p 2 ) 1 / 2 and Peetre's inequality 1 + p 2 ≲ ( 1 + β 2 ) ( 1 + | p + β | 2 ) ; the sixth inequality follows by the assumption that ( 1 + p 2 ) t / 2 g ( p ) is bounded for any t ; the last inequality follows by Hölder's inequality and the integral is convergent when ε > 0 and δ > 1 .

Case 3e: ( X = L 1  and  Z = L p ∩ C 0  then  B = L p ∩ C 0  and  A = S ′ )

Let g ∈ L 1 ( R ) and ψ ∈ L p ∩ C 0 ( R ) . We show that W [ g ; ψ ] ∈ S ′ ( H ) , that is, it has at most polynomial growth at infinity. Because ψ is continuous, ⁎ g ⁎ ψ is continuous. By Lusin's theorem, there exists a continuous function g ⋆ such that g ⋆ ( x ) = g ( x ) for almost every x ∈ R ; thus, by the continuity of ⁎ g ⁎ ψ , (A.32) ⁎ ⁎ g ⋆ ⁎ ψ ( x ) = g ⁎ ψ ( x ) , for every  x ∈ R . By the continuity and the integrability of g ⋆ and ψ , there exist s , t ∈ R such that (A.33) | g ⋆ ( x ) | ≲ ( 1 + x 2 ) − s / 2 , s > 1 (A.34) | ψ ( x ) | ≲ ( 1 + x 2 ) − t / 2 , t p > 1 Therefore, (A.35) | ∫ R g ( x ) ψ ( x − β α ) ‾ 1 α d x | ≲ | ∫ R ( 1 + x 2 ) − s / 2 ( 1 + ( x − β α ) 2 ) − t / 2 d x | α − 1 (A.36) ≲ | ∫ R ( 1 + x 2 ) − s / 2 ( 1 + ( x − β ) 2 ) − t / 2 d x | ( 1 + α 2 ) t / 2 α − 1 (A.37) ≲ ( 1 + β 2 ) − min ⁡ ( s , t ) / 2 ( α + 1 / α ) t − 1 , which means W [ ψ ; g ] is a locally integrable function that grows at most polynomially at infinity.

Note that if ( t − 1 ) p < m − 1 then W [ ψ ; g ] ∈ L p ( H ; α − m d α d β ) , because | W [ ψ ; g ] ( α , β ) | p behaves as β − min ⁡ ( s , t ) α ( t − 1 ) p at infinity.

Case 3f: ( X = D L 1 ′  and  Z = D L p ′  then  B = D L p ′  and  A = S ′ )

Let g ∈ D L 1 ′ ( R ) and ψ ∈ D L p ′ . We estimate (A.22) . Fix an arbitrary T ∈ S ( H ) . By the assumption that ψ ∈ D L p ′ ( R ) , for every fixed α , ⁎ T ( α , ⋅ ) ⁎ ψ ∈ E ∩ L p ( R ) . Therefore, we can take ψ ⋆ ∈ E ∩ L p ( R ) such that ψ ⋆ = ψ a.e. and ⁎ ⁎ T ( α , ⋅ ) ⁎ ψ ⋆ = T ( α , ⋅ ) ⁎ ψ . In the same way, we can take g ⋆ ∈ E ∩ L 1 ( R ) such that g ⋆ = g almost everywhere and ⁎ ⁎ T ( α , ⋅ ) ⁎ g ⋆ = T ( α , ⋅ ) ⁎ g . Therefore, this case reduces to show that if X = L 1 and Z = L p then A = S ′ . This coincides with case 3e.

Step 4: Class Y ( Y m + 1 ) of R ψ f ( u , α , β )

The last column ( Y ) is obtained by applying Y ( Y m + 1 ) = X ( S m − 1 ) ⊗ ˆ A ( H ) . Recall that for S m − 1 , as it is compact, D = S = O M = E and E ′ = O C ′ = S ′ = D L p ′ = D ′ . Therefore, we have Y as in the last column of Table 4 .
Appendix B. Proof of Theorem 5.4

Let ( ψ , η ) ∈ S ( R ) × S ′ ( R ) . Assume that η ˆ is singular at 0. That is, there exists k ∈ N 0 such that (B.1) η ˆ ( ζ ) = ∑ j = 0 k c j δ ( j ) ( ζ ) , ζ ∈ { 0 } . Assume there exists a neighborhood Ω of 0 such that η ˆ ∈ C 0 ( Ω ∖ { 0 } ) . Note that the continuity implies local integrability. We show that ψ and η are admissible if and only if there exists u ∈ O M ( R ) such that (B.2) ⁎ Λ m u = ψ ˜ ‾ ⁎ ( η − ∑ j = 0 k c j z j ) ,  and  ∫ R ∖ { 0 } u ˆ ( ζ ) d ζ ≠ 0 . Recall that the Fourier transform O M ( R ) → O C ′ ( R ) is bijective. Thus, the action of u ˆ on the indicator function 1 R ∖ { 0 } ( ζ ) is always finite.

Sufficiency:

On Ω ∖ { 0 } , η ˆ coincides with a function. Thus the product ψ ˆ ( ζ ) ‾ η ˆ ( ζ ) | ζ | − m is defined in the sense of ordinary functions, and coincides with u ˆ ( ζ ) . On R ∖ Ω , | ζ | − m is in O M ( R ∖ Ω ) . Thus, the product ψ ˆ ( ζ ) ‾ η ˆ ( ζ ) | ζ | − m is defined in the sense of distributions, which is associative because it contains at most one tempered distribution ( S ⋅ S ′ ⋅ O M ), and reduces to u ˆ ( ζ ) . Therefore, (B.3) K ψ , η ( 2 π ) m − 1 = ( ∫ Ω ∖ { 0 } + ∫ R ∖ Ω ) u ˆ ( ζ ) d ζ , which is finite by assumption.

Necessity:

Write Ω 0 : = Ω ∩ [ − 1 , 1 ] and Ω 1 : = R ∖ Ω 0 . By the assumption that ∫ Ω 0 ∖ { 0 } ψ ˆ ( ζ ) ‾ η ˆ ( ζ ) | ζ | − m d ζ is absolutely convergent and η ˆ is continuous in Ω 0 ∖ { 0 } , there exists ε > 0 such that (B.4) | ψ ˆ ( ζ ) ‾ η ˆ ( ζ ) | ≲ | ζ | m − 1 + ε , ζ ∈ Ω 0 ∖ { 0 } . Therefore, there exists v 0 ∈ L 1 ( R ) ∩ C 0 ( R ∖ { 0 } ) such that its restriction to Ω 0 ∖ { 0 } coincides with (B.5) ψ ˆ ( ζ ) ‾ η ˆ ( ζ ) = | ζ | m v 0 ( ζ ) , ζ ∈ Ω 0 ∖ { 0 } . By integrability and continuity, v 0 ∈ L ∞ ( R ) . In particular, both lim ζ → + 0 ⁡ v 0 ( ζ ) and lim ζ → − 0 ⁡ v 0 ( ζ ) are finite.

However, in Ω 1 , | ζ | − m ∈ O M ( Ω 1 ) . By the construction, ψ ˆ ‾ ⋅ η ˆ ∈ O C ′ ( R ) . Thus, there exists v 1 ∈ O C ′ ( R ) such that (B.6) ψ ˆ ( ζ ) ‾ η ˆ ( ζ ) = | ζ | m v 1 ( ζ ) , ζ ∈ Ω 1 where the equality is in the sense of distribution.

Let (B.7) v : = v 0 ⋅ 1 Ω 0 + v 1 ⋅ 1 Ω 1 . Clearly, v ∈ O C ′ ( R ) because v 0 ⋅ 1 Ω 0 ∈ E ′ ( R ) and v 1 ⋅ 1 Ω 1 ∈ O C ′ ( R ) . Therefore, there exists u ∈ O M ( R ) such that u ˆ = v and (B.8) ψ ˆ ( ζ ) ‾ η ˆ ( ζ ) = | ζ | m u ˆ ( ζ ) , ζ ∈ R ∖ { 0 } . By the admissibility condition, (B.9) ∫ R ∖ { 0 } u ˆ ( ζ ) d ζ = ∫ Ω 0 ∖ { 0 } v 0 ( ζ ) d ζ + ∫ Ω 1 v 1 ( ζ ) d ζ ≠ 0 . In consideration of the singularity at 0, we have (B.10) ψ ˆ ( ζ ) ‾ ( η ˆ ( ζ ) − ∑ j = 0 k c j δ ( j ) ( ζ ) ) = | ζ | m u ˆ ( ζ ) , ζ ∈ R . By taking the Fourier inversion in the sense of distributions, (B.11) ⁎ [ ψ ˜ ‾ ⁎ ( η − ∑ j = 0 k c j z j ) ] ( z ) = Λ m u ( z ) , z ∈ R .
Appendix C. Proof of Theorem 5.6

Let f ∈ L 1 ( R m ) satisfy f ˆ ∈ L 1 ( R m ) and ( ψ , η ) ∈ S ( R ) × S 0 ′ ( R ) be admissible. For simplicity, we rescale ψ to satisfy K ψ , η = 1 . Write (C.1) I ( x ; ε , δ ) : = ∫ S m − 1 ∫ ε δ ∫ R R ψ f ( u , α , u ⋅ x − α z ) η ( z ) d z d α d u α m . We show that (C.2) lim δ → ∞ ε → 0 ⁡ I ( x ; ε , δ ) = f ( x ) , a.e.  x ∈ R m and the equality holds at every continuous point of f .

By using the Fourier slice theorem in the sense of distribution, (C.3) ∫ R R ψ f ( u , α , β − α z ) η ( z ) d z = 1 2 π ∫ R f ˆ ( ω u ) ψ ˆ ( α ω ) ‾ η ˆ ( α ω ) e i ω β d ω (C.4) = 1 2 π ∫ R ∖ { 0 } f ˆ ( ω u ) u ˆ ( α ω ) | α ω | m e i ω β d ω , where | ζ | m u ˆ ( ζ ) : = ψ ˆ ( ζ ) ‾ η ˆ ( ζ ) ( ζ ≠ 0 ) is defined as in Theorem 5.4 .

Then, (C.5) ∫ ε δ (C.4) d α α m = 1 2 π ∫ R ∖ { 0 } ∫ ε δ f ˆ ( ω u ) u ˆ ( α ω ) | ω | m e i ω β d α d ω (C.6) = 1 2 π ∫ R ∫ ε ≤ ζ ω ≤ δ u ˆ ( ζ ) f ˆ ( ω u ) e i ω β | ω | m − 1 d ζ d ω (C.7) = 1 2 π ∫ 0 ∞ ∫ r ε ≤ | ζ | ≤ r δ u ˆ ( ζ ) f ˆ ( sgn ( ζ ) r u ) exp ⁡ ( sgn ( ζ ) i r β ) r m − 1 d ζ d r , where the second equation follows by changing the variable ζ ← α ω with α m − 1 d α = | ω | m − 1 | ζ | − m d ζ ; the third equation follows by changing the variable r ← | ω | with sgn ω = sgn ζ . In the following, we substitute β ← u ⋅ x . Observe that in ∫ S m − 1 d u , (C.8) ∫ S m − 1 f ˆ ( − r u ) exp ⁡ ( − i r u ⋅ x ) d u = ∫ S m − 1 f ˆ ( r u ) exp ⁡ ( i r u ⋅ x ) d u . Hence, we can omit sgn ζ .

Then, by substituting β ← u ⋅ x and changing the variable ξ ← r u , (C.9) I ( x ; ε , δ ) = ∫ S m − 1 (C.7) d u (C.10) = 1 2 π ∫ S m − 1 ∫ 0 ∞ [ ∫ r ε ≤ | ζ | ≤ r δ u ˆ ( ζ ) d ζ ] f ˆ ( r u ) e i r u ⋅ x r m − 1 d r d u (C.11) = 1 2 π ∫ R m [ ∫ ‖ ξ ‖ ε ≤ | ζ | ≤ ‖ ξ ‖ δ u ˆ ( ζ ) d ζ ] f ˆ ( ξ ) e i ξ ⋅ x d ζ d ξ . Recall that u ˆ ∈ O C ′ ( R ) ; thus, its action is continuous. That is, the limits and the integral commute.

Therefore, (C.12) R η † R ψ f ( x ) = lim δ → ∞ ε → 0 ⁡ I ( x ; ε , δ ) (C.13) = 1 ( 2 π ) ∫ R m [ ∫ R ∖ { 0 } u ˆ ( ζ ) d ζ ] f ˆ ( ξ ) e i ξ ⋅ x d ξ (C.14) = 1 ( 2 π ) m ∫ R m f ˆ ( ξ ) e i ξ ⋅ x d ξ (C.15) = f ( x ) , a.e.  x ∈ R m where the last equation follows by the Fourier inversion formula, a consequence of which the equality holds at x 0 if f is continuous at x 0 .
Appendix D. Proof of Theorem 5.7

Let f ∈ L 1 ( R m ) and ( ψ , η ) ∈ S ( R ) × S ′ ( R ) . Assume that there exists u ∈ E ∩ L 1 ( R ) that is real-valued, ⁎ Λ m u = ψ ˜ ‾ ⁎ η and ∫ R u ˆ ( ζ ) d ζ = − 1 . Write (D.1) I ( x ; ε , δ ) : = ∫ S m − 1 ∫ ε δ ∫ R R ψ f ( u , α , u ⋅ x − α z ) η ( z ) d z d α d u α m . We show that (D.2) ⁎ lim δ → ∞ ε → 0 ⁡ I ( x ; ε , δ ) = R ⁎ Λ m − 1 R ( x ) , a.e.  x ∈ R m .

In the following we write ( ⋅ ) α ( p ) = ( ⋅ ) ( p / α ) / α . By using the convolution form, (D.3) ⁎ ⁎ ∫ R R ψ f ( u , α , β − α z ) η ( z ) d z = [ R f ( u , ⋅ ) ⁎ ( ψ ˜ ‾ ⁎ η ) α ] ( β ) (D.4) ⁎ = [ R f ( u , ⋅ ) ⁎ ( Λ m u ) α ] ( β ) .

Observe that (D.5) ∫ ε δ ( Λ m u ) α ( p ) d α α m = Λ m − 1 [ ∫ ε δ ( Λ u ) ( p α ) d α α 2 ] (D.6) = Λ m − 1 [ 1 p ∫ p / δ p / ε ( Λ u ) ( z ) d z ] (D.7) = Λ m − 1 [ 1 p H u ( p ε ) − 1 p H u ( p δ ) ] (D.8) = Λ m − 1 [ k ε ( p ) − k δ ( p ) ] , where the first equality follows by repeatedly applying ( Λ u ) α = α Λ ( u α ) ; the second equality follows by substituting z ← p / α ; the fourth equality follows by defining (D.9) k ( z ) : = 1 z H u ( z )  and  k γ ( p ) : = 1 γ k ( p γ )  for  γ = ε , δ . Therefore, we have (D.10) ⁎ ∫ ε δ (D.4) d α α m = [ R f ( u , ⋅ ) ⁎ (D.8) ] ( β ) (D.11) ⁎ = [ Λ m − 1 R f ( u , ⋅ ) ⁎ ( k ε − k δ ) ] ( β ) .

We show that k ∈ L 1 ∩ L ∞ ( R ) and ∫ R k ( z ) d z = 1 . To begin with, k ∈ L 1 ( R ) because there exist s , t > 0 such that (D.12) | k ( z ) | ≲ | z | − 1 + s ,  as  | z | → 0 (D.13) | k ( z ) | ≲ | z | − 1 − t ,  as  | z | → ∞ . The first claim holds because u is real-valued and thus u ˆ is odd, then (D.14) H u ( 0 ) = ∫ R sgn ζ ⋅ u ˆ ( ζ ) d ζ (D.15) = ∫ ( − ∞ , 0 ] u ˆ ( ζ ) d ζ − ∫ ( 0 , ∞ ) u ˆ ( ζ ) d ζ (D.16) = 0 . The second claim holds because u ∈ L 1 ( R ) and thus u as well as H u decays at infinity. Then, by the continuity and the integrability of k , it is bounded. By the assumption that ∫ R u ˆ ( ζ ) d ζ = − 1 , (D.17) ∫ R k ( z ) d z = − ∫ R H u ( z ) 0 − z d z (D.18) = − u ( 0 ) (D.19) = 1 .

Write (D.20) J ( u , p ) : = Λ m − 1 R f ( u , p ) . Because k ∈ L 1 ( R ) and ∫ R k ( z ) d z = 1 , k ε is an approximation of the identity [43, III, Th. 2] . Then, (D.21) ⁎ lim ε → 0 ⁡ J ( u , ⋅ ) ⁎ k ε ( p ) = J ( u , p ) ,  a.e.  ( u , p ) ∈ S m − 1 × R . However, as k ∈ L ∞ ( R ) , (D.22) ⁎ ‖ J ⁎ k δ ‖ L ∞ ( S m − 1 × R ) ≤ δ − 1 ‖ J ‖ L 1 ( S m − 1 × R ) ‖ k ‖ L ∞ ( R ) , and thus, (D.23) ⁎ lim δ → ∞ ⁡ J ( u , ⋅ ) ⁎ k δ ( p ) = 0 ,  a.e.  ( u , p ) ∈ S m − 1 × R .

Because it is an approximation to the identity, ⁎ J ⁎ k γ ∈ L 1 ( S m − 1 × R ) for 0 ≤ γ . Hence, there exists a maximal function M ( u , p ) [43, III, Th. 2] such that (D.24) ⁎ sup 0 < ε ⁡ | ( J ( u , ⋅ ) ⁎ v ε ) ( p ) | ≲ M ( u , p ) . Therefore, ⁎ | J ( u , ⋅ ) ⁎ ( v ε − v δ ) ( u ⋅ x ) | is uniformly integrable [35, Ex. 4.15.4] on S m − 1 . That is, if Ω ⊂ S m − 1 satisfies ∫ Ω d u ≤ A then (D.25) ⁎ ∫ Ω | J ( u , ⋅ ) ⁎ k γ | ( u ⋅ x ) d u ≲ A sup u , p ⁡ | M ( u , p ) | , ∀ γ ≥ 0 . Thus, by the Vitali convergence theorem, we have (D.26) ⁎ R η † R ψ f ( x ) = lim δ → ∞ ε → 0 ⁡ ∫ S m − 1 [ J ( u , ⋅ ) ⁎ ( v ε − v δ ) ] ( u ⋅ x ) d u (D.27) = ∫ S m − 1 J ( u , u ⋅ x ) d u , a . e . x ∈ R m (D.28) ⁎ = R ⁎ Λ m − 1 R f ( x ) .
Appendix E. Proof of Theorem 5.11

Let f ∈ L 2 ( R m ) and ( ψ , η ) be admissible with K ψ , η = 1 . Assume without loss of generality that ( ψ , ψ ) and ( η , η ) are self-admissible respectively. Write (E.1) I [ f ; ( ε , δ ) ] ( x ) : = ∫ S m − 1 ∫ ε δ ∫ R R ψ f ( u , α , u ⋅ x − α z ) η ( z ) d z d α d u α m . In the following we write Ω [ ε , δ ] : = S m − 1 × [ R + ∖ ( ε , δ ) ] × R ⊂ Y m + 1 . We show that (E.2) lim δ → ∞ ε → 0 ⁡ ‖ f − I [ f ; ( ε , δ ) ] ‖ 2 = 0 . Observe that (E.3) ‖ f − I [ f ; ( ε , δ ) ] ‖ 2 = sup ‖ g ‖ 2 = 1 ⁡ | ( f − I [ f ; ( ε , δ ) ] , g ) | (E.4) = sup ‖ g ‖ 2 = 1 ⁡ | ( R ψ f , R η g ) Ω [ ε , δ ] | (E.5) ≤ sup ‖ g ‖ 2 = 1 ⁡ | R ψ f | L 2 ( Ω [ ε , δ ] ) ‖ R η g ‖ L 2 ( Y m + 1 ) (E.6) = sup ‖ g ‖ 2 = 1 ⁡ | R ψ f | L 2 ( Ω [ ε , δ ] ) ‖ g ‖ 2 (E.7) → 0 ⋅ 1 ,  as  ε → 0 , δ → ∞ where the third inequality follows by the Schwartz inequality; the last limit follows by ‖ R ψ f ‖ L 2 ( Ω [ ε , δ ] ) , which shrinks as the domain Ω [ ε , δ ] tends to ∅.
Appendix F. Proofs of Example 6.4 and Example 6.10

Let σ ( z ) : = ( 1 + e − z ) − 1 . Obviously σ ( z ) ∈ E ( R ) .

Step 0: Derivatives of σ ( z ) .

For every k ∈ N , (F.1) σ ( k ) ( z ) = S k ( σ ( z ) ) , where S k ( z ) is a polynomial defined by (F.2) S k ( z ) : = { z ( 1 − z ) k = 1 S k − 1 ′ ( z ) S 1 ( z ) k > 1 , which is justified by induction on k .

Step 1: σ , tanh ⁡ ∈ O M ( R ) .

Recall that | σ ( z ) | ≤ 1 . Hence, for every k ∈ N , (F.3) | σ ( k ) ( z ) | = | S k ( σ ( z ) ) | ≤ max z ∈ [ 0 , 1 ] ⁡ | S k ( z ) | < ∞ . Therefore, every k ∈ N 0 , σ ( k ) ( z ) is bounded, which concludes σ ( z ) ∈ O M ( R ) .

Hence, immediately tanh ⁡ ∈ O M ( R ) because (F.4) tanh ⁡ ( z ) = 2 σ ( 2 z ) − 1 .

Step 2: σ ( k ) ∈ S ( R ) , k ∈ N .

Observe that (F.5) σ ′ ( z ) = ( e z / 2 + e − z / 2 ) − 2 . Hence, σ ′ ( z ) decays faster than any polynomial, which means sup z ⁡ | z ℓ σ ′ ( z ) | < ∞ for any ℓ ∈ N 0 . Then, for every k , ℓ ∈ N 0 , (F.6) sup z ⁡ | z ℓ σ ( k + 1 ) ( z ) | = sup z ⁡ | z ℓ S k + 1 ( σ ( z ) ) | ≤ max z ⁡ | z ℓ σ ′ ( z ) | ⋅ max z ⁡ | S k ′ ( σ ( z ) ) | < ∞ , which concludes σ ′ ∈ S ( R ) . Therefore, σ ( k ) ∈ S ( R ) for every k ∈ N .

Step 3: σ ( − 1 ) ∈ O M ( R ) .

Observe that (F.7) σ ( − 1 ) ( z ) = ∫ 0 z σ ( w ) d w . Hence, it is already known that [ σ ( − 1 ) ] ( k ) = σ ( k − 1 ) ∈ O M ( R ) for every k ∈ N . We show that σ ( − 1 ) ( z ) has at most polynomial growth. Write (F.8) ρ ( z ) : = σ ( − 1 ) ( z ) − z + . Then ρ ( z ) attains at 0 its maximum max z ⁡ ρ ( z ) = log ⁡ 2 , because ρ ′ ( z ) < 0 when z > 0 and ρ ′ ( z ) > 0 when z < 0 . Therefore, (F.9) | σ ( − 1 ) ( z ) | ≤ | ρ ( z ) | + | z + | ≤ log ⁡ 2 + | z | , which concludes σ ( − 1 ) ( z ) ∈ O M .

Step 4: η = σ ( k ) is admissible with ψ = Λ m G when k ∈ N is positive and odd.

Recall that η = σ ( k ) ∈ S ( R ) . Hence, 〈 η ˆ , ψ 0 ˆ 〉 = 〈 η , ψ 0 〉 . Observe that if k is odd, then σ ( k ) is an odd function and thus 〈 η , ψ 0 〉 = 0 . However, if k is even, then σ ( k ) is an even function and thus 〈 η , ψ 0 〉 ≠ 0 .

Step 5: σ and σ ( − 1 ) cannot be admissible with ψ = Λ m G .

This follows by Theorem 5.4 , because both (F.10) ⁎ ⁎ ∫ R ( G ˜ ‾ ⁎ σ ) ( z ) d z  and  ∫ R ( G ˜ ‾ ⁎ σ ( − 1 ) ) ( z ) d z , diverge.

Step 6: σ and σ ( − 1 ) are admissible with ψ = Λ m G ′ and ψ = Λ m G ″ , respectively.

Observe that both (F.11) ⁎ ⁎ ⁎ ⁎ u 0 : = G ′ ˜ ‾ ⁎ σ = G ˜ ‾ ⁎ σ ′  and  u − 1 : = G ″ ˜ ‾ ⁎ σ ( − 1 ) = G ˜ ‾ ⁎ σ ′ , belong to S ( R ) . Hence, u 0 and u − 1 satisfy the sufficient condition in Theorem 5.4 .
References

    [1]
    N. Murata
    An integral representation of functions using three-layered betworks and their approximation bounds
    Neural Netw., 9 (6) (1996), pp. 947-956, 10.1016/0893-6080(96)00000-7
    View PDF View article View in Scopus Google Scholar
    [2]
    S. Sonoda, N. Murata
    Sampling hidden parameters from oracle distribution
    24th Int. Conf. Artif. Neural Networks, LNCS, vol. 8681, Springer International Publishing (2014), pp. 539-546, 10.1007/978-3-319-11179-7_68
    View at publisher
    View in Scopus Google Scholar
    [3]
    X. Glorot, A. Bordes, Y. Bengio
    Deep sparse rectifier neural networks
    14th Int. Conf. Artif. Intell. Stat., vol. 15, JMLR W&CP (2011), pp. 315-323
    View in Scopus Google Scholar
    [4]
    I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, Y. Bengio
    Maxout networks
    30th Int. Conf. Mach. Learn., vol. 28, JMLR W&CP (2013), pp. 1319-1327
    Google Scholar
    [5]
    G.E. Dahl, T.N. Sainath, G.E. Hinton
    Improving deep neural networks for LVCSR using rectified linear units and dropout
    2013 IEEE Int. Conf., Acoust. Speech Signal Process, IEEE (2013), pp. 8609-8613, 10.1109/ICASSP.2013.6639346
    View at publisher
    View in Scopus Google Scholar
    [6]
    A.L. Maas, A.Y. Hannun, A.Y. Ng
    Rectifier nonlinearities improve neural network acoustic models
    ICML 2013 Work. Deep Learn., Audio, Speech, Lang. Process (2013)
    Google Scholar
    [7]
    K. Jarrett, K. Kavukcuoglu, M. Ranzato, Y. LeCun
    What is the best multi-stage architecture for object recognition?
    2009 IEEE 12th Int. Conf. Comput. Vision, IEEE (2009), pp. 2146-2153, 10.1109/ICCV.2009.5459469
    View at publisher
    View in Scopus Google Scholar
    [8]
    A. Krizhevsky, I. Sutskever, G.E. Hinton
    ImageNet classification with deep convolutional neural networks
    Adv. Neural Inf. Process. Syst., vol. 25, Curran Associates, Inc. (2012), pp. 1097-1105
    Google Scholar
    [9]
    M.D. Zeiler, M. Ranzato, R. Monga, M.Z. Mao, K. Yang, Q. Le Viet, P. Nguyen, A.W. Senior, V. Vanhoucke, J. Dean, G.E. Hinton
    On rectified linear units for speech processing
    2013 IEEE Int. Conf. Acoust. Speech Signal Process., IEEE (2013), pp. 3517-3521, 10.1109/ICASSP.2013.6638312
    View at publisher
    View in Scopus Google Scholar
    [10]
    H. Mhaskar, C.A. Micchelli
    Approximation by superposition of sigmoidal and radial basis functions
    Adv. in Appl. Math., 13 (3) (1992), pp. 350-373, 10.1016/0196-8858(92)90016-P
    View PDF View article View in Scopus Google Scholar
    [11]
    M. Leshno, V.Y. Lin, A. Pinkus, S. Schocken
    Multilayer feedforward networks with a nonpolynomial activation function can approximate any function
    Neural Netw., 6 (6) (1993), pp. 861-867, 10.1016/S0893-6080(05)80131-5
    View PDF View article View in Scopus Google Scholar
    [12]
    A. Pinkus
    Approximation theory of the MLP model in neural networks
    Acta Numer., 8 (1999), pp. 143-195, 10.1017/S0962492900002919
    View at publisher
    View in Scopus Google Scholar
    [13]
    Y. Ito
    Representation of functions by superpositions of a step or sigmoid function and their applications to neural network theory
    Neural Netw., 4 (3) (1991), pp. 385-394, 10.1016/0893-6080(91)90075-G
    View PDF View article View in Scopus Google Scholar
    [14]
    P.C. Kainen, V. Kůrková, A. Vogt
    A Sobolev-type upper bound for rates of approximation by linear combinations of Heaviside plane waves
    J. Approx. Theory, 147 (1) (2007), pp. 1-10, 10.1016/j.jat.2006.12.009
    View PDF View article View in Scopus Google Scholar
    [15]
    V. Kůrková
    Complexity estimates based on integral transforms induced by computational units
    Neural Netw., 33 (2012), pp. 160-167, 10.1016/j.neunet.2012.05.002
    View PDF View article View in Scopus Google Scholar
    [16]
    S.M. Carroll, B.W. Dickinson
    Construction of neural nets using the Radon transform
    Int. Jt. Conf. Neural Networks, vol. 1, IEEE (1989), pp. 607-611, 10.1109/IJCNN.1989.118639
    View at publisher
    View in Scopus Google Scholar
    [17]
    S. Helgason
    Integral Geometry and Radon Transforms
    Springer-Verlag, New York (2011), 10.1007/978-1-4419-6055-9
    View at publisher
    Google Scholar
    [18]
    B. Irie, S. Miyake
    Capabilities of three-layered perceptrons
    IEEE Int. Conf. Neural Networks, IEEE (1988), pp. 641-648, 10.1109/ICNN.1988.23901
    View at publisher
    View in Scopus Google Scholar
    [19]
    K.-I. Funahashi
    On the approximate realization of continuous mappings by neural networks
    Neural Netw., 2 (3) (1989), pp. 183-192, 10.1016/0893-6080(89)90003-8
    View PDF View article View in Scopus Google Scholar
    [20]
    L.K. Jones
    A simple lemma on greedy approximation in Hilbert space and convergence rates for projection pursuit regression and neural network training
    Ann. Statist., 20 (1) (1992), pp. 608-613, 10.1214/aos/1176348546
    View at publisher
    Google Scholar
    [21]
    A.R. Barron
    Universal approximation bounds for superpositions of a sigmoidal function
    IEEE Trans. Inform. Theory, 39 (3) (1993), pp. 930-945, 10.1109/18.256500
    View at publisher
    View in Scopus Google Scholar
    [22]
    P.C. Kainen, V. Kůrková, M. Sanguineti
    Approximating multivariable functions by feedforward neural nets
    Handb. Neural Inf. Process., vol. 49, Springer, Berlin, Heidelberg (2013), pp. 143-181, 10.1007/978-3-642-36657-4
    View at publisher
    View in Scopus Google Scholar
    [23]
    E.J. Candès
    Harmonic analysis of neural networks
    Appl. Comput. Harmon. Anal., 6 (2) (1999), pp. 197-218, 10.1006/acha.1998.0248
    View PDF View article View in Scopus Google Scholar
    [24]
    E.J. Candès
    Ridgelets: theory and applications
    Ph.D. thesis
    Standford University (1998)
    Google Scholar
    [25]
    B. Rubin
    The Calderón reproducing formula, windowed X-ray transforms, and radon transforms in L p -spaces
    J. Fourier Anal. Appl., 4 (2) (1998), pp. 175-197, 10.1007/BF02475988
    View at publisher
    View in Scopus Google Scholar
    [26]
    D.L. Donoho
    Tight frames of k -plane ridgelets and the problem of representing objects that are smooth away from d -dimensional singularities in R n
    Proc. Natl. Acad. Sci. USA, 96 (5) (1999), pp. 1828-1833, 10.1073/pnas.96.5.1828
    View at publisher
    View in Scopus Google Scholar
    [27]
    D.L. Donoho
    Ridge functions and orthonormal ridgelets
    J. Approx. Theory, 111 (2) (2001), pp. 143-179, 10.1006/jath.2001.3568
    View PDF View article View in Scopus Google Scholar
    [28]
    J.-L. Starck, F. Murtagh, J.M. Fadili
    The ridgelet and curvelet transforms
    Sparse Image Signal Process. Wavelets, Curvelets, Morphol. Divers., Cambridge University Press (2010), pp. 89-118, 10.1017/CBO9780511730344.006
    View at publisher
    Google Scholar
    [29]
    B. Rubin
    Convolution backprojection method for the k -plane transform, and Calderón's identity for ridgelet transforms
    Appl. Comput. Harmon. Anal., 16 (3) (2004), pp. 231-242, 10.1016/j.acha.2004.03.003
    View PDF View article View in Scopus Google Scholar
    [30]
    S. Kostadinova, S. Pilipović, K. Saneva, J. Vindas
    The ridgelet transform of distributions
    Integral Transforms Spec. Funct., 25 (5) (2014), pp. 344-358, 10.1080/10652469.2013.853057
    View at publisher
    View in Scopus Google Scholar
    [31]
    S. Kostadinova, S. Pilipović, K. Saneva, J. Vindas
    The ridgelet transform and quasiasymptotic behavior of distributions
    Oper. Theory Adv. Appl., 245 (2015), pp. 185-197, 10.1007/978-3-319-14618-8_13
    View at publisher
    View in Scopus Google Scholar
    [32]
    L. Schwartz
    Théorie des Distributions
    (nouvelle edition), Hermann, Paris (1966)
    Google Scholar
    [33]
    F. Trèves
    Tological Vector Spaces, Distributions and Kernels
    Academic Press (1967)
    Google Scholar
    [34]
    W. Rudin
    Functional Analysis
    (2nd edition), Higher Mathematics Series, McGraw–Hill Education (1991)
    Google Scholar
    [35]
    H. Brezis
    Functional Analysis, Sobolev Spaces and Partial Differential Equations
    (1st edition), Universitext, Springer-Verlag, New York (2011), 10.1007/978-0-387-70914-7
    View at publisher
    Google Scholar
    [36]
    K. Yosida
    Functional Analysis
    (6th edition), Springer-Verlag, Berlin, Heidelberg (1995), 10.1007/978-3-642-61859-8
    View at publisher
    Google Scholar
    [37]
    W. Yuan, W. Sickel, D. Yang
    Morrey and Campanato Meet Besov, Lizorkin and Triebel
    Lecture Notes in Mathematics, Springer, Berlin, Heidelberg (2010), 10.1007/978-3-642-14606-0
    View at publisher
    Google Scholar
    [38]
    M. Holschneider
    Wavelets: An Analysis Tool
    Oxford Mathematical Monographs, The Clarendon Press (1995)
    Google Scholar
    [39]
    A. Hertle
    Continuity of the radon transform and its inverse on Euclidean space
    Math. Z., 184 (2) (1983), pp. 165-192, 10.1007/BF01252856
    View at publisher
    View in Scopus Google Scholar
    [40]
    L. Grafakos
    Classical Fourier Analysis
    (2nd edition), Graduate Texts in Mathematics, Springer, New York (2008), 10.1007/978-0-387-09432-8
    View at publisher
    Google Scholar
    [41]
    I.M. Gel'fand, G.E. Shilov
    Properties and Operations, Generalized Functions, vol. 1
    Academic Press, New York (1964)
    Google Scholar
    [42]
    L.A. Shepp, B.F. Logan
    The Fourier reconstruction of a head section
    IEEE Trans. Nucl. Sci., 21 (3) (1974), pp. 21-43, 10.1109/TNS.1974.6499235
    View at publisher
    View in Scopus Google Scholar
    [43]
    E.M. Stein
    Singular Integrals and Differentiability Properties of Functions
    Princeton Mathematical Series (PMS), Princeton University Press (1970)
    Google Scholar

Cited by (292)

    A unified Fourier slice method to derive ridgelet transform for a variety of depth-2 neural networks
    2024, Journal of Statistical Planning and Inference
    Show abstract

    To investigate neural network parameters, it is easier to study the distribution of parameters than to study the parameters in each neuron. The ridgelet transform is a pseudo-inverse operator that maps a given function to the parameter distribution so that a network reproduces , i.e. . For depth-2 fully-connected networks on a Euclidean space, the ridgelet transform has been discovered up to the closed-form expression, thus we could describe how the parameters are distributed. However, for a variety of modern neural network architectures, the closed-form expression has not been known. In this paper, we explain a systematic method using Fourier expressions to derive ridgelet transforms for a variety of modern networks such as networks on finite fields , group convolutional networks on abstract Hilbert space , fully-connected networks on noncompact symmetric spaces , and pooling layers, or the -plane ridgelet transform.
    A ReLU-based linearization approach for maximizing oil production in subsea platforms: An application to flow splitting
    2024, Chemical Engineering Science
    Show abstract
    Predicting Covid-19 pandemic waves with biologically and behaviorally informed universal differential equations
    2024, Heliyon
    Show abstract
    Design of aluminum plate phononic crystals with wide bandgaps via free-form shape optimization using deep neural networks
    2023, Extreme Mechanics Letters
    Show abstract
    Deep reinforcement learning for adaptive mesh refinement
    2023, Journal of Computational Physics
    Show abstract
    Disentangling private classes through regularization
    2023, Neurocomputing
    Show abstract

View all citing articles on Scopus
© 2015 Elsevier Inc.
Recommended articles

    Bidirectional numerical conformal mapping based on the dipole simulation method
    Engineering Analysis with Boundary Elements, Volume 114, 2020, pp. 45-57
    Koya Sakakibara
    View PDF
    Visualization, Discriminability and Applications of Interpretable Saak Features
    Journal of Visual Communication and Image Representation, Volume 66, 2020, Article 102699
    Abinaya Manimaran , …, C.-C. Jay Kuo
    View PDF
    Approximation capability of two hidden layer feedforward neural networks with fixed weights
    Neurocomputing, Volume 316, 2018, pp. 262-269
    Namig J. Guliyev , Vugar E. Ismailov
    View PDF

Show 3 more articles
Article Metrics
Citations

    Citation Indexes: 180 

Captures

    Readers: 199 

Mentions

    References: 3 

Social Media

    Shares, Likes & Comments: 92 

plumX logo
View details
Elsevier logo with wordmark

    About ScienceDirect
    Remote access
    Shopping cart
    Advertise
    Contact and support
    Terms and conditions
    Privacy policy 

Cookies are used by this site. Cookie Settings

All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply.
RELX group home page
