
Skip to main content
Cornell University
We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate
arxiv logo > stat > arXiv:1607.06450

Help | Advanced Search
Search
Statistics > Machine Learning
(stat)
[Submitted on 21 Jul 2016]
Title: Layer Normalization
Authors: Jimmy Lei Ba , Jamie Ryan Kiros , Geoffrey E. Hinton
View a PDF of the paper titled Layer Normalization, by Jimmy Lei Ba and 2 other authors
View PDF

    Abstract: Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques. 

Subjects: 	Machine Learning (stat.ML) ; Machine Learning (cs.LG)
Cite as: 	arXiv:1607.06450 [stat.ML]
  	(or arXiv:1607.06450v1 [stat.ML] for this version)
  	https://doi.org/10.48550/arXiv.1607.06450
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Jimmy Ba [ view email ]
[v1] Thu, 21 Jul 2016 19:57:52 UTC (305 KB)
Full-text links:
Access Paper:

    View a PDF of the paper titled Layer Normalization, by Jimmy Lei Ba and 2 other authors
    View PDF
    TeX Source
    Other Formats 

view license
Current browse context:
stat.ML
< prev   |   next >
new | recent | 1607
Change to browse by:
cs
cs.LG
stat
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

10 blog links
( what is this? )
a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

