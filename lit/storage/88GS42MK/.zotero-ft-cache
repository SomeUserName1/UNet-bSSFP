
Skip to main content
Cornell University
We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate
arxiv logo > cs > arXiv:2205.12952

Help | Advanced Search
Search
Computer Science > Computer Vision and Pattern Recognition
(cs)
[Submitted on 25 May 2022]
Title: Pretraining is All You Need for Image-to-Image Translation
Authors: Tengfei Wang , Ting Zhang , Bo Zhang , Hao Ouyang , Dong Chen , Qifeng Chen , Fang Wen
View a PDF of the paper titled Pretraining is All You Need for Image-to-Image Translation, by Tengfei Wang and 6 other authors
View PDF

    Abstract: We propose to use pretraining to boost general image-to-image translation. Prior image-to-image translation methods usually need dedicated architectural design and train individual translation models from scratch, struggling for high-quality generation of complex scenes, especially when paired training data are not abundant. In this paper, we regard each image-to-image translation problem as a downstream task and introduce a simple and generic framework that adapts a pretrained diffusion model to accommodate various kinds of image-to-image translation. We also propose adversarial training to enhance the texture synthesis in the diffusion model training, in conjunction with normalized guidance sampling to improve the generation quality. We present extensive empirical comparison across various tasks on challenging benchmarks such as ADE20K, COCO-Stuff, and DIODE, showing the proposed pretraining-based image-to-image translation (PITI) is capable of synthesizing images of unprecedented realism and faithfulness. 

Comments: 	Project Page: this https URL
Subjects: 	Computer Vision and Pattern Recognition (cs.CV)
Cite as: 	arXiv:2205.12952 [cs.CV]
  	(or arXiv:2205.12952v1 [cs.CV] for this version)
  	https://doi.org/10.48550/arXiv.2205.12952
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Tengfei Wang [ view email ]
[v1] Wed, 25 May 2022 17:58:26 UTC (16,670 KB)
Full-text links:
Access Paper:

    View a PDF of the paper titled Pretraining is All You Need for Image-to-Image Translation, by Tengfei Wang and 6 other authors
    View PDF
    TeX Source
    Other Formats 

view license
Current browse context:
cs.CV
< prev   |   next >
new | recent | 2205
Change to browse by:
cs
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

