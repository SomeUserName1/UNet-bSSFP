This is a repository copy of Simultaneous super-resolution and cross-modality synthesis of 3D medical images using weakly-supervised joint convolutional sparse coding. White Rose Research Online URL for this paper: http://eprints.whiterose.ac.uk/135019/ Version: Published Version
Proceedings Paper: Huang, Y, Shao, L and Frangi, AF orcid.org/0000-0002-2675-528X (2017) Simultaneous super-resolution and cross-modality synthesis of 3D medical images using weakly-supervised joint convolutional sparse coding. In: Proceedings of 30th IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017). CVPR 2017, 21-26 Jul 2017, Honolulu, Hawaii, USA. IEEE , pp. 5787-5796. ISBN 9781538604571 https://doi.org/10.1109/CVPR.2017.613
Reuse Items deposited in White Rose Research Online are protected by copyright, with all rights reserved unless indicated otherwise. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. This is indicated by the licence information on the White Rose Research Online record for the item. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request.
eprints@whiterose.ac.uk https://eprints.whiterose.ac.uk/

Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Images using Weakly-Supervised Joint Convolutional Sparse Coding
Yawen Huang1, Ling Shao2, Alejandro F. Frangi1 1Department of Electronic and Electrical Engineering, The University of Shefﬁeld, UK
2School of Computing Sciences, University of East Anglia, UK
{yhuang36, a.frangi}@sheffield.ac.uk, ling.shao@uea.ac.uk

Abstract
Magnetic Resonance Imaging (MRI) offers highresolution in vivo imaging and rich functional and anatomical multimodality tissue contrast. In practice, however, there are challenges associated with considerations of scanning costs, patient comfort, and scanning time that constrain how much data can be acquired in clinical or research studies. In this paper, we explore the possibility of generating high-resolution and multimodal images from low-resolution single-modality imagery. We propose the weakly-supervised joint convolutional sparse coding to simultaneously solve the problems of super-resolution (SR) and cross-modality image synthesis. The learning process requires only a few registered multimodal image pairs as the training set. Additionally, the quality of the joint dictionary learning can be improved using a larger set of unpaired images1. To combine unpaired data from different image resolutions/modalities, a hetero-domain image alignment term is proposed. Local image neighborhoods are naturally preserved by operating on the whole image domain (as opposed to image patches) and using joint convolutional sparse coding. The paired images are enhanced in the joint learning process with unpaired data and an additional maximum mean discrepancy term, which minimizes the dissimilarity between their feature distributions. Experiments show that the proposed method outperforms state-of-the-art techniques on both SR reconstruction and simultaneous SR and cross-modality synthesis.
1. Introduction
With the rapid progress in Magnetic Resonance Imaging (MRI), there are a multitude of mechanisms to generate tissue contrast that are associated with various anatomical
1Unpaired data/images: acquisitions are from different subjects without registration. Paired data/images: acquisitions of the same subject obtained from different modalities are registered.

or functional features. However, the acquisition of a complete multimodal set of high-resolution images faces constraints associated with scanning costs, scanner availability, scanning time, and patient comfort. In addition, long-term longitudinal studies such as ADNI [24] imply that changes exist in the scanner or acquisition protocol over time. In these situations, it is not uncommon to have images of the same subject but obtained from different sources, or to be confronted with missing or corrupted data from earlier time points. In addition, high-resolution (HR) 3D medical imaging usually requires long breath-hold and repetition times, which lead to long-term scanning times that are challenging or unfeasible in clinical routine. Acquiring low-resolution (LR) images and/or skipping some imaging modalities altogether from the acquisition are then not uncommon. In all such scenarios, it is highly desirable to be able to generate HR data from the desired target modality from the given LR modality data.
The relevant literature in this area can be divided into either super-resolution (SR) reconstruction from single/multiple image modalities or cross-modality (image) synthesis (CMS). On the one hand, SR is typically concerned with achieving improved visual quality or overcoming the resolution limits of the acquired image data. Such a problem is generally under-determined and ill-posed, hence, the solution is not unique. To mitigate this fact, the solution space needs to be constrained by incorporating strong priors. Prior information comes in the form of smoothness assumptions as in, for example, interpolationbased SR [20, 28]. State-of-the-art methods mostly adopt either external data or internal data to guide the learning algorithms [25, 30]. On the other hand, due to variations in optimal image representations across modalities, the learned image model from one modality data may not be the optimal model for a different modality. How to reveal the relationship between different representations of the underlying image information is a major research issue to be explored. In order to synthesize one modality from another, recent methods in CMS proposed utilizing non-parametric

16070

methods like nearest neighbor (NN) search [8], nonlinear regression forests [19], coupled dictionary learning [26], and convolutional neural network (CNN) [10], to name a few. Although these algorithms achieve remarkable results, most of them suffer from the fundamental limitations associated with supervised learning and/or patch-based synthesis. Supervised approaches require a large number of training image pairs, which is impractical in many medical imaging applications. Patch-based synthesis suffers from inconsistencies introduced during the fusion process that takes place in areas where patches overlap.
In this paper, we propose a weakly-supervised convolutional sparse coding method with an application to neuroimaging that utilizes a small set of registered multimodal image pairs and solves the SR and CMS problems simultaneously. Rather than factorizing each patch into a linear combination of patches drawn from a dictionary built under sparsity constraints (sparse coding), or requiring a training set with fully registered multimodal image pairs, or requiring the same sparse code to be used for both modalities involved, we generate a uniﬁed learning model that automatically learns a joint representation for heterogeneous data (e.g., different resolutions, modalities and relative poses). This representation is learned in a common feature space that preserves the local consistency of the images. Speciﬁcally, we utilize the co-occurrence of texture features across both domains. A manifold ranking method picks features of the target domain from the most similar subjects in the source domain. Once the correspondence between images in different domains is established, we directly work on a whole image representation that intrinsically respects local neighborhoods. Furthermore, a mapping function is learned that links the representations between the two modalities involved. We call the proposed method WEakly-supErvised joiNt convolutIonal sparsE coding (WEENIE), and perform extensive experiments to verify its performance.
The main contributions of this paper are as follows: 1) This is the ﬁrst attempt to jointly solve the SR and CMS problems in 3D medical imaging using weakly-supervised joint convolutional sparse coding; 2) To exploit unpaired images from different domains during the learning phase, a hetero-domain image alignment term is proposed, which allows identifying correspondences across source and target domains and is invariant to pose transformations; 3) To map LR and HR cross-modality image pairs, joint learning based on convolutional sparse coding is proposed that includes a maximum mean discrepancy term; 4) Finally, extensive experimental results show that the proposed model yields better performance than state-of-the-art methods in both reconstruction error and visual quality assessment measures.
2. Related Work
With the goal to transfer the modality information from the source domain to the target domain, recent devel-

opments in CMS, such as texture synthesis [6, 10, 13], face photo-sketch synthesis [9, 36], and multi-modal retrieval [23, 29], have shown promising results. In this paper, we focus on the problems of image super-resolution and cross-modality synthesis, so only review related methods on these two aspects.
Image Super-Resolution: The purpose of image SR is to reconstruct an HR image from its LR counterpart. According to the image priors, image SR methods can be grouped into two main categories: interpolationbased, external or internal data driven learning methods. Interpolation-based SR works, including the classic bilinear [21], bicubic [20], and some follow-up methods [28, 41], interpolate much denser HR grids by the weighted average of the local neighbors. Most modern image SR methods have shifted from interpolation to learning based. These methods focus on learning a compact dictionary or manifold space to relate LR/HR image pairs, and presume that the lost high-frequency (HF) details of LR images can be predicted by learning from either external datasets or internal self-similarity. The external data driven SR approaches [3, 7, 38] exploit a mapping relationship between LR and HR image pairs from a speciﬁed external dataset. In the pioneer work of Freeman et al. [7], the NN of an LR patch is found, with the corresponding HR patch, and used for estimating HF details in a Markov network. Chang et al. [3] projected multiple NNs of the local geometry from the LR feature space onto the HR feature space to estimate the HR embedding. Furthermore, sparse coding-based methods [27, 38] were explored to generate a pair of dictionaries for LR and HR patch pairs to address the image SR problem. Wang et al. [35] and Huang et al. [14] further suggested modeling the relationship between LR and HR patches in the feature space to relax the strong constraint. Recently, an efﬁcient CNN based approach was proposed in [5], which directly learned an end-to-end mapping between LR and HR images to perform complex nonlinear regression tasks. For internal dataset driven SR methods, this can be built using the similarity searching [25] and/or scale-space pyramid of the given image itself [15].
Cross-Modality Synthesis: In parallel, various CMS methods have been proposed for synthesizing unavailable modality data from available source images, especially in the medical imaging community [26, 33, 34]. One of the well-established modality transformation approaches is the example-based learning method generated by Freeman et al. [8]. Given a patch of a test image, several NNs with similar properties are picked from the source image space to reconstruct the target one using Markov random ﬁelds. Roy et al. [26] used sparse coding for desirable MR contrast synthesis assuming that cross-modality patch pairs have same representations and can be directly used for training dictionaries to estimate the contrast of the target modality. Sim-

26071

Paired

Unpaired

Common Space

RKHS

Mapping W

Convolutional Decomposition
Feature Convert via W
Convolution

Figure 1. Flowchart of the proposed method (WEENIE) for simultaneous SR and cross-modality synthesis.

ilar work was also used in [17]. In [1], a canonical correlation analysis-based approach was proposed to yield a feature space that can get underlying common structures of co-registered data for better correlation of dictionary pairs. More recently, a location-sensitive deep network [33] has been put forward to explicitly utilize the voxel image coordinates by incorporating image intensities and spatial information into a deep network for synthesizing purposes. Gatys et al. [10] introduced a CNN algorithm of artistic style, that new images can be generated by performing a pre-image search in high-level image content to match generic feature representations of example images. In addition to the aforementioned methods, most CMS algorithms rely on the strictly registered pairs to train models. As argued in [34], it would be preferable to use an unsupervised approach to deal with input data instead of ensuring data to be coupled invariably.

3. Weakly-Supervised Joint Convolutional Sparse Coding

3.1. Preliminaries

Convolutional Sparse Coding (CSC) was introduced in the context of modeling receptive ﬁelds preciously, and later generalized to image processing, in which the representation of an entire image is computed by the sum of a set convolutions with dictionary ﬁlters. The goal of CSC is to remedy the shortcoming of conventional patch-based sparse coding methods by removing shift variations for consistent approximation of local neighbors on whole images. Concretely, given the vectorized image x, the problem of generating a set of vectorized ﬁlters for sparse feature maps is solved by minimizing the objective function that combines the squared reconstruction error and the l1-norm penalty on the representations:

arg

min
f ,z

1 2

K

2

K

x − fk ∗ zk + λ

zk 1

k=1

2

k=1

(1)

s.t.

fk

2 2

≤

1

∀k

=

{1,

..., K}

,

where x is an m × n image in vector form, fk refers to the k-th d × d ﬁlter in vector form, zk is the sparse feature map corresponding to fk with size (m + d − 1) × (n + d − 1)

to approximate x, λ controls the l1 penalty, and ∗ denotes the 2D convolution operator. f = f1T , ..., fKT T and z = zT1 , ..., zTK T are K ﬁlters and feature maps stacked as the single column vector, respectively. Here, the inequality constraint on each column of vectorized fk prevents the ﬁlter from absorbing all the energy of the system.
Similar to the original sparse coding problem, Zeiler et al. [39] proposed to solve the CSC in Eq. (1) through alternatively optimizing one variable while ﬁxing the other one in the spatial domain. Advances in recent fast convolutional sparse coding (FCSC) [2] have shown that feature learning can be efﬁciently and explicitly solved by incorporating CSC within an alternating direction method of multipliers (ADMMs) framework in the Fourier domain.
3.2. Problem Formulation
The simultaneous SR and cross-modality synthesis problem can be formulated as: given a three-dimensional LR image X of modality M1, the task is to infer from X a target 3D image Y that is as similar as possible to the HR ground truth of desirable modality M2. Suppose that we are given a group of LR images of modality M1, i.e., X = [X1, ..., XP ] ∈ Rm×n×t×P , and a set of HR images of modality M2, i.e., Y = [Y2, ..., YQ] ∈ Rm×n×t×Q. P and Q are the numbers of samples in the training sets, and m, n denote the dimensions of axial view of each image, while t is the size of the image along the z-axis. Moreover, in both training sets, subjects of source modality M1 are mostly different from target modality M2, that is, we are working with a small number of paired data while most of them are unpaired. Therefore, the difﬁculties of this problem vary with hetero-domain images, e.g., resolutions and modalities, and how well the two domains ﬁt. To bridge image appearances across heterogeneous representations, we propose a method for automatically establishing a one-toone correlation between data in X and Y ﬁrstly, then employ the aligned data to jointly learn a pair of ﬁlters, while assuming that there exists a mapping function F (·) for associating and predicting cross-modality data in the projected common feature space. Particularly, we want to synthesize MRI of human brains in this paper. An overview of our proposed work is depicted in Fig. 1.
Notation: For simplicity, we denote matrices and 3D im-

36072

ages as upper-case bold (e.g., image X), vectors and vectorized 2D images as lower-case bold (e.g., ﬁlter f ), and scalars as lower-case (e.g., the number of ﬁlter k). Image with modality M1 called source modality belongs to the source domain, and with modality M2 called target modality belongs to the target domain.

3.3. Hetero-Domain Image Alignment

The design of an alignment A (·) from X to Y requires

a combination of extracting common components from

LR/HR images and some measures of correlation between

both modalities. In SR literature, common components are

usually accomplished by extracting high-frequency (HF)

edges and texture features from LR/HR images, respec-

tively [3, 38]. In this paper, we adopt ﬁrst- and second-

order derivatives involving horizontal and vertical gradi-

ents as the features for LR images by Xhpf = G ∗ Xp.

G=

G11, G21 G12, G22

, and each gradient G has the same length

of z-axis as input image while g11 = [−1, 0, 1], g12 = g11T , and g21 = [−2, −1, 0, 1, 2], g22 = g21T . For HR images,

HF features are obtained through directly subtracting mean

value, i.e., Yphf = Yp − mean(Yp). To deﬁne the heterodomain image alignment term A (·), we assume that the in-

trinsic structures of brain MRI of a subject across image

modalities are also similar in the HF space since images of

different modalities are more likely to be described differ-

ently by features. When HF features of both domains are

obtained, it is possible to build a way for cross-modality

data alignment (in particular, a unilateral cross-modality

matching can be thought as a special case in [16]). To this

end, we deﬁne a subject-speciﬁc transformation matrix A as

K(Xh1f , Y1hf ) · · · K(Xh1f , YQhf )

A= 

...

...

...

 , (2) 

K(XhPf , Y1hf ) · · · K(XhPf , YQhf )

where K(Xhpf , Yqhf ) is used for measuring the distances between each pair of HF data in X and Y computed by the
Gaussian kernel as

K(Xhpf , Yqhf )

=

√1

e | | , −

Xhpf −Yqhf 2σ2

2

( 2πσ)3

(3)

where σ determines the width of Gaussian kernel. In order to establish a one-to-one correspondence across different domains, for each element of X , the most relevant image with maximum K from Y is preserved while discarding the rest of the elements:

max (K (1, :))



A= 

...

 , (4) 

max (K (P, :))

where max (K (p, :)) denotes the maximum element of the

p-th row of A. We further set max (K (p, :)) to 1, and

all the blank elements to 0. Therefore, A is a binary ma-

trix. Since A is calculated in a subject-speciﬁc manner,

each subject of X can only be connected to one target of

the most similar brain structures. Hence, images under a

hetero-domain can be treated as being the registered pairs,

i.e., Pi = {Xi, Yi}Pi=1, by constructing virtual correspon-

dence: A(X , Y) =

Xhf − AYhf

2 2

.

3.4. Objective Function

For image modality transformation, coupled sparse coding [18, 38] has important advantages, such as reliability of correspondence dictionary pair learning and less memory cost. However, the arbitrarily aligned bases related to the small part of images may lead to shifted versions of the same structures or inconsistent representations based on the overlapped patches. CSC [39] was then proposed to generate a global decomposition framework based on the whole image for solving the above problem. In spired by CSC and the beneﬁts of coupled sparsity [18], we introduce a joint convolutional sparse coding method in a weakly-supervised setting for hetero-domain images. The small number of originally registered pairs are used to carry the intrinsic relationship between X and Y while the majority of unpaired data are introduced to exploit and enhance the diversity of the original learning system.
Assume that the aforementioned alignment approach leads to a perfect correspondence across X and Y, such that each aligned pair of images possesses approximately identical (or the same for co-registered data) information. Moreover, to facilitate image mappings in a joint manner, we require sparse feature maps of each pair of corresponding source and target images to be associated. That is, suppose that there exists a mapping function F (·), where the feature maps of LR M1 modality images can be converted to their HR M2 versions. Given X and Y, we propose to learn a pair of ﬁlters with corresponding feature maps and a mapping function together with the aligned term by

arg

min

1

2 Fx,Fy ,Zx,Zy ,W

K

2

X − Fxk ∗ Zxk

k=1

F

+

1 2

K

2

K

Y − Fyk ∗ Zyk + β

Zyk − WkZxk

2 F

(5)

k=1

F

k=1

K

K

K

+λ

Zxk 1 +

Zyk 1 + γ

Wk

2 F

k=1

k=1

k=1

+

Xhf − AYhf

2 2

s.t.

fkx

2 2

≤

1,

fky

2 2

≤

1

∀k,

where Zxk and Zyk are the k-th sparse feature maps that estimate the aligned data terms X and Y when convolved

46073

with the k-th ﬁlters Fxk and Fyk of a ﬁxed spatial support, ∀k = {1, ..., K}. Concretely, X denotes the aligned im-

age from P with LR and M1 modality; Y denotes the aligned image from P containing HR and M2 modality. A convolution operation is represented as ∗ operator, and

· F denotes a Frobenius norm chosen to induce the convolutional least squares approximate solution. Fx and Fy are adopted to list all K ﬁlters, while Zx and Zy repre-

sent corresponding K feature maps for source and target

domains, respectively. A (X , Y) is combined to enforce the

correspondence for unpaired auxiliary subjects. The map-

ping function F projection Wk

(Zxk, Wk) of Zxk and

= Wk Zyk by

Zxk is modeled as a linear solving a set of the least

squares problem (i.e., minW

K k=1

Zyk − WkZxk

2 F

).

Pa-

rameters λ, β and γ balance sparsity, feature representation

and association mapping.

It is worth noting that Pi = {Xi, Yi} may not be perfect since HF feature alignment in Eq. (4) is not good enough

for very heterogeneous domain adaptation by matching the

ﬁrst- and second-order derivatives of X and means of Y,

which leads to suboptimal ﬁlter pairs and inaccurate re-

sults. To overcome such a problem, we need additional con-

straints to ensure the correctness of registered image pairs

produced by the alignment. Generally, when feature dif-

ference is substantially large, there always exists some sub-

jects of the source domain that are not particularly related

to target ones even in the HF subspace. Thus, a registered

subject pairs’ divergence assessment procedure should be

cooperated with the aforementioned joint learning model to

handle this difﬁcult setting. Recent works [4, 22, 42] have

performed instance/domain adaptation via measuring data

distribution divergence using the maximum mean discrep-

ancy (MMD) criterion. We follow such an idea and em-

ploy the empirical MMD as the nonparametric distribution

measure to handle the hetero-domain image pair mismatch

problem in the reproducing kernel Hilbert space (RKHS).

This is done by minimizing the difference between distri-

butions of aligned subjects while keeping dissimilar ’regis-

tered’ pairs (i.e., discrepant distributions) apart in the sparse

feature map space:

1P P

K

Wk(i)Zxk(i) − Zyk(i)

2 H

i=1 k=1

(6)

K

K

= (WkZxk)T MiZyk = T r( ZykM(WkZxk)T ),

k=1

k=1

where H indicates RKHS space, Zxk(i) and Zyk(i) are the paired sparse feature maps for Pi = {Xi, Yi} with i = 1, ...P , Mi is the i-th element of M while M denotes the MMD matrix and can be computed as follows

Mi =

1 P

,

−

1 P2

,

Zxk(i), Zyk(i) ∈ otherwise.

Pi

,

(7)

By regularizing Fyk are reﬁned and

Eq. the

(5) with Eq. distributions

(6), ﬁlter pairs of real aligned

Fxk and subject

pairs are drawn close under the new feature maps. Putting

the above together, we obtain the objective function:

arg

min

1

2 Fx,Fy ,Zx,Zy ,W

K

2

K

X − Fxk ∗ Zxk + γ

Wk

2 F

k=1

F

k=1

+

1 2

K

2

K

Y − Fyk ∗ Zyk + β

Zyk − WkZxk

2 F

k=1

F

k=1

K

K

K

+λ

Zxk 1 +

Zyk 1 + T r( ZykM(WkZxk)T )

k=1

k=1

k=1

+

Xhf − AYhf

2 2

s.t.

fkx

2 2

≤

1,

fky

2 2

≤

1

∀k.

(8)

3.5. Optimization

We propose a three-step optimization strategy for efﬁciently tackling the objective function in Eq. (8) (termed (WEENIE), summarized in Algorithm 1) considering that such multi-variables and uniﬁed framework cannot be jointly convex to F, Z, and W. Instead, it is convex with respect to each of them while ﬁxing the remaining variables.

3.5.1 Computing Convolutional Sparse Coding

Optimization involving only sparse feature maps Zx and Zy is solved by initialization of ﬁlters Fx, Fy and mapping function W (W is initialized as an identity matrix). Besides the original CSC formulation, we have additional terms associated with data alignment and divergence reducing in the common feature space. Eq. (8) is ﬁrstly converted to two regularized sub-CSC problems. Unfortunately, each of the problems constrained with an l1 penalty term cannot be directly solved, which is not rotation invariant. Recent approaches [2, 12] have been proposed to work around this problem on the theoretical derivation by introducing two auxiliary variables U and S to enforce the constraint inherent in the splitting. To facilitate component-wise multiplications, we exploit the convolution subproblem [2] in the Fourier domain2 derived within the ADMMs framework:

min 1 Zx 2

K

2

Xˆ − Fˆ xk ⊙ Zˆ xk +

k=1

F

Xhf − AYhf

2 2

K

K

+T r( Zˆ ykM(WkZˆ xk)T ) + β

Zˆ yk − WkZˆ xk

2 F

k=1

k=1

K

+λ

Uxk 1 s.t.

Sxk

2 2

≤

1,

Sxk

=

ΦT Fˆ xk,

Uxk

=

Zxk

∀k,

k=1

2Fast Fourier transform (FFT) is utilized to solve the relevant linear system and demonstrated substantially better asymptotic performance than processed in the spatial domain.

56074

min
Zy

1 2

K

2

Yˆ − Fˆ yk ⊙ Zˆ yk +

Xhf − AYhf

2 2

k=1

F

K

K

+T r( Zˆ ykM(WkZˆ xk)T ) + β

Zˆ xk − WkZˆ yk

2 F

k=1

k=1

K
+λ
k=1

Uyk 1 s.t.

Syk

2 2

≤

1, Syk

=

ΦT Fˆ yk, Uyk

=

Zyk

∀k,

(9)

whereˆapplied to any symbol indicates the discrete Fourier transform (DFT), for example Xˆ ← f (X), and f (·) denotes

the Fourier transform operator. ⊙ represents the Hadamard product (i.e., component-wise product), ΦT is the inverse

DFT matrix, and s projects a ﬁlter onto a small spatial support. By utilizing slack variables Uxk, Uyk and Sxk, Syk, the loss function can be treated as the sum of multiple subprob-

lems and with the addition of equality constraints.

3.5.2 Training Filters

Similar to theoretical CSC methods, we alternatively opti-
mize the convolutional least squares term for the basis function pairs Fx and Fy followed by an l1-regularized least squares term for the corresponding sparse feature maps Zx and Zy. Like the subproblem of solving feature maps, ﬁlter pairs can be learned in a similar fashion. With Zˆ xk, Zˆ yk and Wk ﬁxed, we can update the corresponding ﬁlter pairs Fˆ xk, and Fˆ yk as

min 1 Fx,Fy 2

Xˆ −

K

Fˆ xk ⊙ Zˆ xk

2

+

1 2

k=1

F

K

2

Yˆ − Fˆ yk ⊙ Zˆ yk

k=1

F

s.t.

fkx

2 2

≤

1,

fky

2 2

≤

1

∀k,

(10)

The optimization with respect to Eq. (10) can be solved

by a one-by-one update strategy [35] through an augmented

Lagrangian method [2].

3.5.3 Learning Mapping Function

Finally, Wk can be learned by ﬁxing Fxk, Fyk, and Zxk, Zyk:

K
min
W

Zyk − WkZxk

2 F

+

γ β

K

Wk

2 F

k=1

k=1

K

(11)

+T r( ZykM(WkZxk)T ),

k=1

where Eq. (11) is a ridge regression problem with a regular-

ization term. We simplify the regularization term R(tr) =

T r(

K k=1

Zyk M(Wk Zxk )T

)

and

analytically

derive

the

so-

lution

as

W

=

(Zyk Zxk T

−

R(tr))(Zxk Zxk T

+

γ β

I)−1,

where

I is an identity matrix.

Algorithm 1: WEENIE Algorithm

Input: Training data X and Y, parameters λ, γ, σ.

1 Initialize Fx0 , Fy0, Zx0 , Zy0, Ux0 , Uy0, Sx0 , Sy0, W0.

2 Perform FFT Zx0 → Zˆ x0 , Zy0 → Zˆ y0, Fx0 → Fˆ x0 ,

Fy0 → Fˆ y0, Ux0 → Uˆ x0 , Uy0 → Uˆ y0, Sx0 → Sˆx0 , Sy0 → Sˆy0. 3 Let Zˆ y0 ← WZˆ x0 .

4 while not converged do

5

Fix other variables, update Zˆ xk+1, Zˆ yk+1 and Uˆ xk+1,

Uˆ yk+1 by (9).

6

Fix other variables, update Fˆ xk+1, Fˆ yk+1 and Sˆxk+1,

Sˆyk+1 by (10) with Zˆ xk+1, Zˆ yk+1, Uˆ xk+1, Uˆ yk+1 and

Wk .

7 Fix other variables, update Wk by (11) with

Zˆ xk+1, Zˆ yk+1, Uˆ xk+1, Uˆ yk+1, Fˆ xk+1, Fˆ yk+1, and

Sˆxk+1, Sˆyk+1.

8

Inverse FFT Fˆ xk+1 → Fxk+1, Fˆ yk+1 → Fyk+1.

9 end

Output: Fx, Fy, W.

3.6. Synthesis

Once the training stage is completed, generating a set of ﬁlter pairs Fx, Fy and the mapping W, for a given test image Xt in domain X , we can synthesize its desirable HR
version of style Y. This is done by computing the sparse feature maps Zt of Xt with respect to a set of ﬁlters Fx, and associating Zt to the expected feature maps Zˆ t via W, i.e., Zˆ t ≈ WZt. Therefore, the desirable HR M2 modality
image is then obtained by the sum of K converted sparse feature maps Zˆ tk convolved with desired ﬁlters Fyk (termed (SRCMS) summarized in Algorithm 2):

K

K

Yt =

FykWkZtk =

FykZˆ tk.

k=1

k=1

(12)

Algorithm 2: SRCMS

Input: Test image Xt, ﬁlter pairs Fx and Fy,

mapping W.

1 Initialize Zt0.

2 Let Zˆ t0 ← WZt0, Y0t ← FyWZt0.

3 while not converged do

4

Update Ztk+1 and Zˆ tk+1 by (9) with Ykt , and W.

5

Update Ykt +1 ← WZˆ tk+1.

6 end

7 Synthesize Yt by (12).

Output: Synthesized image Yt.

66075

Ground Truth (PSNR, SSIM)

ScSR (31.60, 0.9354)

Zeyde (33.68, 0.9544)

ANR (34.16, 0.9569)

NE+LLE (34.12, 0.9555)

A+ (34.70, 0.9599)

CSC-SR (34.75, 0.9601)

WEENIE (35.34, 0.9632)

Figure 2. Example SR results and corresponding PSNRs, SSIMs

(zoom in for details).

4. Experimental Results

We conduct the experiments using two datasets, i.e., IXI3 and NAMIC brain mutlimodality4 datasets. Following [11, 35, 38], LR counterparts are directly down-sampled from their HR ground truths with rate 1/2 by bicubic interpolation, boundaries are padded (with eight pixels) to avoid the boundary effect of Fourier domain implementation. The regularization parameters σ, λ, β, and γ are empirically set to be 1, 0.05, 0.1, 0.15, respectively. Optimization variables F, S, Z, and U are randomly initialized with Gaussian noise considering [2]. Generally, a larger number of ﬁlters leads to better results. To balance between computation complexity and result quality, we learn 800 ﬁlters following [11]. In our experiments, we perform a more challenging division by applying half of the dataset (processed to be weakly co-registered data) for training while the remaining for testing. To the best of our knowledge, there is no previous work specially designed for SR and cross-modality synthesis simultaneously by learning from the weakly-supervised data. Thus, we extend the range of existing works as the baselines for fair comparison, which can be divided into two categories as follows: (1) brain MRI SR; (2) SR and cross-modality synthesis (one-by-one strategy in comparison models). For the evaluation criteria, we adopt the widely used PSNR and SSIM [37] indices to objectively assess the quality of the synthesized images.
Experimental Data: The IXI dataset consists of 578 256 × 256 × n MR healthy subjects collected at three hospitals with different mechanisms (i.e., Philips 3T system, Philips 1.5T system, and GE 3T system). Here, we utilize 180 Proton Density-weighted (PD-w) MRI subjects for image SR, while applying both PD-w and registered T2weighted (T2-w) MRI scans of all subjects for major SRCMS. Further, we conduct SRCMS experiments on the processed NAMIC dataset, which consists of 20 128×128×m subjects in both T1-weighted (T1-w) and T2-w modalities. As mentioned, we leave half of the dataset out for crossvalidation. We randomly select 30 registered subject pairs

3http://brain-development.org/ixi-dataset/ 4http://hdl.handle.net/1926/1687

Figure 3. Performance comparisons of different SR approaches.

Metric(avg.)
PSNR(dB) SSIM

ScSR [38]
31.63 0.9654

Zeyde [40]
33.68 0.9623

ANR [31]
34.09 0.9433

NE+LLE [3]
34.00 0.9623

A+ [32]
34.55 0.9591

CSC-SR [11]
34.60 0.9604

WEENIE
35.13 0.9681

Table 1. Quantitative evaluation (PSNR and SSIM): WEENIE vs. other SR methods on 95 subjects of the IXI dataset.

for IXI, and 3 registered subject pairs for NAMIC, respectively, from the half of the corresponding dataset for training purposes, and process the reminding training data to be unpaired. Particularly, all the existing methods with respect to cross-modality synthesis in brain imaging request a preprocessing, i.e., skull stripping and/or bias corrections, as done in [34, 26]. We follow such processes and further validate whether pre-processing (especially skull stripping) is always helpful for brain image synthesis.

4.1. Brain MRI Super-Resolution

For the problem of image SR, we focus on the PD-w subjects of the IXI dataset to compare the proposed WEENIE model with several state-of-the-art SR approaches: sparse coding-based SR method (ScSR) [38], anchored neighborhood regression method (ANR) [31], neighbor embedding + locally linear embedding method (NE+LLE) [3], Zeyde’s method [40], convolutional sparse coding-based SR method (CSC-SR) [11], and adjusted anchored neighborhood regression method (A+) [32]. We perform image SR with scaling factor 2, and show visual results on an example slice in Fig. 2. The quantitative results for different methods are shown in Fig. 3, and the average PSNR and SSIM for all 95 test subjects are listed in Table 1. The proposed method, in the case of brain image SR, obtains the best PSNR and SSIM values. The improvements show that the MMD regularized joint learning property on CSC has more inﬂuence than the classic sparse coding-based methods as well as the state-of-the-arts. It states that using MMD combined with the joint CSC indeed improves the representation power of the learned ﬁlter pairs.

4.2. Simultaneous Super-Resolution and CrossModality Synthesis

To comprehensively test the robustness of the proposed WEENIE method, we perform SRCMS on both datasets involving six groups of experiments: (1) synthesizing SR T2-w image from LR PD-w acquisition and (2) vice versa; (3) generating SR T2-w image from LR PD-w input based on pre-processed data (i.e., skull strapping and bias correc-

76076

Source with preprocessing

Source

Target ground truth with Target ground truth pre-processing

MIMECS with pre- WEENIE with pre- WEENIE with pre-

processing

processing + reg only

processing

WEENIE

T2-w -> PD-w

PD-w -> T2-w

Figure 4. Visual comparison of synthesized results using different methods.

Metric(avg.)
PSNR(dB) SSIM

PD− >T2 T2− >PD

WEENIE

37.77

31.77

0.8634

0.8575

MIMECS 30.60 0.7944

IXI PD− >T2+PRE
WEENIE(reg) 30.93 0.8004

WEENIE 33.43 0.8552

MIMECS 29.85 0.7503

T2− >PD+PRE WEENIE(reg) 30.29 0.7612

WEENIE 31.00 0.8595

Metric(avg.)
PSNR(dB) SSIM

MIMECS 24.36 0.8771

T1− >T2 Ve-US Ve-S 26.51 27.14 0.8874 0.8934

NAMIC

WEENIE 27.30 0.8983

MIMECS 27.26 0.9166

T2− >T1 Ve-US Ve-S 27.81 29.04 0.9130 0.9173

WEENIE 30.35 0.9270

Table 2. Quantitative evaluation (PSNR and SSIM): WEENIE vs. other synthesis methods on IXI and NAMIC datasets.

Figure 5. SRCMS results: WEENIE vs. MIMECS on IXI dataset.

denoted by WEENIE(reg) (that can directly substantiate the beneﬁts of involving unpaired data) and the results using all training images with/without preprocessing for the proposed method against MIMECS, V-US and V-S in above six cases and demonstrate examples in Fig. 4 for visual inspection. The advantage of our method over the MIMECS shows, e.g., in white matter structures, as well as in the overall intensity proﬁle. We show the quantitative results in Fig. 5, and Fig. 6, and summarize the averaged values in Table 2, respectively. It can be seen that the performance of our algorithm is consistent across two whole datasets, reaching the best PSNR and SSIM for almost all subjects.

Figure 6. SRCMS: WEENIE vs. MIMECS on NAMIC dataset.
tions) and (4) vice versa; (5) synthesizing SR T1-w image from LR T2-w subject and (6) vice versa. The ﬁrst four sets of experiments are conducted on the IXI dataset while the last two cases are evaluated on the NAMIC dataset. The state-of-the-art synthesis methods include Vemulapalli’s supervised approach (V-S) [34], Vemulapalli’s unsupervised approach (V-US) [34] and MR image exampled-based contrast synthesis (MIMECS) [26] approach. However, Vemulapalli’s methods cannot be applied for our problem, because they only contain the cross-modality synthesis stage used in the NAMIC dataset. Original data (without degradation processing) are used in all Vemulapallis methods. MIMECS takes image SR into mind and adopts two independent steps (i.e. synthesis+SR) to solve the problem. We compare our results on only using registered image pairs

5. Conclusion
In this paper, we proposed a novel weakly-supervised joint convolutional sparse coding (WEENIE) method for simultaneous super-resolution and cross-modality synthesis (SRCMS) in 3D MRI. Different from conventional joint learning approaches based on sparse representation in supervised setting, WEENIE only requires a small set of registered image pairs and automatically aligns the correspondence for auxiliary unpaired images to span the diversities of the original learning system. By means of the designed hetero-domain alignment term, a set of ﬁlter pairs and the mapping function were jointly optimized in a common feature space. Furthermore, we integrated our model with a divergence minimization term to enhance robustness. With the beneﬁt of consistency prior, WEENIE directly employs the whole image, which naturally captures the correlation between local neighborhoods. As a result, the proposed method can be applied to both brain image SR and SRCMS problems. Extensive results showed that WEENIE can achieve superior performance against state-of-the-art methods.

86077

References
[1] K. Bahrami, F. Shi, X. Zong, H. W. Shin, H. An, and D. Shen. Hierarchical reconstruction of 7t-like images from 3t mri using multi-level cca and group sparsity. In International Conference on Medical Image Computing and ComputerAssisted Intervention, pages 659–666. Springer, 2015. 3
[2] H. Bristow, A. Eriksson, and S. Lucey. Fast convolutional sparse coding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 391–398, 2013. 3, 5, 6, 7
[3] H. Chang, D.-Y. Yeung, and Y. Xiong. Super-resolution through neighbor embedding. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, volume 1, pages I–I. IEEE, 2004. 2, 4, 7
[4] L. Chen, W. Li, and D. Xu. Recognizing rgb images by learning from rgb-d data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1418– 1425, 2014. 5
[5] C. Dong, C. C. Loy, K. He, and X. Tang. Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2):295–307, 2016. 2
[6] A. A. Efros and W. T. Freeman. Image quilting for texture synthesis and transfer. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 341–346. ACM, 2001. 2
[7] W. T. Freeman, T. R. Jones, and E. C. Pasztor. Examplebased super-resolution. IEEE Computer graphics and Applications, 22(2):56–65, 2002. 2
[8] W. T. Freeman, E. C. Pasztor, and O. T. Carmichael. Learning low-level vision. International journal of computer vision, 40(1):25–47, 2000. 2
[9] X. Gao, N. Wang, D. Tao, and X. Li. Face sketch–photo synthesis and retrieval using sparse representation. IEEE Transactions on circuits and systems for video technology, 22(8):1213–1226, 2012. 2
[10] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2414–2423, 2016. 2, 3
[11] S. Gu, W. Zuo, Q. Xie, D. Meng, X. Feng, and L. Zhang. Convolutional sparse coding for image super-resolution. In Proceedings of the IEEE International Conference on Computer Vision, pages 1823–1831, 2015. 7
[12] F. Heide, W. Heidrich, and G. Wetzstein. Fast and ﬂexible convolutional sparse coding. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5135–5143. IEEE, 2015. 5
[13] A. Hertzmann, C. E. Jacobs, N. Oliver, B. Curless, and D. H. Salesin. Image analogies. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 327–340. ACM, 2001. 2
[14] D.-A. Huang and Y.-C. Frank Wang. Coupled dictionary and feature space learning with applications to cross-domain image synthesis and recognition. In Proceedings of the IEEE

international conference on computer vision, pages 2496– 2503, 2013. 2 [15] J.-B. Huang, A. Singh, and N. Ahuja. Single image superresolution from transformed self-exemplars. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5197–5206. IEEE, 2015. 2 [16] Y. Huang, F. Zhu, L. Shao, and A. F. Frangi. Color object recognition via cross-domain learning on rgb-d images. In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 1672–1677. IEEE, 2016. 4 [17] J. E. Iglesias, E. Konukoglu, D. Zikic, B. Glocker, K. Van Leemput, and B. Fischl. Is synthesizing mri contrast useful for inter-modality analysis? In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 631–638. Springer, 2013. 3 [18] K. Jia, X. Wang, and X. Tang. Image transformation based on learning dictionaries across image spaces. IEEE transactions on pattern analysis and machine intelligence, 35(2):367–380, 2013. 4 [19] A. Jog, S. Roy, A. Carass, and J. L. Prince. Magnetic resonance image synthesis through patch regression. In 2013 IEEE 10th International Symposium on Biomedical Imaging, pages 350–353. IEEE, 2013. 2 [20] R. Keys. Cubic convolution interpolation for digital image processing. IEEE transactions on acoustics, speech, and signal processing, 29(6):1153–1160, 1981. 1, 2 [21] X. Li and M. T. Orchard. New edge-directed interpolation. IEEE transactions on image processing, 10(10):1521–1527, 2001. 2 [22] M. Long, G. Ding, J. Wang, J. Sun, Y. Guo, and P. S. Yu. Transfer sparse coding for robust image representation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 407–414, 2013. 5 [23] F. Monay and D. Gatica-Perez. Modeling semantic aspects for cross-media image indexing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(10):1802–1817, 2007. 2 [24] S. G. Mueller, M. W. Weiner, L. J. Thal, R. C. Petersen, C. Jack, W. Jagust, J. Q. Trojanowski, A. W. Toga, and L. Beckett. The alzheimer’s disease neuroimaging initiative. Neuroimaging Clinics of North America, 15(4):869– 877, 2005. 1 [25] F. Rousseau, A. D. N. Initiative, et al. A non-local approach for image super-resolution using intermodality priors. Medical image analysis, 14(4):594–605, 2010. 1, 2 [26] S. Roy, A. Carass, and J. L. Prince. Magnetic resonance image example-based contrast synthesis. IEEE transactions on medical imaging, 32(12):2348–2363, 2013. 2, 7, 8 [27] A. Rueda, N. Malpica, and E. Romero. Single-image superresolution of brain mr images using overcomplete dictionaries. Medical image analysis, 17(1):113–132, 2013. 2 [28] L. Shao and M. Zhao. Order statistic ﬁlters for image interpolation. In 2007 IEEE International Conference on Multimedia and Expo, pages 452–455. IEEE, 2007. 1, 2 [29] A. W. Smeulders, M. Worring, S. Santini, A. Gupta, and R. Jain. Content-based image retrieval at the end of the early years. IEEE Transactions on pattern analysis and machine intelligence, 22(12):1349–1380, 2000. 2

96078

[30] Y. Tang and L. Shao. Pairwise operator learning for patchbased single-image super-resolution. IEEE Transactions on Image Processing, 26(2):994–1003, 2017. 1
[31] R. Timofte, V. De Smet, and L. Van Gool. Anchored neighborhood regression for fast example-based super-resolution. In Proceedings of the IEEE International Conference on Computer Vision, pages 1920–1927, 2013. 7
[32] R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted anchored neighborhood regression for fast super-resolution. In Asian Conference on Computer Vision, pages 111–126. Springer, 2014. 7
[33] H. Van Nguyen, K. Zhou, and R. Vemulapalli. Cross-domain synthesis of medical images using efﬁcient location-sensitive deep network. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 677–684. Springer, 2015. 2, 3
[34] R. Vemulapalli, H. Van Nguyen, and S. Kevin Zhou. Unsupervised cross-modal synthesis of subject-speciﬁc scans. In Proceedings of the IEEE International Conference on Computer Vision, pages 630–638, 2015. 2, 3, 7, 8
[35] S. Wang, L. Zhang, Y. Liang, and Q. Pan. Semi-coupled dictionary learning with applications to image super-resolution and photo-sketch synthesis. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2216–2223. IEEE, 2012. 2, 6, 7
[36] X. Wang and X. Tang. Face photo-sketch synthesis and recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(11):1955–1967, 2009. 2
[37] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600–612, 2004. 7
[38] J. Yang, J. Wright, T. S. Huang, and Y. Ma. Image superresolution via sparse representation. IEEE transactions on image processing, 19(11):2861–2873, 2010. 2, 4, 7
[39] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus. Deconvolutional networks. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2528–2535. IEEE, 2010. 3, 4
[40] R. Zeyde, M. Elad, and M. Protter. On single image scale-up using sparse-representations. In International conference on curves and surfaces, pages 711–730. Springer, 2010. 7
[41] L. Zhang and X. Wu. An edge-guided image interpolation algorithm via directional ﬁltering and data fusion. IEEE transactions on Image Processing, 15(8):2226–2238, 2006. 2
[42] F. Zheng, Y. Tang, and L. Shao. Hetero-manifold regularisation for cross-modal hashing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016. 5
160079

