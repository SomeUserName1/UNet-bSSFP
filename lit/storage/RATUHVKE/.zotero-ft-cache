arXiv:1306.3574v1 [stat.ML] 15 Jun 2013

Early stopping and non-parametric regression: An optimal data-dependent stopping rule
Garvesh Raskutti† Martin J. Wainwright†,∗ Bin Yu†,∗
Department of Statistics†, and Department of EECS∗ UC Berkeley, Berkeley, CA 94720
Abstract
The strategy of early stopping is a regularization technique based on choosing a stopping time for an iterative algorithm. Focusing on non-parametric regression in a reproducing kernel Hilbert space, we analyze the early stopping strategy for a form of gradient-descent applied to the least-squares loss function. We propose a data-dependent stopping rule that does not involve hold-out or cross-validation data, and we prove upper bounds on the squared error of the resulting function estimate, measured in either the L2(P) and L2(Pn) norm. These upper bounds lead to minimax-optimal rates for various kernel classes, including Sobolev smoothness classes and other forms of reproducing kernel Hilbert spaces. We show through simulation that our stopping rule compares favorably to two other stopping rules, one based on hold-out data and the other based on Stein’s unbiased risk estimate. We also establish a tight connection between our early stopping strategy and the solution path of a kernel ridge regression estimator.
1 Introduction
The phenomenon of overﬁtting is ubiquitous throughout statistics. It is especially problematic in nonparametric problems, where some form of regularization is essential to prevent overﬁtting. In the problem of nonparametric regression, the most classical form of regularization is that of Tikhonov regularization, where a quadratic smoothness penalty is added to the least-squares loss. An alternative and algorithmic approach to regularization is based on early stopping of an iterative algorithm, such as gradient descent applied to the unregularized loss function. The main advantage of early stopping for regularization, as compared to penalized forms, is lower computational complexity.
The idea of early stopping has a fairly lengthy history, dating back to the 1970’s in the context of the Landweber iteration. (For instance, see the paper by Strand [35], with follow-up work by Anderssen and Prenter [1] as well as Wahba [38].) Early stopping has also been widely used in neural networks (e.g., [28]), in which stochastic gradient descent is used to estimate the network parameters. Past work provided intuitive arguments for the beneﬁts of early stopping. It was argued that each step of an iterative algorithm will reduce bias but increase variance, so early stopping ensures the variance of the estimator is not too high. However, prior to the 1990s, there had been little theoretical justiﬁcation for these claims. A more recent line of work has provided theoretical justiﬁcation for various types of early stopping, including boosting algorithms (e.g., [5, 10, 17, 22, 25, 42, 43]), greedy methods [3], gradient descent over reproducing kernel Hilbert spaces (e.g. [7, 13, 11, 37, 42]), the conjugate gradient algorithm [9], and the power method for eigenvalue computation [29]. Most relevant to our work is the work of Bu¨hlmann and Yu [10], who derived optimal mean-squared error bounds for L2-boosting with early stopping in the case of ﬁxed design regression. However, these optimal rates are based on an “oracle” stopping rule, one that
1

cannot be computed based on the data. Thus, their work left open the following natural question: is there a data-dependent and easily computable stopping rule that produces a minimax-optimal estimator?
The main contribution of this paper is to answer this question in the aﬃrmative for a certain class of non-parametric regression problems, in which the underlying regression function belongs to a reproducing kernel Hilbert space (RKHS). In this setting, a standard estimator is the method of kernel ridge regression (e.g., [39]), which minimizes a weighted sum of the least-squares loss with a squared Hilbert norm penalty as a regularizer. Instead of a penalized form of regression, we analyze early stopping of an iterative update that is equivalent to gradient descent on the leastsquares loss in an appropriately chosen coordinate system. By analyzing the mean-squared error of our iterative update, we derive a data-dependent stopping rule that provides the optimal trade-oﬀ between the estimated bias and variance at each iteration. In particular, our stopping rule is based on the ﬁrst time that a running sum of step-sizes after t steps increases above the critical trade-oﬀ between bias and variance. For Sobolev spaces and other types of kernel classes, we show that the function estimate obtained by this stopping rule achieves minimax-optimal estimation rates in both the empirical and population norms. Importantly, our stopping rule does not require the use of cross-validation or hold-out data.
In more detail, our ﬁrst main result (Theorem 1) provides bounds on the squared prediction error for all iterates prior to the stopping time, and a lower bound on the squared error for all iterations after the stopping time. These bounds are applicable to the case of ﬁxed design, where as our second main result (Theorem 2) provides similar types of upper bounds for randomly sampled covariates. These bounds are stated in terms of the squared L2(P) norm, as opposed to the prediction error or L2(Pn) (semi)norm deﬁned by the data. Both of these theorems apply to any reproducing kernel, and lead to speciﬁc predictions for diﬀerent kernel classes, depending on their eigendecay. For the case of low rank kernel classes and Sobolev spaces, we prove that our stopping rule yields a function estimate that achieves the minimax optimal rate (up to a constant pre-factor), so that the bounds from our analysis are essentially unimprovable. Our proof is based on a combination of analytic techniques [10] with techniques from empirical process theory [36]. We complement these theoretical results with simulation studies that compare its performance to other rules, in particular a method using hold-out data to estimate the risk, as well as a second method based on Stein’s Unbiased Risk Estimate (SURE). In our experiments for ﬁrst-order Sobolev kernels, we ﬁnd that our stopping rule performs favorably compared to these alternatives, especially as the sample size grows. Finally, in Section 3.4, we provide an explicit link between our early stopping strategy and the kernel ridge regression estimator.
2 Background and problem formulation
We begin by introducing some background on non-parametric regression and reproducing kernel Hilbert spaces, before turning to a precise formulation of the problem studied in this paper.
2.1 Non-parametric regression and kernel classes
Suppose that our goal is to use a covariate X ∈ X to predict a real-valued response Y ∈ R. We do so by using a function f : X → R, where the value f (x) represents our prediction of Y based on the realization X = x. In terms of mean-squared error, the optimal choice is the regression function
2

deﬁned by f ∗(x) : = E[Y | x]. In the problem of non-parametric regression with random design, we
observe n samples of the form {(xi, yi), i = 1, . . . , n}, each drawn independently from some joint distribution on the Cartesian product X × R, and our goal is to estimate the regression function f ∗. Equivalently, we observe samples of the form

yi = f ∗(xi) + wi, for i = 1, 2, . . . , n,

(1)

where wi : = yi − f ∗(xi) is a zero-mean noise random variable. Throughout this paper, we assume that the random variables wi are sub-Gaussian with parameter σ, meaning that

E[etwi] ≤ et2σ2/2 for all t ∈ R.

(2)

For instance, this sub-Gaussian condition is satisﬁed for normal variates wi ∼ N (0, σ2), but it also holds for various non-Gaussian random variables. Parts of our analysis also apply to the ﬁxed design setting, in which we condition on a particular realization {xi}ni=1 of the covariates.
In order to estimate the regression function, we make use of the machinery of reproducing
kernel Hilbert spaces [2, 39, 20]. Using P to denote the marginal distribution of the covariates, we consider a Hilbert space H ⊂ L2(P), meaning a family of functions g : X → R, with g L2(P) < ∞, and an associated inner product ·, · H under which H is complete. The space H is a reproducing kernel Hilbert space (RKHS) if there exists a symmetric function K : X × X → R+ such that: (a) for each x ∈ X , the function K(·, x) belongs to the Hilbert space H, and (b) we have the
reproducing relation f (x) = f, K(·, x) H for all f ∈ H. Any such kernel function must be positive semideﬁnite; under suitable regularity conditions, Mercer’s theorem [27] guarantees that the kernel
has an eigen-expansion of the form

∞

K(x, x′) = λkφk(x)φk(x′),

(3)

k=1

where λ1 ≥ λ2 ≥ λ3 ≥ . . . ≥ 0 are a non-negative sequence of eigenvalues, and {φk}∞ k=1 are the associated eigenfunctions, taken to be orthonormal in L2(P). The decay rate of the eigenvalues will

play a crucial role in our analysis.

sionSoinf ctehtehfeoremigefn(fxu)nc=tions∞ k={1φ√k }λ∞ kk=a1k

form an orthonormal basis, φk(x), where for all k such

any that

function λk > 0,

f ∈ H has an expanthe coeﬃcients

ak

:=

√1 λk

f, φk

L2(P) =

f (x)φk(x) d P(x)
X

are rescaled versions of

in H—where f =

∞ k=1

√the generalized λkakφk and g

Fourier

=

∞ k=1

√coeﬃcients. Associated with any two functions λkbkφk—are two distinct inner products. The

ﬁrst is the usual inner product in the space L2(P)—namely, f, g L2(P) : = X f (x)g(x) d P(x). By

Parseval’s theorem, it has an equivalent representation in terms of the expansion coeﬃcients and

kernel eigenvalues—that is,

∞
f, g L2(P) = λkakbk.
k=1

3

The second inner product, denoted f, g H, is the one that deﬁnes the Hilbert space; it can be written in terms of the expansion coeﬃcients as

∞
f, g H = akbk.
k=1

Using this deﬁnition, the Hilbert ball of radius 1 for the Hilbert space H with eigenvalues {λk}∞ k=1 and eigenfunctions {φk}∞ k=1 takes the form

∞

∞

BH(1) : = f =

λkbkφk for some

b2k ≤ 1 .

(4)

k=1

k=1

The class of reproducing kernel Hilbert spaces contains many interesting classes that are widely used in practice, including polynomials of degree d, Sobolev spaces with smoothness ν, and Gaussian kernels. For more background and examples on reproducing kernel Hilbert spaces, we refer the reader to various standard references [2, 31, 32, 39, 16].
Throughout this paper, we assume that any function f in the unit ball of the Hilbert space is uniformly bounded, meaning that there is some constant B < ∞ such that

f ∞ : = sup |f (x)| ≤ B for all f ∈ BH(1).

(5)

x∈X

This boundedness condition (5) is satisﬁed for any RKHS with a kernel such that supx∈X K(x, x) ≤ B. Kernels of this type include the Gaussian and Laplacian kernels, the kernels underlying Sobolev and other spline classes, as well as as well as any trace class kernel with trignometric eigenfunctions. The boundedness condition (5) is quite standard in non-asymptotic analysis of non-parametric regression procedures [e.g. 36].

2.2 Gradient update equation

We now turn to the form of the gradient update that we study in this paper. Given the samples {(xi, yi)}ni=1, consider minimizing the least-squares loss function

L(f )

:=

1 2n

n

yi − f (xi) 2

(6)

i=1

over some subset of the Hilbert space H. By the representer theorem [23], it suﬃces to restrict attention to functions f belonging to the span of the kernel functions {K(·, xi), i = 1, . . . , n}. Accordingly, we adopt the parameterization

f (·)

=

√1 n

n

ωiK(·, xi),

(7)

i=1

for

some

coeﬃcient

vector

ω

∈

Rn.

Here

the

rescaling

by

√ 1/ n

is

for

later

theoretical

convenience.

Our gradient descent procedure is based on a parameterization of the least-squares loss that

involves the empirical kernel matrix K ∈ Rn×n with entries

[K ]ij

=

1 n

K(xi,

xj

)

for i, j = 1, 2, . . . , n.

(8)

4

For any positive semideﬁnite kernel function, this √matrix must be positive semideﬁnite, and so has a unique symmetric square root denoted by K. Introducing the convenient shorthand y1n : = y1 y2 · · · yn ∈ Rn, we can then write the least-squares loss in the form

L(ω)

=

1 2n

y1n

−

√ nK

ω

22.

A direct approach would be to perform gradient descent on this form of the least-squares loss.

For our purposes, it turn√s out to be more natural to perform gradient descent in the transformed co-ordinate system θ = K ω. Some straightforward calculations (see Appendix A for details)

yield that the gradient descent algorithm in this new co-ordinate system generates a sequence of

vectors {θt}∞ t=0 via the recursion

θt+1 = θt − αt

K

θt

−

√1n

√ K

y1n

,

(9)

where {αt}∞ t=0 is a sequence of positive step sizes (to be chosen by the user). We assume throughout that the gradient descent procedure is initialized with θ0 = 0.

The parameter estimate θt at iteration t√deﬁnes a function estimate ft in the following way. We ﬁrst compute1 the weight vector ωt = K−1 θt, which then deﬁnes the function estimate

ft(·)

=

√1 n

n i=1

ωitK(·,

xi

)

as

before.

In

this

paper,

our

goal

is

to

study

how

the

sequence

{ft}∞ t=0

evolves as an approximation to the true regression function f ∗. We measure the error in two

diﬀerent ways: the L2(Pn) norm

ft − f∗

2 n

:

=

1 n

n

f t(xi) − f ∗(xi) 2

(10)

i=1

compares the functions only at the observed design points, whereas the L2(P)-norm

ft − f∗

2 2

:

=

E

f t(X) − f ∗(X) 2

(11)

corresponds to the usual mean-squared error.

2.3 Overﬁtting and early stopping

In order to illustrate the phenomenon of interest in this paper, we performed some simulations on

a simple problem. In particular, we formed n = 100 i.i.d. observations of the form y = f ∗(xi) + wi,

where wi ∼ N (0, 1), and using the ﬁxed design xi = i/n for i = 1, . . . , n. We then implemented the gradient descent update (9) with initialization θ0 = 0 and constant step sizes αt = 0l25. We

performed this experiment with the regression function f ∗(x) = |x − 1/2| − 1/2, and two diﬀerent

choices of kernel functions. The kernel K(x, x′) = min{x, x′} on the unit square [0, 1] × [0, 1]

generates

an

RKHS

of

Lipschitz

functions,

whereas

the

Gaussian

kernel

K(x,

x′)

=

exp(−

1 2

(x − x′ )2 )

generates a smoother class of inﬁnitely diﬀerentiable functions.

Figure 1 provides plots of the squared prediction error

ft − f ∗

2 n

as a function of the iteration

number t. For both kernels, the prediction error decreases fairly rapidly, reaching a minimum before

or around T ≈ 20 iterations, before then beginning to increase. As the analysis of this paper will

1If the empirical matrix K is not invertible, then we use the pseudoinverse. Note that it may appear as though

a matrix inversion is required to estimate ωt for each t which is computationally intensive. However, the weights ωt

may

be

computed

directly

via

the

iteration

ωt+1

= ωt − αtK(ωt −

√y1n n

).

However,

the

equivalent

update

(9)

is

more

convenient for our analysis.

5

Prediction error Prediction error

Prediction error for first−order Sobolev kernel 0.08

Prediction error for Gaussian kernel 0.06

0.075 0.07
0.065 0.06
0.055 0.05
0.045 0.04

0.055 0.05
0.045 0.04
0.035 0.03
0.025

0

20

40

60

80

100

Iteration

0.02 0

20

40

60

80

100

Iteration

(a)

(b)

Figure 1. Behavior of gradient descent update (9) with constant step size α = 0.25 applied to least-

squares loss with n = 100 with equi-distant design points xi = i/n for i = 1, . . . , n, and regression

function f ∗(x) = |x − 1/2| − 1/2. Each panel gives plots the L2(Pn) error

ft − f ∗

2 n

as

a

function

of

the iteration number t = 1, 2, . . . , 100. (a) For the ﬁrst-order Sobolev kernel K(x, x′) = min{x, x′}.

(b)

For

the

Gaussian

kernel

K(x,

x′)

=

exp(−

1 2

(x

−

x′)2).

clarify, too many iterations lead to ﬁtting the noise in the data (i.e., the additive perturbations wi), as opposed to the underlying function f ∗. In a nutshell, the goal of this paper is to quantify precisely the meaning of “too many” iterations, and in a data-dependent and easily computable
manner.

3 Main results and their consequences

In more detail, our main contribution is to formulate a data-dependent stopping rule, meaning a mapping from the data {(xi, yi)}ni=1 to a positive integer T , such that the two forms of prediction error fT − f ∗ n and fT − f ∗ 2 are minimal. In our formulation of such a stopping rule, two quantities play an important role: ﬁrst, the running sum of the step sizes

t−1
ηt : = ατ ,
τ =0

(12)

and secondly, the eigenvalues λ1 ≥ λ2 ≥ · · · ≥ λn ≥ 0 of the empirical kernel matrix K previously deﬁned (8). The kernel matrix and hence these eigenvalues are computable from the data. We also note that there is a large body of work on fast computation of kernel eigenvalues (e.g., see the paper [15] and references therein).

3.1 Stopping rules and general error bounds
Our stopping rule involves the use of a model complexity measure, familiar from past work on uniform laws over kernel classes [4, 26], known as the local empirical Rademacher complexity. For
6

the kernel classes studied in this paper, it takes the form

RK(ε) : =

1 n

n

min

λi, ε2

1/2
.

(13)

i=1

For a given noise variance σ > 0, a closely related quantity—one of central importance to our analysis—is critical empirical radius εn > 0, deﬁned to be the smallest positive solution to the inequality

RK(ε) ≤ ε2/(2eσ).

(14)

The existence and uniqueness of εn is guaranteed for any reproducing kernel Hilbert space; see Appendix D for details. As clariﬁed in our proof, this inequality plays a key role in trading oﬀ the

bias and variance in a kernel regression estimate.

Our stopping rule is deﬁned in terms of an analogous inequality that involves the running sum

ηt =

t−1 τ =0

ατ

of

the step sizes.

Throughout this paper, we assume that the step sizes are chosen

to satisfy the following properties:

• Boundedness: 0 ≤ ατ ≤ min{1, 1/λ1} for all τ = 0, 1, 2, . . ..

• Non-increasing: ατ+1 ≤ ατ for all τ = 0, 1, 2, . . ..

• Inﬁnite travel: the running sum ηt =

t−1 τ =0

ατ

diverges

as

t

→

+∞.

We refer to any sequence {ατ }∞ τ=0 that satisﬁes these conditions as a valid stepsize sequence. We then deﬁne the stopping time

T : = arg min t ∈ N | RK 1/√ηt > (2eσηt)−1 − 1.

(15)

As discussed in Appendix D, the integer T belongs to the interval [0, ∞) and is unique for any valid
stepsize sequence. As will be clariﬁed in our proof, the intuition underlying the stopping rule (15) is
that the sum of the step-sizes ηt acts as a tuning parameter that controls the bias-variance tradeoﬀ. The stated choice of T optimizes this trade-oﬀ.
The following result applies to any sequence {ft}∞ t=0 of function estimates generated by the gradient iteration (9) with a valid stepsize sequence.

Theorem 1. Given the stopping time T deﬁned by the rule (15), there are universal positive constants (c1, c2) such that the following events both hold with probability at least 1−c1 exp(−c2nε2n):
(a) For all iterations t = 1, 2, ..., T :

ft − f ∗

2 n

≤

4 e ηt

.

(16)

(b) At the iteration T chosen according to the stopping rule (15), we have

fT

− f∗

2 n

≤

12

ε2n.

(17)

(c) Moreover, for all t > T ,

E[

ft − f ∗

2n] ≥

σ2 4

ηt

RK

(ηt−1/2

).

(18)

7

Remarks: Although the bounds (a) and (b) are stated as high probability claims, a simple integration argument can be used to show that the expected mean-squared error (over the noise variables, with the design ﬁxed) satisﬁes a bound of the form

E

ft − f ∗

2 n

≤

4 e ηt

.

(19)

Moreover, as will be clariﬁed in corollaries to follow, Theorem 1 can be used to show that our stopping rule provides minimax-optimal rates for various function classes. The interpretation of Theorem 1 is as follows: if the sum of the step-sizes ηt remains below the threshold deﬁned by (15), applying the gradient update (9) reduces the prediction error. Moreover, note that for Hilbert spaces with a larger kernel complexity, the stopping time T is smaller, since ﬁtting functions in a larger class incurs a greater risk of overﬁtting.

In the case of random design xi ∼ P, we can also provide bounds on the L2(P)-error ft − f ∗ 2. In this setting, for the purposes of comparing to minimax lower bounds, it is also useful to state some results in terms of the population analog of the local empirical Rademacher complexity (13), namely the quantity

RK(ε) : =

1 n

∞
min

λj , ε2

1/2
.

(20)

j=1

Using this complexity measure, we deﬁne the critical population rate εn to be the smallest positive solution to the inequality

40

RK(ε)

≤

ε2 σ

.

(21)

(Our choice of the pre-factor 40 is for later theoretical convenience.) In contrast to the critical empirical rate εn, this quantity is not data-dependent, since it depends on the population eigenvalues of the RKHS H.

Theorem 2 (Random design). Suppose that the design variables {xi}ni=1 are sampled i.i.d. according to P. Then under the conditions of Theorem 1, there are universal positive constants
cj, j = 1, 2, 3 such that

fT

− f∗

2 2

≤

c3ε2n

(22)

with probability at least 1 − c1 exp(−c2nε2n).

Theorems 1 and 2 are general results that apply to any reproducing kernel Hilbert space. Their proofs involve combination of direct analysis of our iterative update (9) combined with techniques from empirical process theory and concentration of measure [36, 24]; see Section 4 for the details.
To compare with the past work of Bu¨hlmann and Yu [10], they also provide a theoretical analysis for gradient descent (referred to as L2-boosting in their paper), focusing exclusively on the ﬁxed design case. Our theory applies to random as well as ﬁxed design, and a broader set of step-size choices. The most signiﬁcant diﬀerence between Theorem 1 in our paper and Theorem 3 in the paper [10] is that we provide a data-dependent stopping rule where as their analysis does not lead to a computable stopping rule.

8

3.2 Some consequences for speciﬁc kernel classes
Let us now illustrate some consequences of our general theory for special choices of kernels that are of interest in practice.

Kernels with polynomial eigendecay: We begin with the class of RKHSs whose eigenvalues satisfy a polynomial decay condition, meaning that

λk ≤ C

1 k

2ν

for some ν > 1/2 and constant C.

(23)

Among other examples, this type of scaling covers various types of Sobolev spaces, consisting of functions with ν derivatives (e.g., [8, 19]). For instance, the ﬁrst-order Sobolev kernel K(x, x′) = min{x, x′}
on the unit square [0, 1] × [0, 1] generates an RKHS of functions that are diﬀerentiable almost
everywhere, given by

1

H : = f : [0, 1] → R | f (0) = 0,

(f ′(x))2dx < ∞ ,

(24)

0

For the uniform measure on [0, 1], this class exhibits polynomial eigendecay (23) with ν = 1. For any class that satisﬁes the polynomial decay condition, we have the following corollary:

Corollary 1. Suppose that in addition to the assumptions of Theorem 2, the kernel class H satisﬁes the polynomial eigenvalue decay (23) for some parameter ν > 1/2. Then there is a universal constant c5 such that

E

fT

− f∗

2 2

]

≤

c5

σ2 n

2ν
. 2ν+1

(25)

Moreover, if λk ≥ c (1/k)2ν for all k = 1, 2, . . ., then

E

ft − f ∗

2 2

≥

1 4

min

1,

σ2

(ηt

)

1 2ν

n

for all iterations t = 1, 2, . . ..

(26)

The proof, provided in Section 4.3, involves showing that the population critical rate (20) is of the

order

O(n−

2ν 2ν+1

).

By

known

results

on

non-parametric

regression

[34,

41],

the

error

bound

(25)

is

minimax-optimal.

In the special case of the ﬁrst-order spline family (24), Corollary 1 guarantees that

E[ fT − f ∗ 22]

σ2 n

2/3.

(27)

In order to test the accuracy of this prediction, we performed the following set of simulations. First, we generated samples from the observation model

yi = f ∗(xi) + wi, for i = 1, 2, . . . , n,

(28)

where xi = i/n, and wi ∼ N (0, σ2) are i.i.d. noise terms. We present results for the function f ∗(x) = |x − 1/2| − 1/2, a piecewise linear function belonging to the ﬁrst-order Sobelev class. For

9

Prediction error using our stopping rule
0.14

Transformed prediction error using our rule
1200

Prediciton error (Prediction error)−3/2

0.12

1000

0.1 800
0.08 600
0.06
400 0.04

0.02

200

0

0

50

100

150

200

250

300

Sample size (n)

0

0

50

100

150

200

250

300

Sample size (n)

(a)

(b)

Figure 2. Prediction error obtained from the stopping rule applied to a regression with n samples
of the form f ∗(xi) + wi at equi-distant design points xi = i/n for i = 0, 1, . . . 99, and i.i.d. Gaussian noise wi ∼ N (0, ). For these simulations, the true regression function is given by f ∗(x) = |x − 1/2| − 1/2. Panel (a): Mean-squared error (MSE) using the stopping rule (15) versus the sample size n. Each point is based on 10, 000 independent realizations of the noise variables {wi}ni=1. 10, 000 randomizations of (wi)ni=1 against the sample size n. Panel (b): Plots of the quantity M SE−3/2 versus sample size n. As predicted by the theory, this representation yields a straight line.

all our experiments, the noise variance σ2 was set to one, but so as to have a data-dependent method, this knowledge was not provided to the estimator. There is a large body of work on estimating the noise variance σ2 in non-parametric regression (see e.g. Hall and Marron [21]). For our simulations, we use the simple estimator based on Hall and Marron [21]. They proved that their estimator is ratio consistent, which is suﬃcient for our purposes.
For a range of sample sizes n between 10 and 300, we performed the updates (9) with constant stepsize α = 0.25, stopping at the speciﬁed time T . For each sample size, we performed 10, 000 independent trials, and averaged the resulting prediction errors. In panel (a) of Figure 2, we plot the mean-squared error versus the sample size, which shows consistency of the method. We also plotted the mean-squared error raised to the power −3/2 versus the sample size. After this rescaling, the bound (27) predicts a linear relation, as is observed in panel (b) of Figure 2. We also performed the same experiments for the case of randomly drawn designs xi ∼ Unif(0, 1). In this case, we observed similar results but with more trials required to average out the additional randomness in the design.
Finite rank kernels: We now turn to the class of RKHSs based on ﬁnite-rank kernels, meaning that there is some ﬁnite integer m < ∞ such that λj = 0 for all j ≥ m + 1. For instance, the kernel function K(x, x′) = (1 + xx′)2 is a ﬁnite rank kernel (with m = 2) that generates the RKHS of all quadratic functions. More generally, for any integer d ≥ 2, the kernel K(x, x′) = (1 + xx′)d generates the RKHS of all polynomials with degree at most d. For any such kernel, we have the following corollary:

10

Corollary 2. If, in addition to the conditions of Theorem 2, the kernel has ﬁnite rank m, then

E

fT

− f∗

2 2

≤

c5

σ2

m n

.

(29)

Importantly,

for

a

rank

m-kernel,

the

rate

m n

is

minimax

optimal

in

terms

of

squared

L2(P)

er-

ror [e.g., 30].

3.3 Comparison with other stopping rules
In this section, we provide a comparison of our stopping rule to two other stopping rules, as well as a oracle method (that involves knowledge of f ∗, and so cannot be computed in practice).

Hold-out method: First, we consider a simple hold-out method: it performs gradient descent

using 50% of the data, and uses the other 50% of the data to estimate the risk (e.g. [14]). Assuming

that the sample size is even for simplicity, we split the full data set {xi}ni=1 into two equally sized subsets Str and Ste. The data indexed by the training set Str is used to estimate the function fttr

using the gradient descent update (9). At each iteration t = 0, 1, 2, . . ., the data indexed by Ste is

used

to

estimate

the

risk

via

RHO(ft)

=

1 n

i∈Ste yi − fttr(xi) 2, which deﬁnes the stopping rule

THO : = arg min t ∈ N | RHO(fttr+1) > RHO(fttr) − 1.

(30)

A line of past work [42, 7, 13, 11, 12, 37]) has analyzed stopping rules based on this type of holdout rule. For instance, Caponetto [13] analyzes a hold-out method, and shows that it yields rates that are optimal for Sobolev spaces with ν ≤ 1 but not in general. The major drawback of using hold-out as that a percentage of the data is lost which increases the risk.

SURE method: Stein’s Unbiased Risk estimate (SURE) can be used to deﬁne an alternative

stopping rule. If we deﬁne the shrinkage matrix S˜t =

t−1 τ =0

(I

−

ατ K),

then

it

can

be

shown

that

the SURE estimator [33] takes the form

RSU(f t) =

1 n

{nσ2

+

Y

T

(S˜t)2

Y

− 2σ2 trace(S˜t)},

(31)

which is easy to compute. This risk estimate deﬁnes the associated stopping rule

TSU : = arg min t ∈ N | RSU(f t+1) > RSU(f t) − 1.

(32)

In contrast with hold-out, this approach makes use of all the data. However, we are not aware of any theoretical guarantees for early stopping using the stopping rule (32).
It can be shown for both stopping rules (30) and (32), a valid sequence of step-sizes guarantees existence and uniqueness of the stopping point. Note that our stopping rule T based on (15) requires estimation of both the empirical eigenvalues, and the noise variance σ2. In contrast, the SURE-based rule requires estimation of σ2 but not the empirical eigenvalues, whereas the hold-out rule requires no parameters to be estimated, but a percentage of the data is used to estimate the risk.

11

Oracle method: As a third point of reference, we also plot the mean-squared error for an “oracle”

method. It is allowed to base its stopping time on the exact prediction error ROR(f t) =

ft −f∗

2 n

,

which deﬁnes the oracle stopping rule

TOR : = arg min t ∈ N | ROR(f t+1) > ROR(f t) − 1.

(33)

Note that this stopping rule is not computable from the data, since it assumes exact knowledge of the function f ∗ that we are trying to estimate.
In order to compare our stopping rule (15) with these alternatives, we generated i.i.d. samples
from the previously described model (see equation (28) and the following discussion). We varied
the sample size n from 10 to 300, and for each sample size, we performed 10, 000 independent trials (randomizations of the noise variables {wi}ni=1), and computed the average of squared prediction error.

Prediction error comparison of stopping rules
0.14

0.12

Our rule

Hold−out

0.1

SURE

Oracle

0.08

0.06

Comparison of stopping rules on log−log scale

Our Rule

10−1

Hold−out

SURE

Oracle

10−2

Prediction error Prediction error

0.04

0.02

0

0

50

100

150

200

250

300

Sample size (n)

10−3 101

Sample size (n) 102

(a)

(b)

Figure 3. The non-parametric function is f ∗(x) = |x − 1/2| − 1/2 with kernel K(x, y) = min(|x|, |y|). We apply the gradient descent update (9) with αt = 1 for all t, and plot the average mean-squared error over 10, 000 randomizations against the sample size for n = 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300. Mean-squared error is plotted for 4 stopping rules: (i) our stopping rule (15); (ii) holding out 50% of the data and using (30); (iii) SURE (32); and (iv) oracle stopping rule (30). For panel (a) results are plotted on a normal scale and for panel (b), curves are plotted using a log-log scale.

Figure 3 plots the resulting mean-squared errors of our stopping rule, the hold-out stopping rule (30), the SURE-based stopping rule (32), and the oracle rule (33). Panel (a) shows the meansquared error versus sample size, whereas panel (b) shows the same curves in terms of logarithm of mean-squared error. Our proposed rule exhibits better performance than the hold-out and SURE-based rules for sample sizes n larger than 50. On the ﬂip side, since the construction of our stopping rule is based on the assumption that f ∗ belongs to a known RKHS, it is unclear how robust it would be to model mis-speciﬁcation. In contrast, the hold-out and SURE-based stopping rules are generic methods, not based directly on the RKHS structure, so might be more robust to model mis-speciﬁcation. Thus, one interesting direction is to explore the robustness of our stopping rule. On the theoretical front, it would be interesting to determine whether the hold-out and/or
12

SURE-based stopping rules can be proven to achieve minimax optimal rates for general kernels, as we have established for our stopping rule.

3.4 Connections to kernel ridge regression

We conclude by presenting an interesting link between our early stopping procedure and kernel ridge regression. The kernel ridge regression (KRR) estimate is deﬁned as

fν : = arg min
f ∈H

1 2n

n

(yi − f (xi))2 +

1 2ν

f

2 H

,

i=1

(34)

where ν is the (inverse) regularization parameter. For any ν < ∞, the objective is strongly convex,

so that the KRR solution is unique.

Friedman and Popescu [18] observed through simulations that the regularization paths for early

stopping of gradient descent and ridge regression are similar, but did not provide any theoretical

explanation of this fact. As an illustration of this empirical phenomenon, Figure 4 compares the

prediction error

fν

− f∗

2 n

of

the

kernel

ridge

regression

estimate

over

the

interval

ν

∈

[1, 100]

versus that of the gradient update (9) over the ﬁrst 100 iterations. Note that the curves, while not

identical, are qualitatively very similar.

Prediction error for first−order Sobolev kernel 0.3
Kernel ridge Gradient
0.25

Prediction error for first−order Sobolev kernel 0.25
Kernel ridge Gradient
0.2

Prediction error Prediction error

0.2

0.15

0.15

0.1

0.1

0.05

0.05 0

20

40

60

80

100

Inverse penalty parameter

0

0

20

40

60

80

100

Inverse penalty parameter

(a)

(b)

Figure 4. Comparison of the prediction error of the path of kernel ridge regression estimates (34) obtained by varying ν ∈ [1, 100] to those of the gradient updates (9) over 100 iterations with constant step size. All simulations were performed with the kernel K(x, x′) = min{x, x′} based on n = 100 samples at the design points xi = i/n with f ∗(x) = |x − 1/2| − 1/2. (a) Noise variance σ2 = 1. (b) Noise variance σ2 = 2.

From past theoretical work [e.g., 36, 26], kernel ridge regression, with the appropriate setting of the penalty parameter ν, is known to achieve minimax-optimal error for various kernel classes, among them the Sobolev and ﬁnite-rank kernels for which stopping rule is provably optimal. In this section, we provide a theoretical basis for these connections, in particular by showing that if the inverse penalty parameter ν is chosen using the same criterion as our stopping rule, then the prediction error satisﬁes the same type of bounds (with ν now playing the role of the running sum ηt).
13

More precisely, suppose that we choose ν to be the smallest positive solution to the inequality

4σν

−1

√ < RK(1/ ν

.

(35)

Note that this criterion is identical to the one underlying our stopping rule, except that the con-

tinuous parameter ν replaces the discrete parameter ηt =

t−1 τ =0

ατ

.

Proposition 1. Consider the kernel ridge regression estimator (34) applied to n i.i.d. samples
{(xi, yi)} with σ-sub Gaussian noise. Then there are universal constants (c1, c2, c3) such that for all δ > 0, the following claims hold with probability at least 1 − c1 exp(−c2 n ε2n):

(a) For all 0 < ν ≤ ν, we have

fν − f ∗

2 n

≤

2 ν

(36)

(b) With ν chosen according to the rule (35), we have

fν − f ∗

2 n

≤

c3

ε2n.

(37)

(c) Moreover, for all ν > ν, we have

E[

fν − f ∗

2n] ≥

σ2 4

ν RK

(ν

−1/2).

(38)

Note that (apart from a slightly diﬀerent leading constant) the upper bound (36) is identical to

the upper bound in equation (16) in Theorem 1. The only diﬀerence is that the inverse regularization

parameter ν replaces the running sum ηt =

t−1 τ =0

ατ .

Similarly,

part

(b)

of

Proposition

1

guarantees

that the kernel ridge regression (34) has prediction error that is upper bounded by the empirical

critical rate ε2n, as in part (b) of Theorem 1. Let us emphasize that bounds of this type on kernel ridge regression have been derived in past work [e.g., 26, 36]. The novelty here is that the structure

of our result reveals the intimate connection to early stopping, and in fact, the proofs follow a

parallel thread.

In conjunction, Proposition 1 and Theorem 1 provide a theoretical explanation for why, as shown

in Figure 4, the paths of the gradient descent update (9) and kernel ridge regression estimate (34)

are so similar. However, it is important to emphasize that from a computational point of view,

early stopping has certain advantages over kernel ridge regression. In general, solving a quadratic

program of the form (34) requires on the order of O(n3) basic operations, and this must be done

repeatedly at each new choice of ν. On the other hand, by its very construction, the iterates of the

gradient algorithm correspond to the desired path of solutions, and each gradient update involves

multiplication by the kernel matrix, incurring O(n2) operations.

4 Proofs
We now turn to the proofs of our main results. The main steps in each proof are provided in the main text, with some of the more technical results deferred to the appendix.
14

4.1 Proof of Theorem 1

In order to derive upper bounds on the L2(Pn)-error in Theorem 1, we ﬁrst rewrite the gradient update (9) in an alternative form. For each iteration t = 0, 1, 2, . . ., let us introduce the shorthand

f t(xn1 ) : = f t(x1) f t(x2) · · · f t(xn) ∈ Rn,

(39)

corresponding to the n-vector obtained by evaluating the function f t at all design points, and the short-hand

w : = w1, w2, ..., wn ∈ Rn,

(40)

corresponding to the vector of zero mean sub-Gaussian noise random variables. From equation (7),

we have the relation

f t(xn1 )

=

√1 n

K

ωt

=

√1 n

√ K

θt.

√ Consequently, by multiplying both sides of the gradient update (9) by K, we ﬁnd that the sequence

{f t(xn1 )}∞ t=0 evolves according to the recursion

f t+1(xn1 ) = f t(xn1 ) − αtK (f t(xn1 ) − y1n) = In×n − αtK f t(xn1 ) − αtK y1n.

(41)

Since θ0 = 0, the sequence is initialized with f 0(xn1 ) = 0. The recursion (41) lies at the heart of our analysis.
Letting r = rank(K), the empirical kernel matrix has the eigendecomposition K = U ΛU T , where U ∈ Rn×n is an orthonormal matrix (satisfying U U T = U T U = In×n) and

Λ : = diag(λ1, λ2, . . . , λr, 0, 0, · · · , 0)

is the diagonal matrix of eigenvalues, augmented with n − r zero eigenvalues as needed. We then deﬁne a sequence of diagonal shrinkage matrices St as follows:

t−1
St : = (In×n − ατ Λ) ∈ Rn×n.
τ =0

The matrix St indicates the extent of shrinkage towards the origin; since 0 ≤ αt ≤ min{1, 1/λ1} for all iterations t, in the positive semodeﬁnite ordering, we have the sandwich relation

0 St+1 St In×n.
Moreover, the following lemma shows that the L2(Pn)-error at each iteration can be bounded in terms of the eigendecomposition and these shrinkage matrices:

Lemma 1 (Bias/variance decomposition). At each iteration t = 0, 1, 2, . . .,

ft − f ∗

2 n

≤

2 n

r

(St)2jj[U T

f ∗(xn1 )]2j

+

2 n

n

[U

T

f

∗(xn1 )]2j

+

2 n

r

(1 − Sjtj)2[U T w]2j .

j=1

j=r+1

j=1

(42)

Squared Bias Bt2

Variance Vt

Moreover,

we

have

the

lower

bound

E[

ft − f ∗

2 n

]

≥

E[Vt].

15

See Appendix B.1 for the proof of this intermediate claim.

In order to complete the proof of the upper bound in Theorem 1, our next step is to obtain high probability upper bounds on these two terms. We summarize our conclusions in an additional lemma, and use it to complete the proof of Theorem 1(a) before returning to prove it.

Lemma 2 (Bounds on the bias and variance). For all iterations t = 1, 2, . . ., the squared bias is upper bounded as

Bt2

≤

1 e ηt

,

(43)

Moreover, there is a universal constant c1 > 0 such that, for any iteration t = 1, 2, . . . , T ,

Vt ≤ 5σ2 ηtR2K 1/√ηt

(44)

with probability at least 1 − exp

− c1 nε2n

.

Moreover for all t, we have E[Vt] ≥

σ2 4

ηtR2K

1/√ηt

.

on

We the

can now event Vt

≤co5mσp2lηettRe 2Kthe1/p√roηotf

of Theorem , we have

1(a).

The

bound

(16)

follows

quickly:

conditioned

ft − f ∗

(i)

2 n

≤

Bt2

+

Vt

(ii)
≤

1 e ηt

+ 5σ2 ηtR2K

1/√ηt

(iii)
≤

4 e ηt

,

where inequality (i) follows from (42) in Lemma 1, and inequality (ii) follows from the bounds in Lemma 2 and (iii) follows since t ≤ T . The lower bound (c) in equation (18) follows from (44).
Turning to the proof of part (b), using the upper bound from (a)

fT

− f∗

2 n

≤

1 e ηT

+

5 ηT

≤

4 eηT

.

Based

on

the

deﬁnition

of

T

and

εn,

we

are

guaranteed

that

1 ηT +1

≤

ε2n,

Moreover,

by

the

non-

decreasing nature of our step sizes, we have αT +1 ≤ αT , which implies that ηT +1 ≤ 2ηT , and hence

1 ηT

≤

2 ηT +1

≤

2ε2n.

Putting together the pieces establishes the bound claimed in part (b). It remains to establish the bias and variance bounds stated in Lemma 2, and we do so in the
following subsections. The following auxiliary lemma plays a role in both proofs:

Lemma 3 (Properties of shrinkage matrices). For all indices j ∈ {1, 2, . . . , r}, the shrinkage matrices St satisfy the bounds

0≤

(St)2jj

≤

1, 2eηtλj

and

1 2

min{1,

ηtλj }

≤

1

−

Sjtj

≤ min{1, ηtλj}.

See Appendix B.2 for the proof of this result.

(45a) (45b)

16

4.1.1 Bounding the squared bias

Let us now prove the upper bound (43) on the squared bias. We bound each of the two terms in

the deﬁnition (42) of Bt2 in term. Applying the upper bound (45a) from Lemma 3, we see that

2 n

r
(St)2jj[U T f ∗(xn1 )]2j
j=1

≤

e

1 n ηt

r [U T f ∗(xn1 )]2j .

j=1

λj

Now consider the linear operator ΦX : ℓ2(N) → Rn deﬁned element-wise via [ΦX ]jk = φj(xk). Similarly, we deﬁne a (diagonal) linear operator D : ℓ2(N) → ℓ2(N) with entries [D]jj = λj and [D]jk = 0 for j = k. With these deﬁnitions, the vector f (xn1 ) ∈ Rn can be expressed in terms of some sequence a ∈ ℓ2(N) in the form

f (xn1 ) = ΦX D1/2a.

In

terms

of

these

quantities,

we

can

write

K

=

1 n

ΦX

DΦTX

.

Moreover,

as

previously

noted,

we

also

have K = U ΛU T where Λ = diag{λ1, λ2, . . . , λn}, and U ∈ Rn×n is orthonormal. Combining the

two representations, we conclude that

ΦX√D1/2 n

=

U Λ1/2Ψ∗,

for some linear operator Ψ : Rn → ℓ2(N) (with adjoint Ψ∗) such that Ψ∗Ψ = In×n. Using this equality, we have

1 r [U T f ∗(X)]2j

e ηt n j=1

λj

=

e

1 ηt

n

r [U T ΦX D1/2a]2j

j=1

λj

=

1 e ηt

r [U T U Λ1/2V ∗a]2j

j=1

λj

=

1 e ηt

r λj [Ψ∗a]2j j=1 λj

≤

1 e ηt

Ψ∗a

2 2

≤

1 e ηt

,

(46)

Here the ﬁnal step follows from the fact that Ψ is a unitary operator, so that

Ψ∗a

2 2

≤

a

2 2

=

f∗

2 H

≤

1.

Turning to the second term in the deﬁnition (42), we have

n

[U T f ∗(xn1 )]2j

=

2 n

n

[U T ΦX D1/2a]2j

j=r+1

j=r+1

n

=

[U T U Λ1/2Ψ∗a]2j

j=r+1

n

=

[Λ1/2 Ψ∗ a]2j

j=r+1

= 0,

(47)

17

where the ﬁnal step uses the fact that Λ1jj/2 = 0 for all j ∈ {r +1, . . . , n} by construction. Combining the upper bounds (46) and (47) with the deﬁnition (42) of Bt2 yields the claim (43).

4.1.2 Controlling the variance

Let us now prove the bounds (44) on the variance term Vt. (To simplify the proof, we assume throughout that σ = 1; the general case can be recovered by a simple rescaling argument). By the

deﬁnition of Vt, we have

Vt

=

2 n

r

(1 − Sjtj)2[U T w]2j =

2 n

trace(U QU T

wwT

),

j=1

where Q = diag{(1−Sjtj)2, j = 1, . . . , n} is a diagonal matrix. Since E[wwT ] ≤ In×n by assumption,

we

have

E[Vt]

=

2 n

trace(Q).

Using

the

upper

bound

in

equation

(45b)

from

Lemma

3,

we

have

1 n

trace(Q)

≤

1 n

r

min{1, (ηtλj)2} = ηt

RK (1/√ηt )

2
,

j=1

where the ﬁnal equality uses the deﬁnition of RK . Putting together the pieces, we see that

E[Vt] ≤ 2 ηt

RK (1/√ηt)

2
.

(48a)

Similarly, using the lower bound in equation (45b), we can show that

E[Vt]

≥

σ2 4

ηt

RK (1/√ηt )

2
.

(48b)

Our next step is to obtain a bound on the two-sided tail probability P[|Vt −E[Vt]| ≥ δ], for which

we make use of a result on two-sided deviations for quadratic forms in sub-Gaussian variables. In

particular, consider a random variable of the form Qn =

n i,j=1

aij (ZiZj

−

E[Zi Zj ])

where

{Zi}ni=1

are i.i.d. zero-mean and sub-Gaussian variables (with parameter 1). Wright [40] proves that there

is a constant c such that

P |Q − E[Q]| ≥ δ ≤ exp

− c min

δ |||A|||op

,

δ2 |||A|||2F

for all u > 0,

(49)

where (|||A|||op, |||A|||F) are (respectively) the operator and Frobenius norms of the matrix A = {aij}ni,j=1.

If

we

apply

this

result

with

A

=

2 n

U

QU

T

and

Zi

= wi,

then

we

have

Q

=

Vt,

and

moreover

|||A|||op

≤

2 n

,

and

|||A|||2F

=

4 n2

trace(U T QU T U QU T )

=

4 n2

trace(Q2)

≤

4 n2

trace(Q)

≤

4 n

ηt

RK(1/√ηt) .

Consequently, the bound (49) implies that

P |Vt − E[Vt]| ≥ δ

≤ exp

− 4c n δ min{1, δ

ηtRK (1/√ηt)

−1
}.

(50)

Since t ≤ T setting δ = 3σ2ηt RK(1/√ηt) , the claim (44) follows.

18

4.2 Proof of Theorem 2

This proof is based on the following two steps: • ﬁrst, proving that the error fT − f ∗ 2 in the L2(P) norm is, with high probability, close to the error in the L2(Pn) norm, and

• second, showing the empirical critical radius εn deﬁned in equation (14) is upper bounded by the population critical radius εn deﬁned in equation (21).
Our proof is based on a number of more technical auxiliary lemmas, proved in the appendices. The ﬁrst lemma provides a high probability bound on the Hilbert norm of the estimate fT .
Lemma 4. There exist universal constants c1 and c2 > 0 such that ft H ≤ 2 for all t ≤ T with probability greater than or equal to 1 − c1 exp(−c2nε2n).
See Appendix E.1 for the proof of this claim. Our second lemma shows in any bounded RKHS, the L2(P) and L2(Pn) norms are uniformly close up to the population critical radius εn over a Hilbert ball of constant radius:

Lemma 5. Consider a Hilbert space such that g ∞ ≤ B for all g ∈ BH(3). Then there exist universal constants (c1, c2, c3) such that for any t ≥ εn, we have

|g

2 n

−

g

2 2

|

≤

c1t2,

(51)

with probability at least 1 − c2 exp(−c3nt2).

This claim follows from known results on reproducing kernel Hilbert spaces (e.g., Lemma 5.16 in the paper [36] and Theorem 2.1 in the paper [6]). Our ﬁnal lemma, proved in Appendix E.2, relates the critical empirical radius εn to the population radius εn:

Lemma 6. The inequality εn ≤ εn holds with probability at least 1 − c1 exp(−c2nε2n).

With these lemmas in hand, the proof of the theorem is straightforward. First, from Lemma 4, we have fT H ≤ 2 and hence by triangle inequality, fT − f ∗ H ≤ 3 with high probability as well. Next, applying Lemma 5 with t = εn, we ﬁnd that

fT

− f∗

2 2

≤

fT − f ∗

2 n

+

c1ε2n

≤

c4(ε2n

+

ε2n),

with probability greater than 1 − c2 exp(−c3nε2n). Finally, applying Lemma 6 yields that the bound

fT

− f∗

2 2

≤

cε2n

holds with the claimed

probability.

4.3 Proof of Corollaries
In each case, it suﬃces to upper bound the population critical rate ε2n previously deﬁned.

Proof of Corollary 2: In this case, we have

so

that

ε2n

=

c′σ2

m n

.

RK(ǫ) = √1n

m
min{λj, ǫ2} ≤

m n

ǫ

j=1

19

Proof of Corollary 1: For any M ≥ 1, we have

RK(ǫ) = √1n

∞
min{C j−2ν , ǫ2} ≤
j=1
≤
≤

M n

ǫ

+

C n

∞
j−2ν

j=⌈M ⌉

M n

ǫ

+

C′ n

∞
t−2ν dt
M

M n

ǫ

+

C

′′

√1n

(1/M

)ν

−

1 2

Setting

M

=

ǫ−1/ν

yields

RK(ǫ) ≤

C

∗ǫ1−

1 2ν

.

Consequently,

the

critical

inequality

RK(ǫ) ≤ 40ǫ2/σ

is

satisﬁed

for

εn

≍

(σ2

/n)

2ν 2ν+1

,

as

claimed.

4.4 Proof of Proposition 1

We now turn to the proof of our results on the kernel ridge regression estimate (34). The proof follows a very similar structure to that of Theorem 1. Recall the eigendecomposition K = U ΛU T of the empirical kernel matrix, and that we use r to denote its rank. For each ν > 0, we deﬁne the
ridge shrinkage matrix

Rν : = In×n + νΛ −1.

(52)

We then have the following analog of Lemma 2 from the proof of Theorem 1:

Lemma 7 (Bias/variance decomposition for kernel ridge regression). For any ν > 0, the prediction error for the estimate fν is bounded as

fν − f ∗

2 n

≤

2 n

r

[Rν ]2jj [U T

f ∗(xn1 )]2j

+

2 n

n

[U T

f ∗(xn1 )]2j

+

2 n

r

1 − Rjνj 2[U T w]2j .

j=1

j=r+1

j=1

(53)

Note that Lemma 7 is identical to Lemma 2 with the shrinkage matrices St replaced by their analogues Rν. See Appendix C.1 for the proof of this claim.

Our next step is to show that the diagonal elements of the shrinkage matrices Rν are bounded:

Lemma 8 (Properties of kernel ridge shrinkage). For all indices j ∈ {1, 2, . . . , r}, the diagonal entries Rν satisfy the bounds

1 2

min

1, νλj

0

≤

(Rjνj )2 ≤

1, 4ν λj

and

≤ 1 − Rjνj ≤ min 1, νλj .

(54a) (54b)

Note

that

this

is

the

analog

of

Lemma

3

from

Theorem

1,

albeit

with

the

constant

1 4

in

the

bound (54a)

instead

of

1 2e

.

See Appendix C.2

for the

proof of

this

claim.

With

these lemmas

in

place, the remainder of the proof follows as in the proof of Theorem 1.

20

5 Discussion

In this paper, we have analyzed the early stopping strategy as applied to gradient descent on the

non-parametric least squares loss. Our main contribution was to propose an easily computable

and data-dependent stopping rule, and to provide upper bounds on the empirical L2(Pn) error (Theorem 1) and population L2(P) error (Theorem 2). We demonstrate in Corollaries 1 and 2

that our stopping rule yields minimax optimal rates for both low rank kernel classes and Sobolev

spaces. Our simulation results conﬁrm that our stopping rule yields theoretically optimal rates of

convergence for Lipschitz kernels, and performs favorably in comparison to stopping rules based on

hold-out data and Stein’s Unbiased Risk Estimate. We also showed that early stopping with sum

of step-sizes ηt =

t−1 k=0

αk

has

a

regularization

path

that

satisﬁes

almost

identical

mean-squared

error bounds as kernel ridge regression indexed by penalty parameter ν.

Our analysis and stopping rule may be improved and extended in a number of ways. First, it

would interesting to see how our stopping rule can be adapted to mis-speciﬁed models. As speciﬁed,

our method relies on computation of the eigenvalues of the kernel matrix. A stopping rule based

on approximate eigenvalue computations, for instance via some form of sub-sampling [15], would

be interesting to study as well.

Acknowledgements
This work was partially supported by NSF grant DMS-1107000 to MJW and BY. In addition, BY was partially supported by the NSF grant SES-0835531 (CDI), ARO-W911NF-11-1-0114 and the Center for Science of Information (CSoI), an US NSF Science and Technology Center, under grant agreement CCF-0939370, and MJW was also partially supported ONR MURI grant N00014-11-1086. During this work, GR received partial support from a Berkeley Graduate Fellowship.

A Derivation of gradient descent updates

In this appendix, we provide the deta√ils of how the gradient descent updates (9) are obtained. In terms of the transformed vector θ = K ω, the least-squares objective takes the form

L(θ )

:=

1 2n

y1n

−

√√ nK

θ

2 2

=

1 2n

y1n

2 2

−

√1n

y1n,

√ Kθ

+

1 2

(θ

)T

Kθ.

(55)

Given a sequence {αt}∞ t=0, the gradient descent algorithm operates via the recursion θt+1 = θt − αt∇L(θt). Taking the gradient of L yields

∇L(θ )

=

K

θ

−

√1 n

√ K

y1n.

Substituting into the gradient descent update yields the claim (9).

B Auxiliary lemmas for Theorem 1
In this appendix, we collect together the proofs of the lemmas for Theorem 1. 21

B.1 Proof of Lemma 1

We prove this lemma by analyzing the gradient descent iteration in an alternative co-ordinate
system. In particular, given a vector f t(xn1 ) ∈ Rn and the SVD K = U ΛU T of the empirical kernel matrix, we deﬁne the vector γt = √1n U T f t(xn1 ). In this new-coordinate system, our goal is to estimate the vector γ∗ = √1n U T f ∗(xn1 ). Recalling the alternative form (41) of the gradient recursion, some simple algebra yields that the sequence {γt}∞ t=0 evolves as

γt+1

=

γt

+

αtΛ

√w˜ n

−

αtΛ(γt

−

γ∗),

where w˜ : = U T w is a rotated noise vector. Since γ0 = 0, unwrapping this recursion then yields

γt − γ∗ =

I − St

√w˜ n

−

Stγ∗,

where

we

have

made

use

of

the

previously

deﬁned

shrinkage

matrices

St. Using the inequality

a+b

2 2

≤ 2(

a

2 2

+

b 22), we ﬁnd that

γt − γ∗

2 2

≤

2 n

(I − St)w˜

2 2

+

2

Stγ∗

2 2

(=i)

2 n

(I

− St)w˜

2 2

+

2

r

[St]2jj(γj∗j)2 + 2

n
(γj∗j )2 .

j=1

j=r+1

where step (i) uses the fact that λj = 0 for all j ∈ {r + 1, . . . , n}. Finally, the orthogonality of U

implies that

γt − γ∗

2 2

=

1 n

f t(xn1 ) − f ∗(xn1 )

2 2

,

from

which

the

upper

bound

(42)

follows.

B.2 Proof of Lemma 3

Using the deﬁnition of St and the elementary inequality 1 − u ≤ exp(−u), we have

[St]2jj =

t−1
(1 − ατ λj )
τ =0

2

(i)

≤ exp(−2ηtλj) ≤

1, 2eηtλj

where inequality (i) follows from the fact that sup u exp(−u) = 1/e.

u∈R

Turning to the second set of inequalities, we have 1−[St]jj = 1−

t−1 τ =0

(1

−

ατ

λj

).

By

induction,

it can be shown that

1 − [St]jj ≤ 1 − max{0, 1 − ηtλj} = min{1, ηtλj}.

As for the remaining claim, we have

1

−

t−1

(1

−

ατ λj )

(i)
≥

1

−

exp(−ηtλi)

τ =0

(ii)
≥

1

−

(1

+

ηtλi)−1

= ηtλi

1 + ηtλi

≥

1 2

min{1, ηtλi},

where step (i) follows from the inequality 1 − u ≤ exp(−u); and step (ii) follows from the inequality exp(−u) ≤ (1 + u)−1, valid for u > 0.

22

C Auxiliary results for Proposition 1
In this appendix, we prove the auxiliary lemmas used in the proof of Proposition 1 on kernel ridge regression.

C.1 Proof of Lemma 7

By deﬁnition of the KRR estimate, we have

K

+

1 ν

I

fν(xn1 ) = Ky1n. Consequently, some straight-

forward algebra yields the relation

U T fν(xn1 ) = (I − Rν )U T y1n,
where the shrinkage matrix Rν was previously deﬁned (52). The remainder of the proof follows using identical steps to the proof of Lemma 1 with St replaced by Rν.

C.2 Proof of Lemma 8

By

deﬁnition

(52)

of

the

shrinkage

matrix,

we

have

[Rν ]2jj

=

(1 + νλj )−2

≤

1.
4νλj

Moreover,

we

also

have

1 − [Rν]jj = 1 − (1 + νλj)−1 = νλj ≤ min{1, νλj }, and 1 + νλj

1 − [Rν ]jj

=

ν λj 1 + νλj

≥

1 2

min{1, νλj }.

D Properties of the empirical Rademacher complexity

In this section, we prove that the εn lies in the interval (0, ∞), and is unique. Recall that the stopping point T is deﬁned as εn : = arg min ǫ > 0 | RK ǫ ≤ ǫ2/(2eσ) . Re-arranging and

substituting for RK ǫ yields the equivalent expression

n
εn : = arg min ǫ > 0 | min ǫ−2λi, 1 > nǫ2/(4e2σ2) .
i=1

Note that

n i=1

min

ǫ−2λi, 1

is non-increasing in ǫ while nǫ2 is increasing in ǫ. Furthermore when

ǫ = 0, 0 = nǫ2 <

n i=1

min

ǫ−2λi, 1

> 0 while for ǫ = ∞,

n i=1

min

ηtλi, 1

< nǫ2. Hence εn

exists. Further, RK(ǫ) is a continuous function of ǫ since it is the sum of n continuous functions,

Therefore, the critical radius εn exists, is unique and satisﬁes the ﬁxed point equation

RK εn = ε2n/(2eσ).

Finally, we show that the integer T belongs to the interval [0, ∞) and is unique for any valid

sequence of step-sizes. Be the deﬁnition of T given by the stopping rule (15) and εn, we have

1 ηT +1

≤ ε2n ≤

1 ηT

.

Since η0 = 0 and ηt → ∞ as t → ∞ and εn ∈ (0, ∞), there exists a unique

stopping point T in the interval [0, ∞).

23

E Auxiliary results for Theorem 2
This appendix is devoted to the proofs of auxiliary lemmas used in the proof for Theorem 2.

E.1 Proof of Lemma 4

Let us write ft =

∞ k=0

√ λk ak φk ,

so

that

ft

2 H

=

∞ k=0

a2k .

Recall

the

linear

operator

ΦX

:

ℓ2(N)

→

Rn

deﬁned element-wise via [ΦX ]jk = φj(xk) and the diagonal operator D : ℓ2(N) → ℓ2(N) with entries

[D]jj = λj and [D]jk = 0 for j = k. By the deﬁnition of the gradient update (9), we have the

relation

a

=

1 n

D1/2ΦTX

K

−1

ft(xn1

).

Since

1 n

ΦX

DΦTX

= K,

ft

2 H

=

a

2 2

=

1 n

ft

(xn1 )T

K

−1ft(xn1

).

(56)

Recall the eigendecomposition K = U ΛU T with Λ = diag(λ1, λ2, . . . λr), and the relation U T f t(xn1 ) = (I − St)U T y1n. Substituting into equation (56) yields

ft

2 H

=

1 n

(y1n)T

U (I

−

St)2Λ−1U T

y1n

(=i)

1 n

(f

∗(xn1

)

+

w)T U (I

−

St)2Λ−1U T (f ∗(xn1 ) +

w)

=

2 n

wT

U

(I

− St)2Λ−1U T f ∗(xn1 ) +

1 n

wT

U

(I

− St)2Λ−1U T w +

1 n

f

∗(xn1

)T

U

(I

− St)2Λ−1U T f ∗(xn1 )

At

Bt

Ct

where equality (i) follows from the observation equation y1n = f ∗(xn1 ) + w. From Lemma 3, we have

(i)

1 − Sjtj

≤ 1,

and

hence

Ct

≤

1 n

f

∗(xn1

)T

U

Λ−1U

T

f

∗(xn1 )

≤

1,

where the

last

step

follows

from

the

analysis in Section 4.1.1.

It remains to derive upper bounds on the random variables At and Bt.

Bounding At: Since the elements of w are i.i.d, zero-mean and sub-Gaussian with parameter

σ,

we

have

P[|At|

≥

1]

≤

2

exp(−

n 2σ2 ν

2

),

where

ν2

:=

4 n

[f

∗(xn1 )]T

U

(I

−

St)4Λˆ −2U T f ∗(xn1 ).

Since

(1 − (St)jj) ≤ 1, we have

ν2

≤

4 n

f

∗

(xn1 )T

U

(I

− St)Λ−2U T f ∗(xn1 )

≤

4 n

r j=1

[U T f ∗(xn1 )]2j λ2j

min(1, ηtλj)

≤

4

ηt n

r j=1

[U T f ∗(xn1 )]2j λj

≤ 4ηt,

where the ﬁnal inequality follows from the analysis in Section 4.1.1.

Bounding Bt:

We begin by noting that

Bt

=

1 n

r j=1

(1

− Sjtj λj

)2

[U

T

w]2j

=

1 n

trace(U QU T , wwT ),

24

where Q = diag{ (1−Sjtj )2 , j = 1, 2, . . . r}.
λj

Consequently, Bt is a quadratic form in zero-mean

sub-Gaussian variables, and using the tail bound (49), we have

P |Bt − E[Bt]| ≥ 1 ] ≤ exp(−c min{n|||U QU T |||−op1, n2|||U QU T |||−F 2})

for a universal constant c. It remains to bound E[Bt], |||U QU T |||op and |||U QU T |||F. We ﬁrst bound the mean. Since E[wwT ] σ2In×n by assumption, we have

E[Bt]

≤

σ2 trace(Q) 1 r =

n

n

j=1

( (1 − Sjtj)2 ) λj

≤ ηt n

r
min((ηtλj)−1, ηtλj )
j=1

But by the deﬁnition (15) of the stopping rule and the fact that t ≤ T , we have

ηt n

r

min((ηtλj)−1, ηtλj ) ≤ ηt2R2K (1/√ηt)

≤

1 σ2

,

j=1

showing that E[Bt] ≤ 1. Turning to the operator norm, we have

|||U QU T |||op

=

(1 max (
j=1,...,r

− Sjtj)2 ) λj

≤

max
j=1,...,r

min(λj −1, ηt2λj )

≤ ηt.

As for the Frobenius norm, we have

1 |||U QU T n

|||2F

=

r (1 (
j=1

− Sjtj λj 2

)4

)

≤

1 n

r

min(λj−2, ηt4λj 2)

j=1

≤

ηt3 n

r

min(ηt−3λj−2, ηtλj2)

j=1

Using the deﬁnition of the empirical kernel complexity, we have

1 n

|||U

QU

T

|||2F

≤

ηt3R2K (1/√ηt)

≤

ηt σ2

,

where the ﬁnal inequality holds for t ≤ T , using the deﬁnition of the stopping rule. Putting together the pieces, we have shown that

P[|Bt| ≥ 2 or |At| ≥ 1] ≤ exp(−cn/ηt)

for

all

t

≤

T.

Since

1 ηt

≥

ε2n

for

any

t

≤

T,

the

claim

follows.

E.2 Proof of Lemma 6

In this section, we need to show that εn ≤ εn. Recall that εn and εn satisfy

RK (εn)

=

ε2n 2eσ

and

RK(εn)

=

ε2n 40σ

.

It

suﬃces

to

prove

that

RK (εn) ≤

ε2n 2eσ

using

the

deﬁnition

of

εn.

25

In order to prove the claim, we deﬁne the random variables

1n

1n

Zn(w, t) : =

sup
g H≤1 g n≤t

n wig(xi) ,
i=1

and

Zn(w, t) : = Ex

sup
g H≤1 g 2≤t

n wig(xi)
i=1

,

(57)

where wi ∼ N (0, 1) are i.i.d. standard normal, as well as the associated (deterministic) functions

Qn(t) : = Ew Zn(w; t) and Qn(t) : = Ew Zn(w; t) .

(58)

By results of Mendelson [26], there are universal constants 0 < cℓ ≤ cu such that for all t2 ≥ 1/n, we have

cℓRK(t) ≤ Qn(t) ≤ cuRK(t), and cℓRK (t) ≤ Qn(t) ≤ cuRK(t).

We ﬁrst appeal to the concentration of Lipschitz functions for Gaussian random variables to show that Zn(w, t) and Zn(w, t) are concentrated around their respective means. For any t > 0 and vectors w, w′ ∈ Rn, we have

|Zn(w, t) − Zn(w′, t)| ≤

sup
g n≤t

1 n

|

n
(wi
i=1

−

wi′ )g(xi )|

≤

√tn

w − w′

2,

g H≤1

showing that w → Zn(w, t) is √tn -Lipschitz with respect to the ℓ2 norm. A similar calculation for w → Zn(w, t) shows that

|Ex[Zn(w, t)] − Ex[Zn(w′, t)]|

≤

Ex[

sup
g 2≤t g H≤1

1 n

|

n
(wi
i=1

− wi′)g(xi)|]

≤

√tn

w − w′

2,

so

that

it

is

also

Lipschitz

√t n

.

Consequently,

standard

concentration

results

[24]

imply

that

P |Zn(w, t) − Qn(t)| ≥ t0 ≤ 2 exp

−

nt20 2t2

,

and

P |Zn(w, t) − Qn(t)| ≥ t0 ≤ 2 exp

−

nt20 2t2

.

(59)

Let us condition on the two events A(t, t0) : = {|Zn(w, t)−Qn(t)| ≤ t0} and A′(t, t0) : = {|Zn(w, t) − Qn(t)| ≤ t0} We then have

(a)
RK (εn) ≤

Zn(w, εn) +

ε2n 4eσ

(b)
≤

Zn(w, 2εn) +

ε2n 4eσ

(c)
≤

2RK(εn)

+

3ε2n 8eσ

(d)
≤

ε2n 2eσ

,

where

inequality

(a)

follows

the

ﬁrst

bound

in

equation

(59)

with

t0

=

ε2n 4eσ

and

t

=

ε2n,

inequality

(b) follows from Lemma 5 with t = εn, inequality (c) follows from the second bound (59) with

t0 = and

A8εe′2n(σt,atn0)d

t= hold

ε2n, and inequality (d) follows from the deﬁnition with the stated probability, the claim follows.

of

εn.

Since

the

events

A(t, t0)

26

References
[1] R. S. Anderssen and P. M. Prenter. A formal comparison of methods proposed for the numerical solution of ﬁrst kind integral equations. Jour. Australian Math. Soc. (Ser. B), 22:488–500, 1981.
[2] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society, 68:337–404, 1950.
[3] A. R. Barron, A. Cohen, W. Dahmen, and R. A. DeVore. Approximation and learning by greedy algorithms. Annals of Statistics, 36(1):64–94, 2008.
[4] P. Bartlett and S. Mendelson. Gaussian and Rademacher complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463–482, 2002.
[5] P. Bartlett and M. Traskin. Adaboost is consistent. Journal of Machine Learning Research, 8:2347–2368, 2007.
[6] P. L. Bartlett, M. I. Jordan, and J. D. McAuliﬀe. Convexity, classiﬁcation, and risk bounds. Journal of the American Statistical Association, 101:138–156, 2006.
[7] F. Bauer, S. Pereverzev, and L. Rosasco. On regularization algorithms in learning theory. J. Complexity, 23:52–72, 2007.
[8] M. S. Birman and M. Z. Solomjak. Piecewise-polynomial approximations of functions of the classes Wpα. Math. USSR-Sbornik, 2(3):295–317, 1967.
[9] G. Blanchard and M. Kramer. Optimal learning rates for kernel conjugate gradient regression. In Proceedings of the NIPS Conference, 2010.
[10] P. Buhlmann and B.Yu. Boosting with L2 loss: Regression and classiﬁcation. Journal of American Statistical Association, 98:324–340, 2003.
[11] A. Caponetto and Y. Yao. Adaptation for regularization operators in learning theory. Technical Report CBCL Paper #265/AI Technical Report #063, Massachusetts Institute of Technology, September 2006.
[12] A. Caponetto and Y. Yao. Cross-validation based adaptation for regularization operators in learning theory. Analysis and Applications, 8(2):161–183, 2010.
[13] A. Caponneto. Optimal rates for regularization operators in learning theory. Technical Report CBCL Paper #264/AI Technical Report #062, Massachusetts Institute of Technology, September 2006.
[14] L. Devroye and T. J. Wagner. Distribution-free inequalities for the deleted and holdout error estimates. IEEE Trans. on Information Theory, 25:202–207, 1979.
[15] P. Drineas and M. W. Mahoney. On the Nystr¨om method for approximating a Gram matrix for improved kernel-based learning. Journal of Machine Learning Research, 6:2153–2175, 2005.
[16] H. L. Weinert (ed.), editor. Reproducing Kernel Hilbert Spaces : Applications in Statistical Signal Processing. Hutchinson Ross Publishing Co., Stroudsburg, PA, 1982.
27

[17] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997.
[18] J.H. Friedman and B. Popescu. Gradient directed regularization. Technical report, Stanford University, 2004.
[19] C. Gu. Smoothing spline ANOVA models. Springer Series in Statistics. Springer, New York, NY, 2002.
[20] M. G. Gu and H. T. Zhu. Maximum likelihood estimation by markov chain monte carlo approximation. J. R. Statist. Soc. B, 63:339–355, 2001.
[21] P. Hall and J.S. Marron. On variance estimation in nonparametric regression. Biometrika, 77:415–419, 1990.
[22] W. Jiang. Process consistency for adaboost. Annals of Statistics, 32:13–29, 2004.
[23] G. Kimeldorf and G. Wahba. Some results on Tchebycheﬃan spline functions. Jour. Math. Anal. Appl., 33:82–95, 1971.
[24] M. Ledoux. The Concentration of Measure Phenomenon. Mathematical Surveys and Monographs. American Mathematical Society, Providence, RI, 2001.
[25] L. Mason, J. Baxter, P. Bartlett, and M. Frean. Boosting algorithms as gradient descent. In Neural Information Processing Systems (NIPS), December 1999.
[26] S. Mendelson. Geometric parameters of kernel machines. In Proceedings of COLT, pages 29–43, 2002.
[27] J. Mercer. Functions of positive and negative type and their connection with the theory of integral equations. Philosophical Transactions of the Royal Society A, 209:415–446, 1909.
[28] N. Morgan and H. Bourlard. Generalization and parameter estimation in feedforward nets: Some experiments. In Proceedings of Neural Information Processing Systems, 1990.
[29] L. Orecchia and M. W. Mahoney. Implementing regularization implicitly via approximate eigenvector computation. In ICML ’11, 2011.
[30] G. Raskutti, M. J. Wainwright, and B. Yu. Minimax-optimal rates for sparse additive models over kernel classes via convex programming. Journal of Machine Learning Research, 12:389– 427, March 2012.
[31] S. Saitoh. Theory of Reproducing Kernels and its Applications. Longman Scientiﬁc & Technical, Harlow, UK, 1988.
[32] B. Scho¨lkopf and A. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.
[33] C. M. Stein. Estimation of the mean of a multivariate normal distribution. Annals of Statistics, 9(6):1135–1151, 1981.
[34] C. J. Stone. Additive regression and other nonparametric models. Annals of Statistics, 13(2):689–705, 1985.
28

[35] O. N. Strand. Theory and methods related to the singular value expansion and Landweber’s iteration for integral equations of the ﬁrst kind. SIAM J. Numer. Anal., 11:798–825, 1974.
[36] S. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000. [37] E. De Vito, S. Pereverzyev, and L. Rosasco. Adaptive kernel methods using the balancing
principle. Foundations of Computational Mathematics, 10(4):455–479, 2010. [38] G. Wahba. Three topics in ill-posed problems. In M. Engl and G. Groetsch, editors, Inverse
and ill-posed problems, pages 37–50. Academic Press, 1987. [39] G. Wahba. Spline models for observational data. CBMS-NSF Regional Conference Series in
Applied Mathematics. SIAM, Philadelphia, PN, 1990. [40] F. T. Wright. A bound on tail probabilities for quadratic forms in independent random
variables whose distributions are not necessarily symmetric. Annals of Probability, 1(6):1068– 1070, 1973. [41] Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. Annals of Statistics, 27(5):1564–1599, 1999. [42] Y. Yao, L. Rosasco, and A. Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26:289–315, 2007. [43] T. Zhang and B. Yu. Boosting with early stopping: Convergence and consistency. Annal of Statistics, 33:1538–1579, 2005.
29

