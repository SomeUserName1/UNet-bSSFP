
Skip to main content
Cornell University
We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate
arxiv logo > cs > arXiv:1711.05101

Help | Advanced Search
Search
Computer Science > Machine Learning
(cs)
[Submitted on 14 Nov 2017 ( v1 ), last revised 4 Jan 2019 (this version, v3)]
Title: Decoupled Weight Decay Regularization
Authors: Ilya Loshchilov , Frank Hutter
View a PDF of the paper titled Decoupled Weight Decay Regularization, by Ilya Loshchilov and 1 other authors
View PDF

    Abstract: L 2 regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L 2 regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at this https URL 

Comments: 	Published as a conference paper at ICLR 2019
Subjects: 	Machine Learning (cs.LG) ; Neural and Evolutionary Computing (cs.NE); Optimization and Control (math.OC)
Cite as: 	arXiv:1711.05101 [cs.LG]
  	(or arXiv:1711.05101v3 [cs.LG] for this version)
  	https://doi.org/10.48550/arXiv.1711.05101
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Ilya Loshchilov [ view email ]
[v1] Tue, 14 Nov 2017 14:24:06 UTC (5,111 KB)
[v2] Wed, 14 Feb 2018 14:03:35 UTC (7,746 KB)
[v3] Fri, 4 Jan 2019 21:01:49 UTC (8,347 KB)
Full-text links:
Access Paper:

    View a PDF of the paper titled Decoupled Weight Decay Regularization, by Ilya Loshchilov and 1 other authors
    View PDF
    TeX Source
    Other Formats 

view license
Current browse context:
cs.LG
< prev   |   next >
new | recent | 1711
Change to browse by:
cs
cs.NE
math
math.OC
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

5 blog links
( what is this? )
DBLP - CS Bibliography
listing | bibtex
Ilya Loshchilov
Frank Hutter
a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

